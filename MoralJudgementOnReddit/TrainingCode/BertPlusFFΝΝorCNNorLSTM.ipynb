{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BertPlusFFΝΝorCNNorLSTM.ipynb","provenance":[{"file_id":"1csJ0dBnH9ikapt2sbGOrgKaq-juu5a5k","timestamp":1623072324099}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMb3pK2MuPqGKrFGDS3L8RP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kXZHTlLktkET"},"source":["# Procedural"]},{"cell_type":"markdown","metadata":{"id":"sO68x14Ttng8"},"source":["Mount the drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Xy3bv5WtK-L","executionInfo":{"status":"ok","timestamp":1628368347918,"user_tz":-60,"elapsed":838,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"4771a80f-be09-455a-fd0b-63da12f51f24"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hV6EhA_trrn","executionInfo":{"status":"ok","timestamp":1628368352524,"user_tz":-60,"elapsed":3196,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"f0b98629-e4ae-43aa-9ace-0cbdd03719ce"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqEu7p8Vtsa7","executionInfo":{"status":"ok","timestamp":1628368354526,"user_tz":-60,"elapsed":2005,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"aa7c80e3-0904-4a9f-b289-59cb947c438e"},"source":["!pip install psutil"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"erqJDq6KtxDO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628368355018,"user_tz":-60,"elapsed":494,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"a881b9db-fe1e-4e6b-e5c4-37da3b085f51"},"source":["import time\n","import pandas as pd\n","import torch\n","import numpy as np\n","from torch import nn, optim, utils\n","import psutil\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","import matplotlib.pyplot as plt\n","import random\n","import transformers\n","import re"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IM36EX8_ty-T","executionInfo":{"status":"ok","timestamp":1628368355019,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["RANDOM_SEED = 42"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OccH6XtLtzx4"},"source":["# Prepare the data for training"]},{"cell_type":"markdown","metadata":{"id":"m_7woqaXt7SQ"},"source":["Set the hyperparameters needed for data preparation"]},{"cell_type":"code","metadata":{"id":"Zg2YQgbFt3ov","executionInfo":{"status":"ok","timestamp":1628368396618,"user_tz":-60,"elapsed":410,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["BATCH_SIZE = 16\n","\n","MODEL_NAME = \"bert-base-cased\"\n","\n","MAX_LEN = 80\n","DATA_TO_CLASSIFY = \"comments_body\"\n","\n","VALID_TEST_PROPORTION = 0.2 if \"posts\" in DATA_TO_CLASSIFY else 0.05"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0yzfJwBusdn"},"source":["Fetch the test data and create a new series"]},{"cell_type":"code","metadata":{"id":"59SnMlhluGBF","executionInfo":{"status":"ok","timestamp":1628368404734,"user_tz":-60,"elapsed":2955,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["if \"posts\" in DATA_TO_CLASSIFY:\n","    df = pd.read_csv('drive/MyDrive/MastersProject/data/aita_clean.csv')\n","    df['text'] = df[\"title\"] + \" \" + df[\"body\"].fillna(\"\")\n","\n","elif \"comments\" in DATA_TO_CLASSIFY:\n","    df = pd.read_csv('drive/MyDrive/MastersProject/data/comments_for_bert_no_etiquettes.csv')\n","\n","    # Set comment ids as the indices for the dataframe\n","    df.set_index(\"id\", inplace=True)\n","\n","else:\n","    raise ValueError(\"Invalid DATA_TO_CLASSIFY variable!\")\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsGeKaiuNik9","executionInfo":{"status":"ok","timestamp":1628368404734,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"5ffecf44-7689-4fc5-f4e0-5bdd377a0e29"},"source":["pd.set_option('display.max_rows', 10)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', -1)\n","print(df.body.head())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["id\n","cg08970    I've been on the receiving end of this before from a man, and I can tell you that it was really scary. (Context: I am a woman). You're going to need to exercise some empathy here and try and understand why what you did wasn't okay. She's not a bitch, she's just really frightened.\\n\\nYou scared her with your initial texts. You should have just stayed away from her instead of trying to explain yourself, because she's already afraid of you. You made it worse because people generally, and usually unfairly, fear people with mental illnesses. By acting out, you really hurt her image of you as being balanced. Now that you've told her, she's even more* afraid of you because you've proven yourself to be cruel and unpredictable.\\n\\nEdit: Let me add that I am in no way judging you. This is just the way life is. Learn from this experience and move on. This just wasn't a great context to be vulnerable with her in this way because the trust between you two was already damaged. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n","cgdobj3    What if there were no arseholes? These guys want to play video games; you do not. Both are normal behaviours. If you didn't have an agreement beforehand, then there is no clear notion of who is right and who is wrong. \\n\\nTo maintain the situation where there are no arseholes, find a solution. You could negotiate a solution (eg., 11pm school-nights, 2am weekends--or whatever), you could get some white noise in your room (one of my friends swears by pink noise), or you could move out.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n","cgcxbgb    You're not an asshole in my eyes. You did pay a bunch of money to ride the 2nd day.\\n\\nI'm surprised that the mountain was not willing to change his pass and also surprised that your friend was not willing to pay $20 more to ride the 2nd day even if it was the mountain's mistake. Yes the mountain people were assholes but my time is important to me, so for $20 more I would have paid the difference anyway. If I really felt shitty about it I would have broken something or stolen something from their premises to make up for the $20 difference if I had to.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n","cgcfv1n    I was going to say you are but then I read u/movierich's comment and changed my mind. Once pics are on the web, they can pretty much be considered public domain. At the very least, the person *wants* the pics to be seen. And if they're posing sexy, it entails that they want to engage your sexual interest. \\n\\nYou actually had the decency of reaching out to point out something to her which if left unattended could have cause her some harm. That's saying a lot more than most of the other dudes that rubbed one out to the same pics.\\n\\nEdit: Just saw the pic you posted. She's hot as hell. I think *I'm* in love.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n","cgcdfh0    Unless you are grossly exaggerating the value of your management work vs. free rent, you're not the asshole.  \\n\\nI am one of those people who is very uncomfortable asking for help from anyone, and I feel a lot of responsibility to help people I live and work with (even if it's not my job).  So, I am constantly being approached by people who want something from me.  And I am usually surprised how entitled people with their hand out feel.  Forget whether or not they are entitled to a favor...people asking me for a favor seem to assume they're going to get what they want out of me AND they assume I owe them the effort of making them feel good for asking.  If I'm in the middle of something complicated and the interruption is a real problem, people coming to me looking for favors get offended when I'm momentarily distracted and they can tell it's a bad time.  Most people feel like it's my job to act happy that I'm being interrupted and  be thankful for the chance to drop what I'm doing on the alter of their current whim.  Where does this attitude come from?  If you ask me, if you're here to put upon me, you're the one who's called upon to be friendly and accommodating.  You should be thankful that I'm willing to help you, not demand that I do some additional obsequious acting to make you feel better for being a nudge.  \\n\\nI say step-dad is the asshole, and mom is is enabling assholery.  \\n\\nHere's how the conversation would have gone if he wasn't a self-entitled prima donna:  \\n\\nStepDad: Are you busy?\\nYou:        Yes. \\nSD:         Oh, ok.  I just wanted to see if you could list an eBay auction for me.  \\nYou:        Oh!  That doesn't take too long.  I'll make time.  \n","Name: body, dtype: object\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"c0ZvVVX-u7Ll"},"source":["Split the dataframes"]},{"cell_type":"code","metadata":{"id":"KL8ChUTnu7zG","executionInfo":{"status":"ok","timestamp":1628368429555,"user_tz":-60,"elapsed":421,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["df_train, df_valid_test = train_test_split(df, test_size=VALID_TEST_PROPORTION, shuffle=True, random_state=RANDOM_SEED)\n","df_valid, df_test = train_test_split(df_valid_test, test_size=0.5, shuffle=True, random_state=RANDOM_SEED)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FOxKx2wG6dP9"},"source":["Inspect the shapes of the split dataframes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FXIAI0-V6Hbc","executionInfo":{"status":"ok","timestamp":1628368431184,"user_tz":-60,"elapsed":350,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"e743f9ed-cddc-47a0-8089-fcfb6bc1a256"},"source":["print(\"df_train:\", df_train.shape)\n","print(\"df_valid:\", df_valid.shape)\n","print(\"df_test: \", df_test.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["df_train: (582293, 4)\n","df_valid: (15323, 4)\n","df_test:  (15324, 4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D0uRZG7T_u41"},"source":["Create a function that corrects imbalance in a dataframe"]},{"cell_type":"code","metadata":{"id":"PahYGaKU8S0a","executionInfo":{"status":"ok","timestamp":1628368432489,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def create_rebalanced_undersampled_dataframe(df):\n","    '''\n","    Rebalance a dataframe by undersampling the majority class\n","    '''\n","\n","    valid_counts = torch.bincount(torch.tensor(df.is_asshole.values))\n","    prob_drop = (valid_counts[0] - valid_counts[1]) / valid_counts[0]\n","\n","    idcs_to_drop = []\n","    for index, row in df.iterrows():\n","        if row.is_asshole == 0:\n","            drop = random.choices([\"drop\", \"dont_drop\"], [prob_drop, 1-prob_drop])\n","            if drop[0] == \"drop\":\n","                idcs_to_drop.append(index)\n","    df_rebalanced = df.drop(index=idcs_to_drop)\n","    print(\"df_rebalanced: \", df_rebalanced.shape)\n","    print(\"Occurrences of each class: \", torch.bincount(torch.tensor(df_rebalanced.is_asshole.values)))\n","\n","    return df_rebalanced"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b41xSA7M_1za"},"source":["Create a rebalanced validation dataframe"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YI64fPRe-ovu","executionInfo":{"status":"ok","timestamp":1628368435794,"user_tz":-60,"elapsed":1540,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"03d7e719-c41e-4a15-dff8-d0e1c2220510"},"source":["df_valid_reb = create_rebalanced_undersampled_dataframe(df_valid)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["df_rebalanced:  (10346, 4)\n","Occurrences of each class:  tensor([5202, 5144])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7axfbEbPt9o1"},"source":["Specify the dataset class"]},{"cell_type":"code","metadata":{"id":"1CI_PSltuhiG","executionInfo":{"status":"ok","timestamp":1628368436151,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["class AITADatasetTrain(utils.data.Dataset):\n","    # Upon onject instance creation, you feed the text samples, their targets, the tokeniser and the max length.\n","    def __init__(self, texts, targets, tokeniser, max_len, weight_per_class):\n","        self.texts = texts\n","        self.targets = targets\n","        self.tokeniser = tokeniser\n","        self.max_len = max_len\n","        self.weight_per_class = weight_per_class\n","        \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    # This method is called when a batch is created. \"item\" is the index of each sample to be in batch.\n","    def __getitem__(self, item):\n","        # Normally it is already a string\n","        text = str(self.texts[item])\n","\n","        # Create a dictionary constituting the encoding of the current item (i.e. current text)\n","        encoding = tokeniser(\n","            text,\n","            truncation=True,\n","            max_length=self.max_len,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_token_type_ids=False,\n","            return_tensors='pt')\n","        \n","        # These are unnecessary I think\n","        encoding['input_ids'] = encoding['input_ids'].flatten()\n","        encoding['attention_mask'] = encoding['attention_mask'].flatten()\n","\n","        # Find the weight that the current sample should have during training\n","        w = weight_per_class[self.targets[item]]\n","        \n","        # In the encoding dictionary for the current text, add the target corresponding to it and the actual test\n","        dic_out = {'input_ids': encoding['input_ids'],\n","                   'attention_mask': encoding['attention_mask'],\n","                   'targets': torch.tensor(self.targets[item], dtype=torch.long),\n","                   'sample_text': text,\n","                   'weights': w}\n","        \n","        return dic_out"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"0gLx50n17qTv","executionInfo":{"status":"ok","timestamp":1628368443707,"user_tz":-60,"elapsed":342,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["class AITADatasetNoWeights(utils.data.Dataset):\n","    # Upon onject instance creation, you feed the text samples, their targets, the tokeniser and the max length.\n","    def __init__(self, texts, targets, tokeniser, max_len):\n","        self.texts = texts\n","        self.targets = targets\n","        self.tokeniser = tokeniser\n","        self.max_len = max_len\n","        \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    # This method is called when a batch is created. \"item\" is the index of each sample to be in batch.\n","    def __getitem__(self, item):\n","        # Normally it is already a string\n","        text = str(self.texts[item])\n","\n","        # Create a dictionary constituting the encoding of the current item (i.e. current text)\n","        encoding = tokeniser(\n","            text,\n","            truncation=True,\n","            max_length=self.max_len,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_token_type_ids=False,\n","            return_tensors='pt')\n","        \n","        # These are unnecessary I think\n","        encoding['input_ids'] = encoding['input_ids'].flatten()\n","        encoding['attention_mask'] = encoding['attention_mask'].flatten()\n","        \n","        # In the encoding dictionary for the current text, add the target corresponding to it and the actual test\n","        dic_out = {'input_ids': encoding['input_ids'],\n","                   'attention_mask': encoding['attention_mask'],\n","                   'targets': torch.tensor(self.targets[item], dtype=torch.long),\n","                   'sample_text': text}\n","        \n","        return dic_out"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"86HGovmMu0Np"},"source":["Make a function that creates a dataloader for BERT"]},{"cell_type":"code","metadata":{"id":"dyGu7ogUu1Lf","executionInfo":{"status":"ok","timestamp":1628368445473,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def create_dataloader_with_weights(df, tokeniser, max_len, batch_size, weight_per_class, data_to_classify):\n","    '''\n","    Creates a dataset from the given dataframe and a dataloader spitting batches of the dataset\n","    '''\n","    if \"text\" in data_to_classify:\n","        ds = AITADatasetTrain(\n","            texts=df.text.to_numpy(),\n","            targets=df.is_asshole.to_numpy(),\n","            tokeniser=tokeniser,\n","            max_len=max_len,\n","            weight_per_class=weight_per_class)\n","    elif \"body\" in data_to_classify:\n","        ds = AITADatasetTrain(\n","            texts=df.body.to_numpy(),\n","            targets=df.is_asshole.to_numpy(),\n","            tokeniser=tokeniser,\n","            max_len=max_len,\n","            weight_per_class=weight_per_class)\n","    elif \"title\" in data_to_classify:\n","        ds = AITADatasetTrain(\n","            texts=df.title.to_numpy(),\n","            targets=df.is_asshole.to_numpy(),\n","            tokeniser=tokeniser,\n","            max_len=max_len,\n","            weight_per_class=weight_per_class)\n","    else:\n","        raise ValueError(\"Data to classify not recognised!\")\n","    \n","    dataloader = utils.data.DataLoader(ds, batch_size=batch_size, num_workers=2, drop_last=True)\n","    \n","    return dataloader"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJmSgAv37ati","executionInfo":{"status":"ok","timestamp":1628368447060,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def create_dataloader_no_weights(df, tokeniser, max_len, batch_size, data_to_classify):\n","    '''\n","    Creates a dataset from the given dataframe and a dataloader spitting batches of the dataset\n","    '''\n","    if \"text\" in data_to_classify:\n","        ds = AITADatasetTrain(\n","            texts=df.text.to_numpy(),\n","            targets=df.is_asshole.to_numpy(),\n","            tokeniser=tokeniser,\n","            max_len=max_len,\n","            weight_per_class=weight_per_class)\n","    elif \"body\" in data_to_classify:\n","        ds = AITADatasetTrain(\n","            texts=df.body.to_numpy(),\n","            targets=df.is_asshole.to_numpy(),\n","            tokeniser=tokeniser,\n","            max_len=max_len,\n","            weight_per_class=weight_per_class)\n","    elif \"title\" in data_to_classify:\n","        ds = AITADatasetTrain(\n","            texts=df.title.to_numpy(),\n","            targets=df.is_asshole.to_numpy(),\n","            tokeniser=tokeniser,\n","            max_len=max_len,\n","            weight_per_class=weight_per_class)\n","    else:\n","        raise ValueError(\"Data to classify not recognised!\")\n","    \n","    dataloader = utils.data.DataLoader(ds, batch_size=batch_size, num_workers=2, drop_last=True)\n","    \n","    return dataloader"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SjdvegaLwbFf"},"source":["Make a function that calculates class weights that correct imbalance\n"]},{"cell_type":"code","metadata":{"id":"MnHrZCokwaql","executionInfo":{"status":"ok","timestamp":1628368448563,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def create_rebalancing_weights_per_class(df):\n","    '''\n","    Rebalance by weighing samples of each class by their inverse class occurrence rate during training\n","    '''\n","    counts = torch.bincount(torch.tensor(df.is_asshole.values))\n","    weight_per_class = []\n","    num_classes = len(counts)\n","    for i in range(0, num_classes):\n","        w = (sum(counts)) / (num_classes * counts[i])\n","        weight_per_class.append(w)\n","        print(\"class {}: occurrences = {}, weight = {}, occurrences * weight = {}\".format(i, counts[i], w, w * counts[i]))\n","    print()\n","    print(\"These are the weights per class:\", weight_per_class)\n","    return weight_per_class"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFe7oAhh6zBN"},"source":["Create the tokeniser to be used in creating the dataloaders"]},{"cell_type":"code","metadata":{"id":"bW47v8tt1KFX","executionInfo":{"status":"ok","timestamp":1628368457272,"user_tz":-60,"elapsed":6777,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["tokeniser = transformers.BertTokenizer.from_pretrained(MODEL_NAME)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s47qOkdkAMfi"},"source":["Calculate the eights of each class to correct imbalance"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNMvZhtRALx2","executionInfo":{"status":"ok","timestamp":1628368457272,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"5b5a4406-8fad-485e-aa4a-cdf22355153a"},"source":["weight_per_class = create_rebalancing_weights_per_class(df_train)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["class 0: occurrences = 388044, weight = 0.7502924799919128, occurrences * weight = 291146.5\n","class 1: occurrences = 194249, weight = 1.4988313913345337, occurrences * weight = 291146.5\n","\n","These are the weights per class: [tensor(0.7503), tensor(1.4988)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U2U2RYGq66Yd"},"source":["Create the dataloaders"]},{"cell_type":"code","metadata":{"id":"kf3vp0Kk0nSc","executionInfo":{"status":"ok","timestamp":1628368457273,"user_tz":-60,"elapsed":13,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["train_loader = create_dataloader_with_weights(df_train, tokeniser, MAX_LEN, BATCH_SIZE, weight_per_class, DATA_TO_CLASSIFY)\n","valid_loader = create_dataloader_with_weights(df_valid, tokeniser, MAX_LEN, BATCH_SIZE, weight_per_class, DATA_TO_CLASSIFY)\n","valid_reb_loader = create_dataloader_no_weights(df_valid_reb, tokeniser, MAX_LEN, BATCH_SIZE, DATA_TO_CLASSIFY)\n","test_loader = create_dataloader_no_weights(df_test, tokeniser, MAX_LEN, BATCH_SIZE, DATA_TO_CLASSIFY)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8L0OK_PSAeaO"},"source":["Inspect the train_loader"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVCgbsU_1U4P","executionInfo":{"status":"ok","timestamp":1628368457273,"user_tz":-60,"elapsed":13,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"321ed633-06cd-4378-d4af-5dd624c37e29"},"source":["train_batch = next(iter(train_loader))\n","loader_keys = train_batch.keys()\n","print(\"Each loader batch is a dictionary with keys:\", loader_keys)\n","print()\n","print(\"input_ids batch shape        :\", train_batch['input_ids'].shape)\n","print(\"attention_mask batch shape   :\", train_batch['attention_mask'].shape)\n","print(\"targets batch shape          :\", train_batch['targets'].shape)\n","print(\"weights batch shape          :\", train_batch['weights'].shape)\n","print()\n","print(\"targets batch:\", train_batch['targets'][10:18])\n","print(\"weights batch:\", train_batch['weights'][10:18])\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Each loader batch is a dictionary with keys: dict_keys(['input_ids', 'attention_mask', 'targets', 'sample_text', 'weights'])\n","\n","input_ids batch shape        : torch.Size([16, 80])\n","attention_mask batch shape   : torch.Size([16, 80])\n","targets batch shape          : torch.Size([16])\n","weights batch shape          : torch.Size([16])\n","\n","targets batch: tensor([0, 1, 0, 0, 0, 0])\n","weights batch: tensor([0.7503, 1.4988, 0.7503, 0.7503, 0.7503, 0.7503])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UJZqzjkcEsYT"},"source":["# Create the classifier"]},{"cell_type":"markdown","metadata":{"id":"ymkIbWaOE3Rx"},"source":["Set the hyperparameters needed for creating the classifier"]},{"cell_type":"code","metadata":{"id":"Kfqehl1QBEgq","executionInfo":{"status":"ok","timestamp":1628368484133,"user_tz":-60,"elapsed":319,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["# Hyperparameters relevant to all classifier types\n","DROPOUT_PROPORTION = 0\n","BERT_OUTPUT_USED = \"pooled\"       # in {full, pooled}\n","CLASSIFIER_TYPE = \"FFNN\"        # in {FFNN, CNN, LSTM}\n","NUM_UNFROZEN_BERT_LAYERS = 8\n","\n","# Hyperparameters relevant only to the FFNN\n","NUMBER_NEURONS_LAYER_1 = 0\n","NUMBER_NEURONS_LAYER_2 = 0\n","NUMBER_NEURONS_LAYER_3 = 0\n","NUMBER_NEURONS_LAYER_4 = 0\n","BATCHNORM_ON = True\n","\n","# Hyperparameters relevant only to the CNN\n","CNN_CHANNELS_OUT = 10\n","\n","# Hyperparameters relevant only to the LSTM\n","LSTM_HIDDEN_DIM = 100"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNIH7RVSE8CJ"},"source":["Use GPU if available"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2CV64PyE7I0","executionInfo":{"status":"ok","timestamp":1628368488921,"user_tz":-60,"elapsed":335,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"1ad5736d-be01-40b3-f0fb-ed7a5f12e6dd"},"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","torch.cuda.empty_cache()\n","print(device)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2YPIdt-pFDW8"},"source":["Specify the architecture of the classifier including frozen BERT"]},{"cell_type":"code","metadata":{"id":"YCE2VrlFFGqU","executionInfo":{"status":"ok","timestamp":1628368491349,"user_tz":-60,"elapsed":346,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["class SentimentClassifier(nn.Module):\n","    '''\n","    The sentiment classifier class that includes BERT\n","    '''\n","    def __init__(self, device, batch_size, bert_out_to_classify, classifier_type, dropout_proportion, bert_max_seq_len, model_name, num_unfrozen_bert_layers,\n","                 \n","                 n_neurons_1, n_neurons_2, n_neurons_3, n_neurons_4, batchnorm_on,\n","\n","                 CNN_channels_out,\n","\n","                 LSTM_hidden_dim):\n","        super(SentimentClassifier, self).__init__()\n","\n","        # Instantiate the straight forward attributes relating to all 3 classifier types\n","        self.dropout_proportion = dropout_proportion\n","        self.bert_out_to_classify = bert_out_to_classify\n","        self.classifier_type = classifier_type\n","        self.batch_size = batch_size\n","        self.device = device\n","        self.embedding_dim = 768\n","        self.max_seq_len = bert_max_seq_len\n","        self.batchnorm_on = batchnorm_on\n","        self.num_unfrozen_bert_layers = num_unfrozen_bert_layers\n","\n","        # Instantiate BERT\n","        self.bert_config = transformers.BertConfig(vocab_size=28996, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, max_position_embeddings=512)\n","        self.bert = transformers.BertModel.from_pretrained(model_name, config=self.bert_config)\n","        # Freeze BERT so that its weights are not further fine-tuned from their pretrained values\n","        if self.bert_out_to_classify == \"full\":\n","            # modules = [self.bert.embeddings, *self.bert.encoder.layer[:], self.bert.pooler]\n","            modules = [self.bert.embeddings, *self.bert.encoder.layer[:12 - self.num_unfrozen_bert_layers], self.bert.pooler]\n","        else:\n","            modules = [self.bert.embeddings, *self.bert.encoder.layer[:12 - self.num_unfrozen_bert_layers]]\n","        for module in modules:     \n","            for param in module.parameters():\n","                param.requires_grad = False\n","\n","        # First tackle the case of linear layers on top of BERT\n","        if self.classifier_type == \"FFNN\":\n","\n","            # Establish the number of neurons each layer has\n","            self.n_neurons_1 = n_neurons_1\n","            self.n_neurons_2 = n_neurons_2\n","            self.n_neurons_3 = n_neurons_3\n","            self.n_neurons_4 = n_neurons_4       \n","\n","            # Determine the features of each sample input into classification layers depending on whether it is pooled output or full last hidden state of BERT.\n","            if \"pooled\" in self.bert_out_to_classify:\n","                self.feats_in = self.embedding_dim\n","            elif \"full\" in self.bert_out_to_classify:\n","                self.feats_in = self.embedding_dim * self.max_seq_len\n","            else:\n","                raise ValueError(\"BERT output to classify not recognised\")\n","\n","            # Determine the number of layers based on the given neurons for each layer prior to the last which by default has 1 neuron.\n","            if self.n_neurons_1 == 0:\n","                self.num_layers = 1\n","            elif self.n_neurons_2 == 0:\n","                self.num_layers = 2\n","            elif self.n_neurons_3 == 0:\n","                self.num_layers = 3\n","            elif self.n_neurons_4 == 0:\n","                self.num_layers = 4\n","            else:\n","                self.num_layers = 5\n","\n","\n","           # Structure the architecture of the network depending on the number of layers and their number of neurons for the case WITH BATCHNORM\n","            if self.batchnorm_on:\n","                if self.num_layers == 1:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, 1),\n","                        nn.Sigmoid())\n","\n","                elif self.num_layers == 2:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, self.n_neurons_1),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_1),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","\n","                        nn.Linear(self.n_neurons_1, 1),\n","                        nn.Sigmoid())\n","                \n","                elif self.num_layers == 3:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, self.n_neurons_1),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_1),\n","                        # nn.Dropout(p=0.9),\n","\n","                        nn.Linear(self.n_neurons_1, self.n_neurons_2),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_2),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","                        \n","                        nn.Linear(self.n_neurons_2, 1),\n","                        nn.Sigmoid())\n","                \n","                elif self.num_layers == 4:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, self.n_neurons_1),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_1),\n","                        # nn.Dropout(p=0.9),\n","\n","                        nn.Linear(self.n_neurons_1, self.n_neurons_2),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_2),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","\n","                        nn.Linear(self.n_neurons_2, self.n_neurons_3),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_3),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","                        \n","                        nn.Linear(self.n_neurons_3, 1),\n","                        nn.Sigmoid())\n","                \n","                elif self.num_layers == 5:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, self.n_neurons_1),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_1),\n","                        # nn.Dropout(p=0.9),\n","\n","                        nn.Linear(self.n_neurons_1, self.n_neurons_2),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_2),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","\n","                        nn.Linear(self.n_neurons_2, self.n_neurons_3),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_3),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","\n","                        nn.Linear(self.n_neurons_3, self.n_neurons_4),\n","                        nn.ReLU(),\n","                        nn.BatchNorm1d(self.n_neurons_4),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","                        \n","                        nn.Linear(self.n_neurons_4, 1),\n","                        nn.Sigmoid())\n","                    \n","\n","            # Structure the architecture of the network depending on the number of layers and their number of neurons for the case WITHOUT BATCHNORM\n","            elif not self.batchnorm_on:\n","                if self.num_layers == 1:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, 1),\n","                        nn.Sigmoid())\n","\n","                elif self.num_layers == 2:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, self.n_neurons_1),\n","                        nn.ReLU(),\n","                        nn.Dropout(p=self.dropout_proportion),\n","\n","                        nn.Linear(self.n_neurons_1, 1),\n","                        nn.Sigmoid())\n","                \n","                elif self.num_layers == 3:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, self.n_neurons_1),\n","                        nn.ReLU(),\n","                        # nn.Dropout(p=0.9),\n","\n","                        nn.Linear(self.n_neurons_1, self.n_neurons_2),\n","                        nn.ReLU(),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","                        \n","                        nn.Linear(self.n_neurons_2, 1),\n","                        nn.Sigmoid())\n","                \n","                elif self.num_layers == 4:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, self.n_neurons_1),\n","                        nn.ReLU(),\n","                        # nn.Dropout(p=0.9),\n","\n","                        nn.Linear(self.n_neurons_1, self.n_neurons_2),\n","                        nn.ReLU(),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","\n","                        nn.Linear(self.n_neurons_2, self.n_neurons_3),\n","                        nn.ReLU(),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","                        \n","                        nn.Linear(self.n_neurons_3, 1),\n","                        nn.Sigmoid())\n","                \n","                elif self.num_layers == 5:\n","\n","                    self.classifier = nn.Sequential(\n","                        nn.Linear(self.feats_in, self.n_neurons_1),\n","                        nn.ReLU(),\n","                        # nn.Dropout(p=0.9),\n","\n","                        nn.Linear(self.n_neurons_1, self.n_neurons_2),\n","                        nn.ReLU(),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","\n","                        nn.Linear(self.n_neurons_2, self.n_neurons_3),\n","                        nn.ReLU(),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","\n","                        nn.Linear(self.n_neurons_3, self.n_neurons_4),\n","                        nn.ReLU(),\n","                        # nn.Dropout(p=self.dropout_proportion),\n","                        \n","                        nn.Linear(self.n_neurons_4, 1),\n","                        nn.Sigmoid())\n","                    \n","        \n","        # Now tackle the case of a CNN on top of BERT\n","        elif self.classifier_type == \"CNN\":\n","            if \"full\" not in self.bert_out_to_classify:\n","                # The input to the CNN needs to be embedding so it will always be the full hidden state output of BERT\n","                raise ValueError(\"Cannot have a CNN classifier on top of BERT pooled output!\")\n","\n","            # How many filters of each size to apply to input\n","            self.c_out = CNN_channels_out\n","            \n","            # Set kernel sizes\n","            self.kernel_1_size = 1\n","            self.kernel_2_size = 2\n","            self.kernel_3_size = 3\n","            self.kernel_4_size = 4\n","            self.kernel_5_size = 5\n","            self.kernel_6_size = 6\n","\n","            # Set convolution layers\n","            self.conv_1 = nn.Conv1d(self.embedding_dim, self.c_out, self.kernel_1_size, stride=1, padding=0)\n","            self.conv_2 = nn.Conv1d(self.embedding_dim, self.c_out, self.kernel_2_size, stride=1, padding=0)\n","            self.conv_3 = nn.Conv1d(self.embedding_dim, self.c_out, self.kernel_3_size, stride=1, padding=0)\n","            self.conv_4 = nn.Conv1d(self.embedding_dim, self.c_out, self.kernel_4_size, stride=1, padding=0)\n","            self.conv_5 = nn.Conv1d(self.embedding_dim, self.c_out, self.kernel_5_size, stride=1, padding=0)\n","            self.conv_6 = nn.Conv1d(self.embedding_dim, self.c_out, self.kernel_6_size, stride=1, padding=0)\n","\n","            # Set the max pooling layers\n","            self.pool_1 = nn.MaxPool1d(self.max_seq_len)\n","            self.pool_2 = nn.MaxPool1d(self.max_seq_len - 1)\n","            self.pool_3 = nn.MaxPool1d(self.max_seq_len - 2)\n","            self.pool_4 = nn.MaxPool1d(self.max_seq_len - 3)\n","            self.pool_5 = nn.MaxPool1d(self.max_seq_len - 4)\n","            self.pool_6 = nn.MaxPool1d(self.max_seq_len - 5)\n","            \n","            if self.batchnorm_on:\n","                # Set the fully connected output layer\n","                self.fc = nn.Sequential(\n","                    nn.BatchNorm1d(6 * self.c_out),\n","                    nn.Dropout(p=self.dropout_proportion),\n","                    nn.Linear(6 * self.c_out, 1),\n","                    nn.Sigmoid())\n","            else:\n","\n","                # Set the fully connected output layer\n","                self.fc = nn.Sequential(\n","                    # nn.Dropout(p=self.dropout_proportion),\n","                    nn.Linear(6 * self.c_out, 1),\n","                    nn.Sigmoid())\n","\n","        \n","        # Finally tackle the case of an LSTM on top of BERT\n","        elif self.classifier_type == \"LSTM\":\n","            if \"full\" not in self.bert_out_to_classify:\n","                # Once again, the input to the LSTM needs to be embeddings so it will always be the full hidden state output of BERT\n","                raise ValueError(\"Cannot have an LSTM classifier on top of BERT pooled output!\")\n","            \n","            # Set the number of features in the hidden state vectors output from the LSTM\n","            self.hidden_dim = LSTM_hidden_dim\n","\n","            # Instantiate the bidirectional LSTM\n","            self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, bidirectional=True)\n","            \n","            if self.batchnorm_on:\n","                # Instantiate the fully connected output layer\n","                self.hidden2label = nn.Sequential(\n","                    nn.BatchNorm1d(self.hidden_dim * 2),\n","                    nn.Dropout(p=self.dropout_proportion),\n","                    nn.Linear(self.hidden_dim * 2, 1),\n","                    nn.Sigmoid())\n","            else:\n","                 # Instantiate the fully connected output layer\n","                self.hidden2label = nn.Sequential(\n","                    # nn.Dropout(p=self.dropout_proportion),\n","                    nn.Linear(self.hidden_dim * 2, 1),\n","                    nn.Sigmoid())\n","\n","            # intialise the hidden state vector\n","            self.hidden = self.init_hidden()\n","\n","\n","    def init_hidden(self):\n","        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n","\n","                \n","    def forward(self, input_ids, attention_mask):\n","\n","        dic_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        if self.classifier_type == \"CNN\":\n","            X = dic_out['last_hidden_state'].permute(0, 2, 1)\n","\n","            # Apply Convolution layer 1\n","            x1 = self.conv_1(X)\n","            x1 = torch.relu(x1)\n","            x1 = self.pool_1(x1)\n","            \n","            # Apply Convolution layer 2\n","            x2 = self.conv_2(X)\n","            x2 = torch.relu((x2))\n","            x2 = self.pool_2(x2)\n","        \n","            # Apply Convolution layer 3\n","            x3 = self.conv_3(X)\n","            x3 = torch.relu(x3)\n","            x3 = self.pool_3(x3)\n","            \n","            # Apply Convolution layer 4\n","            x4 = self.conv_4(X)\n","            x4 = torch.relu(x4)\n","            x4 = self.pool_4(x4)\n","\n","            # Apply Convolution layer 5\n","            x5 = self.conv_5(X)\n","            x5 = torch.relu(x5)\n","            x5 = self.pool_5(x5)\n","\n","            # Apply Convolution layer 6\n","            x6 = self.conv_6(X)\n","            x6 = torch.relu(x6)\n","            x6 = self.pool_6(x6)\n","            \n","            # Concatenate outputs of convolutions into unique matrix\n","            union = torch.cat((x1, x2, x3, x4, x5, x6), 2)\n","            union = union.reshape(union.size(0), -1)\n","\n","            # The \"flattened\" vector is passed through a fully connected layer\n","            out = self.fc(union)\n","            out = out.flatten()\n","\n","\n","        elif self.classifier_type == \"FFNN\":\n","            if self.bert_out_to_classify == \"full\":\n","                X = dic_out['last_hidden_state']\n","            else:\n","                X = dic_out['pooler_output']\n","            current_batch_size = X.shape[0]\n","            X = X.view(current_batch_size, -1)\n","\n","            out = self.classifier(X)\n","            out = out.view(-1)\n","\n","\n","        elif self.classifier_type == \"LSTM\":\n","\n","            # Get the full last hidden state of BERT of shape (batch_size x sequence_length x embedding_dim) = (128 x 512 x 768)\n","            X = dic_out['last_hidden_state']\n","            # Change its shape to (sequence_length x batch_size x embedding_dim) = (512 x 128 x 768) because that is how the LSTM likes its input\n","            X = X.permute(1, 0, 2)\n","\n","            # Send X and the previous hidden state tuple through the LSTM and get: out_shape = (512 x 2*100) = (sequence_length x num_directions * hidden_dim) plus a new hidden state tuple\n","            lstm_out, hidden_out = self.lstm(X, self.hidden)\n","            # Detach the two new hidden state vectors because otherwise they are always connected to previous ones and gradients go deeper with each batch\n","            self.hidden = (hidden_out[0].detach(), hidden_out[1].detach())\n","\n","            # Send the output of the LSTM through the fully connected layer\n","            out = self.hidden2label(lstm_out[-1])\n","            out = out.view(-1)\n","            \n","        return out"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFFK_CwTI4tf"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPOYVP7nIuFS","executionInfo":{"status":"ok","timestamp":1628368501167,"user_tz":-60,"elapsed":5179,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"ef5588b9-540b-4b73-8bea-917cc0649d6c"},"source":["model = SentimentClassifier(device, BATCH_SIZE, BERT_OUTPUT_USED, CLASSIFIER_TYPE, DROPOUT_PROPORTION, MAX_LEN, MODEL_NAME, NUM_UNFROZEN_BERT_LAYERS,\n","                            NUMBER_NEURONS_LAYER_1, NUMBER_NEURONS_LAYER_2, NUMBER_NEURONS_LAYER_3, NUMBER_NEURONS_LAYER_4, BATCHNORM_ON, \n","                            CNN_CHANNELS_OUT, LSTM_HIDDEN_DIM)\n","model = model.to(device)\n","\n","# Print info on the model\n","print(model)\n","print()\n","model_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"The total number of trainable parameters in the classifier is: {}\".format(model_trainable_params))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["SentimentClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=768, out_features=1, bias=True)\n","    (1): Sigmoid()\n","  )\n",")\n","\n","The total number of trainable parameters in the classifier is: 57294337\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TRS2mVzvKnfp"},"source":["Checking the ouputs for a batch of inputs are as expected"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJBQxtt1JtWn","executionInfo":{"status":"ok","timestamp":1628368510133,"user_tz":-60,"elapsed":316,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"9af5d63e-f12c-42f2-f8a1-59ae0361e6c2"},"source":["# OPTIONAL\n","\n","demo_input_ids = train_batch['input_ids'].to(device)\n","demo_input_masks = train_batch['attention_mask'].to(device)\n","with torch.no_grad():\n","    demo_out = model(demo_input_ids, demo_input_masks)\n","    print(demo_out)\n","    print(demo_out.shape)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["tensor([0.5267, 0.5240, 0.5330, 0.4958, 0.5161, 0.5338, 0.5310, 0.5283, 0.5072,\n","        0.5148, 0.5071, 0.5319, 0.5326, 0.5167, 0.5290, 0.5081],\n","       device='cuda:0')\n","torch.Size([16])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CJSNhnb2KzwP"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"4camVhnSLBi2"},"source":["Set the hyperparameters needed for training"]},{"cell_type":"code","metadata":{"id":"se2WDKsRKzeU","executionInfo":{"status":"ok","timestamp":1628368640621,"user_tz":-60,"elapsed":325,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["LR = 0.0000008\n","EPOCHS = 7\n","OPTIMISER = \"AdamW\"   # in {SGD, AdamW}\n","WEIGHT_DECAY = 0.1"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZRS_NBuLFPT"},"source":["Create a function that yields evaluation metrics from a give confusion matrix"]},{"cell_type":"code","metadata":{"id":"xgzqmZfDLCQn","executionInfo":{"status":"ok","timestamp":1628368641682,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def get_metrics_from_conf_matrix(confusion_matrix: np.ndarray, print_on=False):\n","    '''\n","    Takes in a confusion matrix and deduces evaluation metrics based on it.\n","\n","    params: confusion_matrix: numpy array giving the confusion matrix of binary classification\n","            print_on: Boolean determining whether or not to print input confusion matrix and deduced matrices\n","    return: Various self-explanatory evaluation metrics\n","    '''\n","\n","    # Get TP, FP, FN, TN (with different names).\n","    true_asshole = confusion_matrix[1][1]\n","    false_asshole = confusion_matrix[0][1]\n","    true_sweetheart = confusion_matrix[0][0]\n","    false_sweetheart = confusion_matrix[1][0]\n","\n","    # Calculate\n","    accuracy = (true_asshole + true_sweetheart) / (true_sweetheart + true_asshole + false_asshole + false_sweetheart)\n","    asshole_detection_precision = true_asshole / (true_asshole + false_asshole)\n","    sweetheart_detection_precision = true_sweetheart / (true_sweetheart + false_sweetheart)\n","    asshole_detection_accuracy = true_asshole / (true_asshole + false_sweetheart)\n","    sweetheart_detection_accuracy = true_sweetheart / (true_sweetheart + false_asshole)\n","    f1_ass = 2 * ((asshole_detection_precision * asshole_detection_accuracy) / (asshole_detection_precision + asshole_detection_accuracy))\n","    f1_sweet = 2 * ((sweetheart_detection_precision * sweetheart_detection_accuracy) / (sweetheart_detection_precision + sweetheart_detection_accuracy))\n","\n","    # If print is on, print the input confusion matrix and its evaluation metrics\n","    if print_on:\n","        print(\"Here is the confusion matrix:\")\n","        print(confusion_matrix)\n","        print()\n","        print(\"Here are the metrics derived from the confusion matrix:\")\n","        print(\"recall wrt assholes =\", asshole_detection_accuracy)\n","        print(\"recall wrt sweethearts =\", sweetheart_detection_accuracy)\n","        print(\"precision wrt assholes =\", asshole_detection_precision)\n","        print(\"precision wrt sweethearts) =\", sweetheart_detection_precision)\n","        print()\n","        print(\"accuracy =\", accuracy)\n","        print(\"f1 wrt assholes =\", f1_ass)\n","        print(\"f1 wrt sweethearts =\", f1_sweet)\n","\n","    # Return various self explanatory evaluation metrics\n","    return(asshole_detection_accuracy, sweetheart_detection_accuracy,\n","           asshole_detection_precision, sweetheart_detection_precision,\n","           accuracy, f1_ass, f1_sweet)"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w26MQ9bELLnz"},"source":["Create a function that performs one epoch of training"]},{"cell_type":"code","metadata":{"id":"00GA0slYLMZ6","executionInfo":{"status":"ok","timestamp":1628368645109,"user_tz":-60,"elapsed":790,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def train_epoch(model, dataloader, optimiser, device, scheduler=None, print_on=False):\n","    '''\n","    Function that performs one epoch (one pass through each sample in the given loader) of training of the given model.\n","    '''\n","\n","    model = model.train()\n","\n","    # Create some things for storing and calculating training metrics on a per epoch level\n","    losses = []\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    # Create some lists for storing and calculating training metrics on a per some batches level\n","    many_batches_losses = []\n","    many_batches_accs = []\n","    for i, batch in enumerate(dataloader):\n","  \n","      # Isolate the numpy arrays from the current batch that are needed for training\n","      input_ids = batch['input_ids'].to(device)\n","      attention_mask = batch['attention_mask'].to(device)\n","      y = batch['targets'].float().to(device)\n","      w = batch['weights'].to(device)\n","\n","      # Pass the batch through the classifier (output layers)\n","      y_out = model(input_ids=input_ids, attention_mask=attention_mask)\n","      del input_ids, attention_mask\n","\n","      # Binarise output probs to predictions in {0, 1}\n","      y_preds = y_out.detach()\n","      y_preds = torch.where(y_preds > 0.5, 1, 0)\n","      correct_predictions += int(torch.sum(y_preds == y))\n","      total_predictions += len(y_preds)\n","      many_batches_accs.append(torch.sum(y_preds == y) / len(y_preds))\n","\n","      # Get the mean loss for the batch\n","      loss_fn = nn.BCELoss(weight=w, reduction=\"mean\").to(device)\n","      loss = loss_fn(y_out, y.float())\n","      losses.append(loss.item())\n","      many_batches_losses.append(loss.item())\n","      loss.backward()\n","      \n","      # Not sure why I am clipping the grad here. Apparently it helps prevent exploding gradients.\n","      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","      # Take an optimisation step\n","      optimiser.step()\n","      if scheduler:\n","        scheduler.step()\n","      optimiser.zero_grad()\n","\n","      torch.cuda.empty_cache()\n","      del y, w\n","\n","      if print_on:\n","      # Every so often print the current training accuracy\n","        if (i + 1) % 100 == 0:\n","          print(\"...Batch #{} : Training Loss={}, Training Accuracy={}\".format(i + 1, sum(many_batches_losses) / len(many_batches_losses), sum(many_batches_accs) / len(many_batches_accs)))\n","          many_batches_losses = []\n","          many_batches_accs = []\n","\n","    # Return the training accuracy and the mean training loss for the given epoch\n","    return correct_predictions / total_predictions, np.mean(losses)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G11_1CU9MnfJ"},"source":["Create a function that performs one epoch of evaluation"]},{"cell_type":"code","metadata":{"id":"pVO6zPQxMCFH","executionInfo":{"status":"ok","timestamp":1628368648111,"user_tz":-60,"elapsed":776,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def eval_model(model, dataloader, device, include_weights=True, print_conf_matr_on=True):\n","    '''\n","    Function that performs evaluation of the given model for one full pass of the samples in the give dataloader\n","    '''\n","\n","    model = model.eval()\n","\n","    losses = []\n","    # Instantiate the total confusion metric for the whole validation epoch\n","    total_conf_matr = np.array([[0, 0], [0, 0]])\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            y = batch['targets'].float().to(device)\n","            if include_weights:\n","                w = batch['weights'].to(device)\n","\n","            # Send the current batch through the model to get output probabilities\n","            y_out = model(input_ids=input_ids, attention_mask=attention_mask)\n","            del input_ids, attention_mask\n","\n","            # Turn the probabilities into binary predictions\n","            y_preds = y_out.detach()\n","            y_preds = torch.where(y_preds > 0.5, 1, 0)\n","\n","            # Get the confusion matrix for the current batch and add it to the total confusion matrix\n","            small_conf_matr = confusion_matrix(y.cpu(), y_preds.cpu())\n","            total_conf_matr += small_conf_matr\n","\n","            # Get the mean loss for the batch\n","            if include_weights:\n","                loss_fn = nn.BCELoss(weight=w, reduction=\"mean\").to(device)\n","            else:\n","                loss_fn = nn.BCELoss(reduction=\"mean\").to(device)\n","            loss = loss_fn(y_out, y.float())\n","            losses.append(loss.item())\n","\n","            torch.cuda.empty_cache()\n","            del y\n","\n","    # Get various metrics from the total confusion matrix\n","    asshole_recall, sweetheart_recall, asshole_precision, sweetheart_precision, accuracy, f1_ass, f1_sweet = get_metrics_from_conf_matrix(total_conf_matr, print_on=print_conf_matr_on)\n","\n","    # Get the average loss for the current evaluation epoch by averaging all batch-averaged losses.\n","    epoch_loss = np.mean(losses)\n","\n","    # Return the appropriate evaluation metrics\n","    return epoch_loss, accuracy, f1_ass, f1_sweet"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg9RntJKMu-5"},"source":["Perform many consecutive epochs of training and evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uT_Aapb9Mi6b","executionInfo":{"status":"ok","timestamp":1628405399650,"user_tz":-60,"elapsed":36747631,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"b50bd6e7-1138-4ab1-b406-b8ad035ec1e4"},"source":["# Record the start time so as to time the process\n","start_time = time.time()\n","\n","# Create the optimiser with its initial learning rate\n","if OPTIMISER == \"AdamW\":\n","    optimiser = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","elif OPTIMISER == \"SGD\":\n","    optimiser = optim.SGD(model.parameters(), lr=LR)\n","\n","# Create lists to store the six metrics to be plotted later\n","train_losses = []\n","train_accs = []\n","valid_accs = []\n","valid_reb_accs = []\n","valid_f1s_ass = []\n","valid_f1s_sweet = []\n","valid_losses = []\n","\n","# Perform 1 epoch of validation, store and report the relevant validation metrics\n","print(\"Validation prior to training:\")\n","val_loss, val_acc, val_f1_ass, val_f1_sweet = eval_model(model, valid_loader, device, include_weights=True, print_conf_matr_on=False)\n","valid_losses.append(val_loss)\n","valid_accs.append(val_acc)\n","valid_f1s_ass.append(val_f1_ass)\n","valid_f1s_sweet.append(val_f1_sweet)\n","print(\"Valid: loss {}, accuracy {}, f1_ass {}, f1_sweet {}\".format(val_loss, val_acc, val_f1_ass, val_f1_sweet))\n","print()\n","\n","# Perform 1 epoch of validation on the rebalanced validation dataset and store the validation accuracy\n","val_reb_loss, val_reb_acc, val_reb_f1_ass, val_reb_f1_sweet = eval_model(model, valid_reb_loader, device, include_weights=False, print_conf_matr_on=False)\n","valid_reb_accs.append(val_reb_acc)\n","\n","for epoch in range(EPOCHS):\n","    print(\"Epoch {} / {}\".format(epoch+1, EPOCHS))\n","\n","    # Perform 1 epoch of training, store and report the relevant training metrics\n","    train_acc, train_loss = train_epoch(model, train_loader, optimiser, device, print_on=True)\n","    train_losses.append(train_loss)\n","    train_accs.append(train_acc)\n","    print(\"Train: loss {}, accuracy {}\".format(train_loss, train_acc))\n","\n","    # Perform 1 epoch of validation, store and report the relevant validation metrics\n","    val_loss, val_acc, val_f1_ass, val_f1_sweet = eval_model(model, valid_loader, device, include_weights=True, print_conf_matr_on=False)\n","    valid_losses.append(val_loss)\n","    valid_accs.append(val_acc)\n","    valid_f1s_ass.append(val_f1_ass)\n","    valid_f1s_sweet.append(val_f1_sweet)\n","    print(\"Valid: loss {}, accuracy {}, f1_ass {}, f1_sweet {}\".format(val_loss, val_acc, val_f1_ass, val_f1_sweet))\n","\n","    # Perform 1 epoch of validation on the rebalanved validation dataset and store the validation accuracy\n","    val_reb_loss, val_reb_acc, val_reb_f1_ass, val_reb_f1_sweet = eval_model(model, valid_reb_loader, device, include_weights=False, print_conf_matr_on=False)\n","    valid_reb_accs.append(val_reb_acc)\n","\n","    print(100*\"#\")\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Validation prior to training:\n","Valid: loss 0.6966788931326433, accuracy 0.3370559038662487, f1_ass 0.4998768290880426, f1_sweet 0.01704270359252445\n","\n","Epoch 1 / 7\n","...Batch #100 : Training Loss=0.690797164440155, Training Accuracy=0.351874977350235\n","...Batch #200 : Training Loss=0.6980119204521179, Training Accuracy=0.3918749988079071\n","...Batch #300 : Training Loss=0.686118136048317, Training Accuracy=0.4481250047683716\n","...Batch #400 : Training Loss=0.6954477488994598, Training Accuracy=0.5099999904632568\n","...Batch #500 : Training Loss=0.6939221435785293, Training Accuracy=0.4962500035762787\n","...Batch #600 : Training Loss=0.6891137254238129, Training Accuracy=0.5043749809265137\n","...Batch #700 : Training Loss=0.6902959424257279, Training Accuracy=0.5768749713897705\n","...Batch #800 : Training Loss=0.6746432590484619, Training Accuracy=0.6668750047683716\n","...Batch #900 : Training Loss=0.6882867550849915, Training Accuracy=0.6181249618530273\n","...Batch #1000 : Training Loss=0.6886646777391434, Training Accuracy=0.59375\n","...Batch #1100 : Training Loss=0.6723229432106018, Training Accuracy=0.6274999976158142\n","...Batch #1200 : Training Loss=0.6691733410954476, Training Accuracy=0.6581249833106995\n","...Batch #1300 : Training Loss=0.6699880242347718, Training Accuracy=0.6368749737739563\n","...Batch #1400 : Training Loss=0.677045710682869, Training Accuracy=0.5924999713897705\n","...Batch #1500 : Training Loss=0.6566002050042152, Training Accuracy=0.65625\n","...Batch #1600 : Training Loss=0.6440426117181778, Training Accuracy=0.6587499976158142\n","...Batch #1700 : Training Loss=0.6268416592478752, Training Accuracy=0.6887499690055847\n","...Batch #1800 : Training Loss=0.6350921911001205, Training Accuracy=0.6637499928474426\n","...Batch #1900 : Training Loss=0.6426462724804878, Training Accuracy=0.6706249713897705\n","...Batch #2000 : Training Loss=0.6254591050744057, Training Accuracy=0.6806249618530273\n","...Batch #2100 : Training Loss=0.6442666301131248, Training Accuracy=0.6574999690055847\n","...Batch #2200 : Training Loss=0.6418043780326843, Training Accuracy=0.6556249856948853\n","...Batch #2300 : Training Loss=0.6262010517716408, Training Accuracy=0.6725000143051147\n","...Batch #2400 : Training Loss=0.6258279824256897, Training Accuracy=0.6506249904632568\n","...Batch #2500 : Training Loss=0.6236151766777038, Training Accuracy=0.675000011920929\n","...Batch #2600 : Training Loss=0.6144367039203644, Training Accuracy=0.6943749785423279\n","...Batch #2700 : Training Loss=0.6083421298861503, Training Accuracy=0.6956250071525574\n","...Batch #2800 : Training Loss=0.6075101009011269, Training Accuracy=0.6893749833106995\n","...Batch #2900 : Training Loss=0.6003904297947884, Training Accuracy=0.6956250071525574\n","...Batch #3000 : Training Loss=0.6056019577383995, Training Accuracy=0.6868749856948853\n","...Batch #3100 : Training Loss=0.6021054327487946, Training Accuracy=0.6937499642372131\n","...Batch #3200 : Training Loss=0.6187999391555786, Training Accuracy=0.6825000047683716\n","...Batch #3300 : Training Loss=0.5849303531646729, Training Accuracy=0.7131249904632568\n","...Batch #3400 : Training Loss=0.6053031486272812, Training Accuracy=0.7012499570846558\n","...Batch #3500 : Training Loss=0.5899668452143669, Training Accuracy=0.7012499570846558\n","...Batch #3600 : Training Loss=0.5745969548821449, Training Accuracy=0.73499995470047\n","...Batch #3700 : Training Loss=0.5885560420155526, Training Accuracy=0.7068749666213989\n","...Batch #3800 : Training Loss=0.5875651282072067, Training Accuracy=0.7106249928474426\n","...Batch #3900 : Training Loss=0.5820990845561027, Training Accuracy=0.7087500095367432\n","...Batch #4000 : Training Loss=0.5762576135993004, Training Accuracy=0.7174999713897705\n","...Batch #4100 : Training Loss=0.5847343096137047, Training Accuracy=0.7268750071525574\n","...Batch #4200 : Training Loss=0.5742166975140571, Training Accuracy=0.7293750047683716\n","...Batch #4300 : Training Loss=0.587885535955429, Training Accuracy=0.71937495470047\n","...Batch #4400 : Training Loss=0.5657085767388343, Training Accuracy=0.7306249737739563\n","...Batch #4500 : Training Loss=0.5626641702651978, Training Accuracy=0.7393749952316284\n","...Batch #4600 : Training Loss=0.5999945092201233, Training Accuracy=0.7093749642372131\n","...Batch #4700 : Training Loss=0.5796529573202133, Training Accuracy=0.7106249928474426\n","...Batch #4800 : Training Loss=0.5925719997286797, Training Accuracy=0.7137500047683716\n","...Batch #4900 : Training Loss=0.5964356836676598, Training Accuracy=0.6993749737739563\n","...Batch #5000 : Training Loss=0.5827959808707237, Training Accuracy=0.71875\n","...Batch #5100 : Training Loss=0.5806491661071778, Training Accuracy=0.7074999809265137\n","...Batch #5200 : Training Loss=0.5860482168197632, Training Accuracy=0.7112500071525574\n","...Batch #5300 : Training Loss=0.585898770391941, Training Accuracy=0.7168749570846558\n","...Batch #5400 : Training Loss=0.5486985790729523, Training Accuracy=0.7606250047683716\n","...Batch #5500 : Training Loss=0.5682088154554367, Training Accuracy=0.7268750071525574\n","...Batch #5600 : Training Loss=0.563900137245655, Training Accuracy=0.7337499856948853\n","...Batch #5700 : Training Loss=0.5625374099612236, Training Accuracy=0.7425000071525574\n","...Batch #5800 : Training Loss=0.6038772192597389, Training Accuracy=0.6962499618530273\n","...Batch #5900 : Training Loss=0.5780988946557045, Training Accuracy=0.7131249904632568\n","...Batch #6000 : Training Loss=0.5618564009666442, Training Accuracy=0.7381249666213989\n","...Batch #6100 : Training Loss=0.5711274081468583, Training Accuracy=0.7299999594688416\n","...Batch #6200 : Training Loss=0.5800959086418152, Training Accuracy=0.7181249856948853\n","...Batch #6300 : Training Loss=0.5682795605063439, Training Accuracy=0.7362499833106995\n","...Batch #6400 : Training Loss=0.5508225858211517, Training Accuracy=0.737500011920929\n","...Batch #6500 : Training Loss=0.5724724847078323, Training Accuracy=0.7256249785423279\n","...Batch #6600 : Training Loss=0.5554801431298256, Training Accuracy=0.7368749976158142\n","...Batch #6700 : Training Loss=0.5743353706598282, Training Accuracy=0.7212499976158142\n","...Batch #6800 : Training Loss=0.5731224808096885, Training Accuracy=0.7181249856948853\n","...Batch #6900 : Training Loss=0.5546123707294464, Training Accuracy=0.7268750071525574\n","...Batch #7000 : Training Loss=0.5373175269365311, Training Accuracy=0.7524999976158142\n","...Batch #7100 : Training Loss=0.5500198349356651, Training Accuracy=0.7387499809265137\n","...Batch #7200 : Training Loss=0.5503174981474876, Training Accuracy=0.7431249618530273\n","...Batch #7300 : Training Loss=0.5734616425633431, Training Accuracy=0.7299999594688416\n","...Batch #7400 : Training Loss=0.5453036433458328, Training Accuracy=0.7543749809265137\n","...Batch #7500 : Training Loss=0.5570874843001365, Training Accuracy=0.7400000095367432\n","...Batch #7600 : Training Loss=0.5475080478191375, Training Accuracy=0.7400000095367432\n","...Batch #7700 : Training Loss=0.5606757551431656, Training Accuracy=0.7268750071525574\n","...Batch #7800 : Training Loss=0.5488219147920609, Training Accuracy=0.7481249570846558\n","...Batch #7900 : Training Loss=0.5523194113373756, Training Accuracy=0.7425000071525574\n","...Batch #8000 : Training Loss=0.5401909103989602, Training Accuracy=0.7437499761581421\n","...Batch #8100 : Training Loss=0.5299183708429337, Training Accuracy=0.7618749737739563\n","...Batch #8200 : Training Loss=0.5323733830451965, Training Accuracy=0.7562499642372131\n","...Batch #8300 : Training Loss=0.5285340860486031, Training Accuracy=0.7618749737739563\n","...Batch #8400 : Training Loss=0.5408107790350914, Training Accuracy=0.7481249570846558\n","...Batch #8500 : Training Loss=0.5466773369908333, Training Accuracy=0.75062495470047\n","...Batch #8600 : Training Loss=0.5165767812728882, Training Accuracy=0.7631250023841858\n","...Batch #8700 : Training Loss=0.5498956164717674, Training Accuracy=0.7256249785423279\n","...Batch #8800 : Training Loss=0.5302709299325943, Training Accuracy=0.7437499761581421\n","...Batch #8900 : Training Loss=0.564565002322197, Training Accuracy=0.7274999618530273\n","...Batch #9000 : Training Loss=0.5435341373085976, Training Accuracy=0.7481249570846558\n","...Batch #9100 : Training Loss=0.5669744437932969, Training Accuracy=0.715624988079071\n","...Batch #9200 : Training Loss=0.541216318309307, Training Accuracy=0.7574999928474426\n","...Batch #9300 : Training Loss=0.5317287915945053, Training Accuracy=0.7649999856948853\n","...Batch #9400 : Training Loss=0.5479712784290314, Training Accuracy=0.7381249666213989\n","...Batch #9500 : Training Loss=0.5338924300670623, Training Accuracy=0.7587499618530273\n","...Batch #9600 : Training Loss=0.5189448916912078, Training Accuracy=0.7724999785423279\n","...Batch #9700 : Training Loss=0.541533916592598, Training Accuracy=0.746874988079071\n","...Batch #9800 : Training Loss=0.5639961993694306, Training Accuracy=0.7406249642372131\n","...Batch #9900 : Training Loss=0.547086666226387, Training Accuracy=0.7400000095367432\n","...Batch #10000 : Training Loss=0.5270982748270034, Training Accuracy=0.7637499570846558\n","...Batch #10100 : Training Loss=0.5558950757980347, Training Accuracy=0.7337499856948853\n","...Batch #10200 : Training Loss=0.521905187368393, Training Accuracy=0.7599999904632568\n","...Batch #10300 : Training Loss=0.5365921157598496, Training Accuracy=0.75\n","...Batch #10400 : Training Loss=0.5392574223876, Training Accuracy=0.7393749952316284\n","...Batch #10500 : Training Loss=0.507420372068882, Training Accuracy=0.7681249976158142\n","...Batch #10600 : Training Loss=0.5542308393120766, Training Accuracy=0.7306249737739563\n","...Batch #10700 : Training Loss=0.5412490576505661, Training Accuracy=0.7618749737739563\n","...Batch #10800 : Training Loss=0.5276850575208664, Training Accuracy=0.7518749833106995\n","...Batch #10900 : Training Loss=0.5314114981889725, Training Accuracy=0.7712500095367432\n","...Batch #11000 : Training Loss=0.5475761049985886, Training Accuracy=0.7387499809265137\n","...Batch #11100 : Training Loss=0.5230071824789048, Training Accuracy=0.7493749856948853\n","...Batch #11200 : Training Loss=0.5241468477249146, Training Accuracy=0.7599999904632568\n","...Batch #11300 : Training Loss=0.5294404470920563, Training Accuracy=0.7543749809265137\n","...Batch #11400 : Training Loss=0.5105557745695114, Training Accuracy=0.7737500071525574\n","...Batch #11500 : Training Loss=0.5315982028841972, Training Accuracy=0.7543749809265137\n","...Batch #11600 : Training Loss=0.5375798772275447, Training Accuracy=0.7481249570846558\n","...Batch #11700 : Training Loss=0.52349589407444, Training Accuracy=0.7381249666213989\n","...Batch #11800 : Training Loss=0.542784638851881, Training Accuracy=0.7512499690055847\n","...Batch #11900 : Training Loss=0.5294316720962524, Training Accuracy=0.7587499618530273\n","...Batch #12000 : Training Loss=0.5315844509005546, Training Accuracy=0.7524999976158142\n","...Batch #12100 : Training Loss=0.5333289808034897, Training Accuracy=0.75062495470047\n","...Batch #12200 : Training Loss=0.5158304166793823, Training Accuracy=0.762499988079071\n","...Batch #12300 : Training Loss=0.5344748704135418, Training Accuracy=0.7568749785423279\n","...Batch #12400 : Training Loss=0.5193935142457485, Training Accuracy=0.7556250095367432\n","...Batch #12500 : Training Loss=0.5323074884712696, Training Accuracy=0.7462499737739563\n","...Batch #12600 : Training Loss=0.5063146871328353, Training Accuracy=0.7568749785423279\n","...Batch #12700 : Training Loss=0.5156398834288121, Training Accuracy=0.7606250047683716\n","...Batch #12800 : Training Loss=0.5101880547404289, Training Accuracy=0.7674999833106995\n","...Batch #12900 : Training Loss=0.5202447798848152, Training Accuracy=0.7599999904632568\n","...Batch #13000 : Training Loss=0.5079779420793057, Training Accuracy=0.768750011920929\n","...Batch #13100 : Training Loss=0.4904018883407116, Training Accuracy=0.778124988079071\n","...Batch #13200 : Training Loss=0.5141659216582775, Training Accuracy=0.765625\n","...Batch #13300 : Training Loss=0.5205451537668705, Training Accuracy=0.7768749594688416\n","...Batch #13400 : Training Loss=0.49825767368078233, Training Accuracy=0.7731249928474426\n","...Batch #13500 : Training Loss=0.48995945900678634, Training Accuracy=0.78125\n","...Batch #13600 : Training Loss=0.529746915847063, Training Accuracy=0.75\n","...Batch #13700 : Training Loss=0.5029599459469318, Training Accuracy=0.7718749642372131\n","...Batch #13800 : Training Loss=0.49817006543278697, Training Accuracy=0.7774999737739563\n","...Batch #13900 : Training Loss=0.5025555554032326, Training Accuracy=0.7637499570846558\n","...Batch #14000 : Training Loss=0.5131523635983467, Training Accuracy=0.7606250047683716\n","...Batch #14100 : Training Loss=0.5057618889212608, Training Accuracy=0.768750011920929\n","...Batch #14200 : Training Loss=0.5001764933764935, Training Accuracy=0.7774999737739563\n","...Batch #14300 : Training Loss=0.5062654773890972, Training Accuracy=0.7599999904632568\n","...Batch #14400 : Training Loss=0.5062198841571808, Training Accuracy=0.7724999785423279\n","...Batch #14500 : Training Loss=0.5031148554384708, Training Accuracy=0.76624995470047\n","...Batch #14600 : Training Loss=0.5014154401421547, Training Accuracy=0.7693749666213989\n","...Batch #14700 : Training Loss=0.5066359429061413, Training Accuracy=0.7649999856948853\n","...Batch #14800 : Training Loss=0.5187242859601975, Training Accuracy=0.7581250071525574\n","...Batch #14900 : Training Loss=0.49359401166439054, Training Accuracy=0.7681249976158142\n","...Batch #15000 : Training Loss=0.5099859562516212, Training Accuracy=0.7437499761581421\n","...Batch #15100 : Training Loss=0.5186251735687256, Training Accuracy=0.7637499570846558\n","...Batch #15200 : Training Loss=0.4977636957168579, Training Accuracy=0.768750011920929\n","...Batch #15300 : Training Loss=0.5214344216883182, Training Accuracy=0.7537499666213989\n","...Batch #15400 : Training Loss=0.5185692030191421, Training Accuracy=0.7556250095367432\n","...Batch #15500 : Training Loss=0.5163199999928474, Training Accuracy=0.7487499713897705\n","...Batch #15600 : Training Loss=0.5004633508622647, Training Accuracy=0.7618749737739563\n","...Batch #15700 : Training Loss=0.49079602807760236, Training Accuracy=0.76624995470047\n","...Batch #15800 : Training Loss=0.5053289590775967, Training Accuracy=0.7512499690055847\n","...Batch #15900 : Training Loss=0.5138753432035447, Training Accuracy=0.7562499642372131\n","...Batch #16000 : Training Loss=0.5024959737062454, Training Accuracy=0.7712500095367432\n","...Batch #16100 : Training Loss=0.5145394125580788, Training Accuracy=0.7587499618530273\n","...Batch #16200 : Training Loss=0.5166405177116394, Training Accuracy=0.7599999904632568\n","...Batch #16300 : Training Loss=0.5179390731453896, Training Accuracy=0.7612499594688416\n","...Batch #16400 : Training Loss=0.5167344756424427, Training Accuracy=0.7612499594688416\n","...Batch #16500 : Training Loss=0.47219089046120644, Training Accuracy=0.7899999618530273\n","...Batch #16600 : Training Loss=0.5123798806965351, Training Accuracy=0.7631250023841858\n","...Batch #16700 : Training Loss=0.48294517293572425, Training Accuracy=0.78187495470047\n","...Batch #16800 : Training Loss=0.5066616430878639, Training Accuracy=0.7631250023841858\n","...Batch #16900 : Training Loss=0.4782137183845043, Training Accuracy=0.768750011920929\n","...Batch #17000 : Training Loss=0.4967366848886013, Training Accuracy=0.7699999809265137\n","...Batch #17100 : Training Loss=0.4988107745349407, Training Accuracy=0.7712500095367432\n","...Batch #17200 : Training Loss=0.5039635820686817, Training Accuracy=0.7581250071525574\n","...Batch #17300 : Training Loss=0.5054698678851127, Training Accuracy=0.7643749713897705\n","...Batch #17400 : Training Loss=0.47237013712525366, Training Accuracy=0.7856249809265137\n","...Batch #17500 : Training Loss=0.481147205978632, Training Accuracy=0.7868750095367432\n","...Batch #17600 : Training Loss=0.49451107531785965, Training Accuracy=0.7799999713897705\n","...Batch #17700 : Training Loss=0.49184329390525816, Training Accuracy=0.778124988079071\n","...Batch #17800 : Training Loss=0.49442887485027315, Training Accuracy=0.7756249904632568\n","...Batch #17900 : Training Loss=0.5198979258537293, Training Accuracy=0.7574999928474426\n","...Batch #18000 : Training Loss=0.496755385696888, Training Accuracy=0.7699999809265137\n","...Batch #18100 : Training Loss=0.5085334493219853, Training Accuracy=0.7524999976158142\n","...Batch #18200 : Training Loss=0.49408411800861357, Training Accuracy=0.7693749666213989\n","...Batch #18300 : Training Loss=0.4879702204465866, Training Accuracy=0.7774999737739563\n","...Batch #18400 : Training Loss=0.4892695342004299, Training Accuracy=0.78125\n","...Batch #18500 : Training Loss=0.4793718183040619, Training Accuracy=0.7749999761581421\n","...Batch #18600 : Training Loss=0.5045056261122227, Training Accuracy=0.7643749713897705\n","...Batch #18700 : Training Loss=0.48608787193894387, Training Accuracy=0.7774999737739563\n","...Batch #18800 : Training Loss=0.4719651462137699, Training Accuracy=0.7893750071525574\n","...Batch #18900 : Training Loss=0.5247508065402507, Training Accuracy=0.7556250095367432\n","...Batch #19000 : Training Loss=0.5018079063296318, Training Accuracy=0.7706249952316284\n","...Batch #19100 : Training Loss=0.49824096336960794, Training Accuracy=0.768750011920929\n","...Batch #19200 : Training Loss=0.4966831564903259, Training Accuracy=0.768750011920929\n","...Batch #19300 : Training Loss=0.4726272052526474, Training Accuracy=0.7806249856948853\n","...Batch #19400 : Training Loss=0.5062027165293693, Training Accuracy=0.7649999856948853\n","...Batch #19500 : Training Loss=0.4874680230021477, Training Accuracy=0.7849999666213989\n","...Batch #19600 : Training Loss=0.4897832962870598, Training Accuracy=0.7731249928474426\n","...Batch #19700 : Training Loss=0.518358973711729, Training Accuracy=0.7456249594688416\n","...Batch #19800 : Training Loss=0.47894079610705376, Training Accuracy=0.7862499952316284\n","...Batch #19900 : Training Loss=0.5112928459048272, Training Accuracy=0.7643749713897705\n","...Batch #20000 : Training Loss=0.49409599870443344, Training Accuracy=0.768750011920929\n","...Batch #20100 : Training Loss=0.47756235152482984, Training Accuracy=0.79749995470047\n","...Batch #20200 : Training Loss=0.4530153416097164, Training Accuracy=0.7962499856948853\n","...Batch #20300 : Training Loss=0.5042700351774693, Training Accuracy=0.7762500047683716\n","...Batch #20400 : Training Loss=0.49606285586953164, Training Accuracy=0.768750011920929\n","...Batch #20500 : Training Loss=0.48427956968545915, Training Accuracy=0.7768749594688416\n","...Batch #20600 : Training Loss=0.4977060368657112, Training Accuracy=0.7693749666213989\n","...Batch #20700 : Training Loss=0.47271319195628164, Training Accuracy=0.7749999761581421\n","...Batch #20800 : Training Loss=0.4754592272639275, Training Accuracy=0.7731249928474426\n","...Batch #20900 : Training Loss=0.49605332225561144, Training Accuracy=0.7568749785423279\n","...Batch #21000 : Training Loss=0.4767310819029808, Training Accuracy=0.7774999737739563\n","...Batch #21100 : Training Loss=0.48149575516581533, Training Accuracy=0.78187495470047\n","...Batch #21200 : Training Loss=0.48039901718497274, Training Accuracy=0.778124988079071\n","...Batch #21300 : Training Loss=0.4535683067142963, Training Accuracy=0.8081249594688416\n","...Batch #21400 : Training Loss=0.4751994776725769, Training Accuracy=0.7868750095367432\n","...Batch #21500 : Training Loss=0.5069713687896729, Training Accuracy=0.7568749785423279\n","...Batch #21600 : Training Loss=0.48362732052803037, Training Accuracy=0.78187495470047\n","...Batch #21700 : Training Loss=0.4800421042740345, Training Accuracy=0.7731249928474426\n","...Batch #21800 : Training Loss=0.4428637357056141, Training Accuracy=0.8025000095367432\n","...Batch #21900 : Training Loss=0.49380809560418126, Training Accuracy=0.7824999690055847\n","...Batch #22000 : Training Loss=0.5036237786710263, Training Accuracy=0.7693749666213989\n","...Batch #22100 : Training Loss=0.49671093344688416, Training Accuracy=0.765625\n","...Batch #22200 : Training Loss=0.4589779780805111, Training Accuracy=0.7856249809265137\n","...Batch #22300 : Training Loss=0.48856296703219415, Training Accuracy=0.78187495470047\n","...Batch #22400 : Training Loss=0.4731774510443211, Training Accuracy=0.7899999618530273\n","...Batch #22500 : Training Loss=0.49154462069272997, Training Accuracy=0.7749999761581421\n","...Batch #22600 : Training Loss=0.46294971615076064, Training Accuracy=0.7824999690055847\n","...Batch #22700 : Training Loss=0.49602032750844954, Training Accuracy=0.7806249856948853\n","...Batch #22800 : Training Loss=0.4540777519345284, Training Accuracy=0.7956249713897705\n","...Batch #22900 : Training Loss=0.4660401061177254, Training Accuracy=0.7806249856948853\n","...Batch #23000 : Training Loss=0.46237919077277184, Training Accuracy=0.7874999642372131\n","...Batch #23100 : Training Loss=0.47859580427408216, Training Accuracy=0.7868750095367432\n","...Batch #23200 : Training Loss=0.47331932216882705, Training Accuracy=0.7906249761581421\n","...Batch #23300 : Training Loss=0.48333608731627464, Training Accuracy=0.768750011920929\n","...Batch #23400 : Training Loss=0.4748311544954777, Training Accuracy=0.78187495470047\n","...Batch #23500 : Training Loss=0.4627737279236317, Training Accuracy=0.78187495470047\n","...Batch #23600 : Training Loss=0.4778000584244728, Training Accuracy=0.7793749570846558\n","...Batch #23700 : Training Loss=0.4701111949980259, Training Accuracy=0.7793749570846558\n","...Batch #23800 : Training Loss=0.4694418512284756, Training Accuracy=0.7837499976158142\n","...Batch #23900 : Training Loss=0.49116781026124956, Training Accuracy=0.7756249904632568\n","...Batch #24000 : Training Loss=0.458098528534174, Training Accuracy=0.7837499976158142\n","...Batch #24100 : Training Loss=0.4682058362662792, Training Accuracy=0.7756249904632568\n","...Batch #24200 : Training Loss=0.4796038556098938, Training Accuracy=0.7799999713897705\n","...Batch #24300 : Training Loss=0.4737429654598236, Training Accuracy=0.7949999570846558\n","...Batch #24400 : Training Loss=0.4644447039067745, Training Accuracy=0.7768749594688416\n","...Batch #24500 : Training Loss=0.5074123506247997, Training Accuracy=0.7668749690055847\n","...Batch #24600 : Training Loss=0.47815678343176843, Training Accuracy=0.7881249785423279\n","...Batch #24700 : Training Loss=0.49566435664892194, Training Accuracy=0.778124988079071\n","...Batch #24800 : Training Loss=0.4785969367623329, Training Accuracy=0.7768749594688416\n","...Batch #24900 : Training Loss=0.4654415909945965, Training Accuracy=0.7837499976158142\n","...Batch #25000 : Training Loss=0.4816314908862114, Training Accuracy=0.7893750071525574\n","...Batch #25100 : Training Loss=0.46945898473262787, Training Accuracy=0.7837499976158142\n","...Batch #25200 : Training Loss=0.4856648081541061, Training Accuracy=0.7706249952316284\n","...Batch #25300 : Training Loss=0.47104896858334544, Training Accuracy=0.7874999642372131\n","...Batch #25400 : Training Loss=0.4629658015072346, Training Accuracy=0.8062499761581421\n","...Batch #25500 : Training Loss=0.4603277763724327, Training Accuracy=0.7887499928474426\n","...Batch #25600 : Training Loss=0.4817521670460701, Training Accuracy=0.7887499928474426\n","...Batch #25700 : Training Loss=0.49155420660972593, Training Accuracy=0.7774999737739563\n","...Batch #25800 : Training Loss=0.4746816509962082, Training Accuracy=0.7856249809265137\n","...Batch #25900 : Training Loss=0.47040013656020163, Training Accuracy=0.784375011920929\n","...Batch #26000 : Training Loss=0.4477494986355305, Training Accuracy=0.7918750047683716\n","...Batch #26100 : Training Loss=0.4796413376927376, Training Accuracy=0.7849999666213989\n","...Batch #26200 : Training Loss=0.49634946659207346, Training Accuracy=0.7693749666213989\n","...Batch #26300 : Training Loss=0.4596071720123291, Training Accuracy=0.7831249833106995\n","...Batch #26400 : Training Loss=0.4424889975786209, Training Accuracy=0.7981249690055847\n","...Batch #26500 : Training Loss=0.48855281800031664, Training Accuracy=0.7724999785423279\n","...Batch #26600 : Training Loss=0.473173273652792, Training Accuracy=0.7906249761581421\n","...Batch #26700 : Training Loss=0.454572769254446, Training Accuracy=0.7987499833106995\n","...Batch #26800 : Training Loss=0.4499081811308861, Training Accuracy=0.8037499785423279\n","...Batch #26900 : Training Loss=0.44355226889252664, Training Accuracy=0.8043749928474426\n","...Batch #27000 : Training Loss=0.45985158696770667, Training Accuracy=0.7912499904632568\n","...Batch #27100 : Training Loss=0.45513865530490877, Training Accuracy=0.8087499737739563\n","...Batch #27200 : Training Loss=0.43507031112909317, Training Accuracy=0.8081249594688416\n","...Batch #27300 : Training Loss=0.46691870748996733, Training Accuracy=0.7881249785423279\n","...Batch #27400 : Training Loss=0.47313457146286964, Training Accuracy=0.7887499928474426\n","...Batch #27500 : Training Loss=0.47316230535507203, Training Accuracy=0.7874999642372131\n","...Batch #27600 : Training Loss=0.4588466295599937, Training Accuracy=0.796875\n","...Batch #27700 : Training Loss=0.4845729421079159, Training Accuracy=0.7899999618530273\n","...Batch #27800 : Training Loss=0.48490834251046183, Training Accuracy=0.78187495470047\n","...Batch #27900 : Training Loss=0.4400978194177151, Training Accuracy=0.8031249642372131\n","...Batch #28000 : Training Loss=0.48348210960626603, Training Accuracy=0.7824999690055847\n","...Batch #28100 : Training Loss=0.4781897708773613, Training Accuracy=0.78125\n","...Batch #28200 : Training Loss=0.4462889739871025, Training Accuracy=0.8087499737739563\n","...Batch #28300 : Training Loss=0.45383441135287284, Training Accuracy=0.7956249713897705\n","...Batch #28400 : Training Loss=0.4928866398334503, Training Accuracy=0.7649999856948853\n","...Batch #28500 : Training Loss=0.45323447152972224, Training Accuracy=0.8012499809265137\n","...Batch #28600 : Training Loss=0.4534806333482265, Training Accuracy=0.7924999594688416\n","...Batch #28700 : Training Loss=0.49174228772521017, Training Accuracy=0.7612499594688416\n","...Batch #28800 : Training Loss=0.47180205017328264, Training Accuracy=0.7893750071525574\n","...Batch #28900 : Training Loss=0.4584607082605362, Training Accuracy=0.7956249713897705\n","...Batch #29000 : Training Loss=0.4547196239233017, Training Accuracy=0.793749988079071\n","...Batch #29100 : Training Loss=0.4679438179731369, Training Accuracy=0.7874999642372131\n","...Batch #29200 : Training Loss=0.45278089463710786, Training Accuracy=0.7981249690055847\n","...Batch #29300 : Training Loss=0.4448273767530918, Training Accuracy=0.8050000071525574\n","...Batch #29400 : Training Loss=0.4667438377439976, Training Accuracy=0.796875\n","...Batch #29500 : Training Loss=0.46707565784454347, Training Accuracy=0.7912499904632568\n","...Batch #29600 : Training Loss=0.4766836978495121, Training Accuracy=0.7856249809265137\n","...Batch #29700 : Training Loss=0.45742424577474594, Training Accuracy=0.7981249690055847\n","...Batch #29800 : Training Loss=0.45844546422362326, Training Accuracy=0.7912499904632568\n","...Batch #29900 : Training Loss=0.4837198995053768, Training Accuracy=0.768750011920929\n","...Batch #30000 : Training Loss=0.47130808562040327, Training Accuracy=0.7762500047683716\n","...Batch #30100 : Training Loss=0.462321617603302, Training Accuracy=0.7981249690055847\n","...Batch #30200 : Training Loss=0.4624179570376873, Training Accuracy=0.7943750023841858\n","...Batch #30300 : Training Loss=0.43540227629244327, Training Accuracy=0.7987499833106995\n","...Batch #30400 : Training Loss=0.44813994228839876, Training Accuracy=0.7993749976158142\n","...Batch #30500 : Training Loss=0.46087017983198164, Training Accuracy=0.7899999618530273\n","...Batch #30600 : Training Loss=0.47702015966176986, Training Accuracy=0.7793749570846558\n","...Batch #30700 : Training Loss=0.4377946858108044, Training Accuracy=0.809374988079071\n","...Batch #30800 : Training Loss=0.45598127409815786, Training Accuracy=0.7906249761581421\n","...Batch #30900 : Training Loss=0.46315744206309317, Training Accuracy=0.7862499952316284\n","...Batch #31000 : Training Loss=0.44411918699741365, Training Accuracy=0.796875\n","...Batch #31100 : Training Loss=0.4673448427021503, Training Accuracy=0.7774999737739563\n","...Batch #31200 : Training Loss=0.46056902706623076, Training Accuracy=0.7837499976158142\n","...Batch #31300 : Training Loss=0.44534782752394675, Training Accuracy=0.8018749952316284\n","...Batch #31400 : Training Loss=0.4541760841012001, Training Accuracy=0.7999999523162842\n","...Batch #31500 : Training Loss=0.473064957857132, Training Accuracy=0.7856249809265137\n","...Batch #31600 : Training Loss=0.4714823167026043, Training Accuracy=0.78187495470047\n","...Batch #31700 : Training Loss=0.4564901115000248, Training Accuracy=0.7987499833106995\n","...Batch #31800 : Training Loss=0.4791522122919559, Training Accuracy=0.7631250023841858\n","...Batch #31900 : Training Loss=0.4324490550160408, Training Accuracy=0.8012499809265137\n","...Batch #32000 : Training Loss=0.4678852494060993, Training Accuracy=0.7943750023841858\n","...Batch #32100 : Training Loss=0.4390323819220066, Training Accuracy=0.8106249570846558\n","...Batch #32200 : Training Loss=0.46679738610982896, Training Accuracy=0.7981249690055847\n","...Batch #32300 : Training Loss=0.48515518993139267, Training Accuracy=0.784375011920929\n","...Batch #32400 : Training Loss=0.47162176355719565, Training Accuracy=0.7931249737739563\n","...Batch #32500 : Training Loss=0.4435528892278671, Training Accuracy=0.8006249666213989\n","...Batch #32600 : Training Loss=0.46031836673617366, Training Accuracy=0.8050000071525574\n","...Batch #32700 : Training Loss=0.44837392069399357, Training Accuracy=0.7956249713897705\n","...Batch #32800 : Training Loss=0.46282376974821093, Training Accuracy=0.7956249713897705\n","...Batch #32900 : Training Loss=0.4651050013303757, Training Accuracy=0.7874999642372131\n","...Batch #33000 : Training Loss=0.46427215203642846, Training Accuracy=0.7868750095367432\n","...Batch #33100 : Training Loss=0.4530211316049099, Training Accuracy=0.8037499785423279\n","...Batch #33200 : Training Loss=0.4488774389028549, Training Accuracy=0.7943750023841858\n","...Batch #33300 : Training Loss=0.449597227871418, Training Accuracy=0.8031249642372131\n","...Batch #33400 : Training Loss=0.4579964292794466, Training Accuracy=0.7931249737739563\n","...Batch #33500 : Training Loss=0.45235884338617327, Training Accuracy=0.8018749952316284\n","...Batch #33600 : Training Loss=0.4731989777088165, Training Accuracy=0.7881249785423279\n","...Batch #33700 : Training Loss=0.4521617567539215, Training Accuracy=0.7956249713897705\n","...Batch #33800 : Training Loss=0.4618330876529217, Training Accuracy=0.78187495470047\n","...Batch #33900 : Training Loss=0.45143203556537626, Training Accuracy=0.7881249785423279\n","...Batch #34000 : Training Loss=0.43828553140163423, Training Accuracy=0.7993749976158142\n","...Batch #34100 : Training Loss=0.4526530313491821, Training Accuracy=0.7999999523162842\n","...Batch #34200 : Training Loss=0.42145920522511005, Training Accuracy=0.8168749809265137\n","...Batch #34300 : Training Loss=0.44842593431472777, Training Accuracy=0.8025000095367432\n","...Batch #34400 : Training Loss=0.46243131831288337, Training Accuracy=0.78125\n","...Batch #34500 : Training Loss=0.4460890333354473, Training Accuracy=0.8087499737739563\n","...Batch #34600 : Training Loss=0.46066080704331397, Training Accuracy=0.8018749952316284\n","...Batch #34700 : Training Loss=0.4383885495364666, Training Accuracy=0.7881249785423279\n","...Batch #34800 : Training Loss=0.4364788433909416, Training Accuracy=0.7987499833106995\n","...Batch #34900 : Training Loss=0.45439025104045866, Training Accuracy=0.7987499833106995\n","...Batch #35000 : Training Loss=0.4610609187185764, Training Accuracy=0.784375011920929\n","...Batch #35100 : Training Loss=0.45894659757614137, Training Accuracy=0.8025000095367432\n","...Batch #35200 : Training Loss=0.4581368434429169, Training Accuracy=0.7949999570846558\n","...Batch #35300 : Training Loss=0.4330368511378765, Training Accuracy=0.8050000071525574\n","...Batch #35400 : Training Loss=0.4330848915874958, Training Accuracy=0.8018749952316284\n","...Batch #35500 : Training Loss=0.4422569054365158, Training Accuracy=0.8018749952316284\n","...Batch #35600 : Training Loss=0.45985588923096654, Training Accuracy=0.7862499952316284\n","...Batch #35700 : Training Loss=0.4471047988533974, Training Accuracy=0.8012499809265137\n","...Batch #35800 : Training Loss=0.46746187210083007, Training Accuracy=0.7962499856948853\n","...Batch #35900 : Training Loss=0.44756753340363503, Training Accuracy=0.8018749952316284\n","...Batch #36000 : Training Loss=0.42860930651426316, Training Accuracy=0.8187499642372131\n","...Batch #36100 : Training Loss=0.45103489339351654, Training Accuracy=0.7987499833106995\n","...Batch #36200 : Training Loss=0.45389796540141103, Training Accuracy=0.8075000047683716\n","...Batch #36300 : Training Loss=0.4280257107317448, Training Accuracy=0.8043749928474426\n","Train: loss 0.5117107550174573, accuracy 0.7569570384414586\n","Valid: loss 0.4283905494938699, accuracy 0.8145245559038663, f1_ass 0.734529818657693, f1_sweet 0.8574726488005622\n","####################################################################################################\n","Epoch 2 / 7\n","...Batch #100 : Training Loss=0.45637252047657967, Training Accuracy=0.7906249761581421\n","...Batch #200 : Training Loss=0.4524310001730919, Training Accuracy=0.7943750023841858\n","...Batch #300 : Training Loss=0.4241093153506517, Training Accuracy=0.8143749833106995\n","...Batch #400 : Training Loss=0.4405416452884674, Training Accuracy=0.7918750047683716\n","...Batch #500 : Training Loss=0.4600778369605541, Training Accuracy=0.793749988079071\n","...Batch #600 : Training Loss=0.4306335709989071, Training Accuracy=0.8062499761581421\n","...Batch #700 : Training Loss=0.4489596489071846, Training Accuracy=0.8025000095367432\n","...Batch #800 : Training Loss=0.43356897950172424, Training Accuracy=0.8012499809265137\n","...Batch #900 : Training Loss=0.4614892929792404, Training Accuracy=0.7899999618530273\n","...Batch #1000 : Training Loss=0.4547817349433899, Training Accuracy=0.778124988079071\n","...Batch #1100 : Training Loss=0.4301747015118599, Training Accuracy=0.8018749952316284\n","...Batch #1200 : Training Loss=0.43020200192928315, Training Accuracy=0.793749988079071\n","...Batch #1300 : Training Loss=0.43477200739085675, Training Accuracy=0.8231250047683716\n","...Batch #1400 : Training Loss=0.4624958065152168, Training Accuracy=0.7837499976158142\n","...Batch #1500 : Training Loss=0.4134157544374466, Training Accuracy=0.8181250095367432\n","...Batch #1600 : Training Loss=0.4224495443701744, Training Accuracy=0.809374988079071\n","...Batch #1700 : Training Loss=0.4501945053040981, Training Accuracy=0.809374988079071\n","...Batch #1800 : Training Loss=0.425282741189003, Training Accuracy=0.8062499761581421\n","...Batch #1900 : Training Loss=0.4468848641216755, Training Accuracy=0.7949999570846558\n","...Batch #2000 : Training Loss=0.4465839694440365, Training Accuracy=0.8050000071525574\n","...Batch #2100 : Training Loss=0.43416898712515833, Training Accuracy=0.7931249737739563\n","...Batch #2200 : Training Loss=0.43663234695792197, Training Accuracy=0.8006249666213989\n","...Batch #2300 : Training Loss=0.45560147643089294, Training Accuracy=0.7949999570846558\n","...Batch #2400 : Training Loss=0.42919697776436805, Training Accuracy=0.7981249690055847\n","...Batch #2500 : Training Loss=0.45069568932056425, Training Accuracy=0.7906249761581421\n","...Batch #2600 : Training Loss=0.4226648482680321, Training Accuracy=0.8106249570846558\n","...Batch #2700 : Training Loss=0.4452436371147633, Training Accuracy=0.8056249618530273\n","...Batch #2800 : Training Loss=0.45778404459357264, Training Accuracy=0.7918750047683716\n","...Batch #2900 : Training Loss=0.44644717648625376, Training Accuracy=0.7887499928474426\n","...Batch #3000 : Training Loss=0.41498411908745764, Training Accuracy=0.8137499690055847\n","...Batch #3100 : Training Loss=0.43493027552962304, Training Accuracy=0.8037499785423279\n","...Batch #3200 : Training Loss=0.45649080231785777, Training Accuracy=0.7856249809265137\n","...Batch #3300 : Training Loss=0.4282522715628147, Training Accuracy=0.8043749928474426\n","...Batch #3400 : Training Loss=0.4414628021419048, Training Accuracy=0.8068749904632568\n","...Batch #3500 : Training Loss=0.4225443413853645, Training Accuracy=0.8062499761581421\n","...Batch #3600 : Training Loss=0.4340191516280174, Training Accuracy=0.8106249570846558\n","...Batch #3700 : Training Loss=0.4122658835351467, Training Accuracy=0.8243749737739563\n","...Batch #3800 : Training Loss=0.42326207876205446, Training Accuracy=0.8112499713897705\n","...Batch #3900 : Training Loss=0.4376081193983555, Training Accuracy=0.8025000095367432\n","...Batch #4000 : Training Loss=0.4353206729888916, Training Accuracy=0.8037499785423279\n","...Batch #4100 : Training Loss=0.4374210199713707, Training Accuracy=0.8062499761581421\n","...Batch #4200 : Training Loss=0.43658857271075246, Training Accuracy=0.8112499713897705\n","...Batch #4300 : Training Loss=0.4352677774429321, Training Accuracy=0.8031249642372131\n","...Batch #4400 : Training Loss=0.3970536879450083, Training Accuracy=0.8299999833106995\n","...Batch #4500 : Training Loss=0.4092856840789318, Training Accuracy=0.8106249570846558\n","...Batch #4600 : Training Loss=0.45585487127304075, Training Accuracy=0.7962499856948853\n","...Batch #4700 : Training Loss=0.44149414464831355, Training Accuracy=0.7887499928474426\n","...Batch #4800 : Training Loss=0.46660107105970383, Training Accuracy=0.7868750095367432\n","...Batch #4900 : Training Loss=0.4648929481208324, Training Accuracy=0.7912499904632568\n","...Batch #5000 : Training Loss=0.46036618307232857, Training Accuracy=0.7906249761581421\n","...Batch #5100 : Training Loss=0.45953933089971544, Training Accuracy=0.7962499856948853\n","...Batch #5200 : Training Loss=0.43988916128873823, Training Accuracy=0.7999999523162842\n","...Batch #5300 : Training Loss=0.4528891743719578, Training Accuracy=0.7912499904632568\n","...Batch #5400 : Training Loss=0.3971327062696218, Training Accuracy=0.8268749713897705\n","...Batch #5500 : Training Loss=0.42911266326904296, Training Accuracy=0.8137499690055847\n","...Batch #5600 : Training Loss=0.43640717744827273, Training Accuracy=0.8149999976158142\n","...Batch #5700 : Training Loss=0.4243047492206097, Training Accuracy=0.8112499713897705\n","...Batch #5800 : Training Loss=0.4690324939787388, Training Accuracy=0.7893750071525574\n","...Batch #5900 : Training Loss=0.4421897013485432, Training Accuracy=0.7999999523162842\n","...Batch #6000 : Training Loss=0.4366465525329113, Training Accuracy=0.8156249523162842\n","...Batch #6100 : Training Loss=0.4479196618497372, Training Accuracy=0.7924999594688416\n","...Batch #6200 : Training Loss=0.4587826803326607, Training Accuracy=0.7949999570846558\n","...Batch #6300 : Training Loss=0.4339445625245571, Training Accuracy=0.7931249737739563\n","...Batch #6400 : Training Loss=0.42502899646759035, Training Accuracy=0.8037499785423279\n","...Batch #6500 : Training Loss=0.44204827457666396, Training Accuracy=0.79749995470047\n","...Batch #6600 : Training Loss=0.42215899504721166, Training Accuracy=0.809374988079071\n","...Batch #6700 : Training Loss=0.440627773553133, Training Accuracy=0.8137499690055847\n","...Batch #6800 : Training Loss=0.45108589857816694, Training Accuracy=0.7956249713897705\n","...Batch #6900 : Training Loss=0.42581899344921115, Training Accuracy=0.8018749952316284\n","...Batch #7000 : Training Loss=0.4116570593416691, Training Accuracy=0.8193749785423279\n","...Batch #7100 : Training Loss=0.41509547650814055, Training Accuracy=0.8224999904632568\n","...Batch #7200 : Training Loss=0.4341366766393185, Training Accuracy=0.8125\n","...Batch #7300 : Training Loss=0.4600888392329216, Training Accuracy=0.793749988079071\n","...Batch #7400 : Training Loss=0.41768559604883193, Training Accuracy=0.8149999976158142\n","...Batch #7500 : Training Loss=0.44205065742135047, Training Accuracy=0.8006249666213989\n","...Batch #7600 : Training Loss=0.44031446509063243, Training Accuracy=0.8062499761581421\n","...Batch #7700 : Training Loss=0.45162732899188995, Training Accuracy=0.7924999594688416\n","...Batch #7800 : Training Loss=0.4401209130138159, Training Accuracy=0.7962499856948853\n","...Batch #7900 : Training Loss=0.45186597436666487, Training Accuracy=0.79749995470047\n","...Batch #8000 : Training Loss=0.4207757584005594, Training Accuracy=0.8193749785423279\n","...Batch #8100 : Training Loss=0.41689752466976643, Training Accuracy=0.8118749856948853\n","...Batch #8200 : Training Loss=0.4312351055443287, Training Accuracy=0.8137499690055847\n","...Batch #8300 : Training Loss=0.42481589660048485, Training Accuracy=0.8199999928474426\n","...Batch #8400 : Training Loss=0.42244291335344314, Training Accuracy=0.8156249523162842\n","...Batch #8500 : Training Loss=0.4316410383582115, Training Accuracy=0.809374988079071\n","...Batch #8600 : Training Loss=0.40323676720261575, Training Accuracy=0.8368749618530273\n","...Batch #8700 : Training Loss=0.42251022785902026, Training Accuracy=0.8056249618530273\n","...Batch #8800 : Training Loss=0.4430038201063871, Training Accuracy=0.8037499785423279\n","...Batch #8900 : Training Loss=0.44173591405153273, Training Accuracy=0.8062499761581421\n","...Batch #9000 : Training Loss=0.4620641349256039, Training Accuracy=0.7887499928474426\n","...Batch #9100 : Training Loss=0.4518354318290949, Training Accuracy=0.78125\n","...Batch #9200 : Training Loss=0.44547533959150315, Training Accuracy=0.8056249618530273\n","...Batch #9300 : Training Loss=0.436689168214798, Training Accuracy=0.8068749904632568\n","...Batch #9400 : Training Loss=0.4224549002200365, Training Accuracy=0.8137499690055847\n","...Batch #9500 : Training Loss=0.444912089407444, Training Accuracy=0.8050000071525574\n","...Batch #9600 : Training Loss=0.4080481491982937, Training Accuracy=0.824999988079071\n","...Batch #9700 : Training Loss=0.4214310595393181, Training Accuracy=0.8068749904632568\n","...Batch #9800 : Training Loss=0.4510334870219231, Training Accuracy=0.8025000095367432\n","...Batch #9900 : Training Loss=0.42276650622487066, Training Accuracy=0.8081249594688416\n","...Batch #10000 : Training Loss=0.44022873044013977, Training Accuracy=0.8056249618530273\n","...Batch #10100 : Training Loss=0.4584313003718853, Training Accuracy=0.7943750023841858\n","...Batch #10200 : Training Loss=0.4238649728894234, Training Accuracy=0.8162499666213989\n","...Batch #10300 : Training Loss=0.44195654228329656, Training Accuracy=0.8012499809265137\n","...Batch #10400 : Training Loss=0.4285828831791878, Training Accuracy=0.8062499761581421\n","...Batch #10500 : Training Loss=0.40995276302099226, Training Accuracy=0.82874995470047\n","...Batch #10600 : Training Loss=0.45275974795222285, Training Accuracy=0.7987499833106995\n","...Batch #10700 : Training Loss=0.4356573525071144, Training Accuracy=0.809374988079071\n","...Batch #10800 : Training Loss=0.4375180593132973, Training Accuracy=0.8118749856948853\n","...Batch #10900 : Training Loss=0.41318607188761236, Training Accuracy=0.8187499642372131\n","...Batch #11000 : Training Loss=0.4728794427216053, Training Accuracy=0.793749988079071\n","...Batch #11100 : Training Loss=0.42468147203326223, Training Accuracy=0.8006249666213989\n","...Batch #11200 : Training Loss=0.4364113625884056, Training Accuracy=0.7943750023841858\n","...Batch #11300 : Training Loss=0.4436847938597202, Training Accuracy=0.79749995470047\n","...Batch #11400 : Training Loss=0.4168043828010559, Training Accuracy=0.8174999952316284\n","...Batch #11500 : Training Loss=0.4307509080320597, Training Accuracy=0.8174999952316284\n","...Batch #11600 : Training Loss=0.4377489114552736, Training Accuracy=0.8068749904632568\n","...Batch #11700 : Training Loss=0.4179927168786526, Training Accuracy=0.81312495470047\n","...Batch #11800 : Training Loss=0.46578364238142966, Training Accuracy=0.7787500023841858\n","...Batch #11900 : Training Loss=0.44175183191895484, Training Accuracy=0.8125\n","...Batch #12000 : Training Loss=0.4326450574398041, Training Accuracy=0.7987499833106995\n","...Batch #12100 : Training Loss=0.44002944201231003, Training Accuracy=0.8012499809265137\n","...Batch #12200 : Training Loss=0.43020924031734464, Training Accuracy=0.8087499737739563\n","...Batch #12300 : Training Loss=0.4545045939087868, Training Accuracy=0.7987499833106995\n","...Batch #12400 : Training Loss=0.41565809413790705, Training Accuracy=0.8193749785423279\n","...Batch #12500 : Training Loss=0.43723382130265237, Training Accuracy=0.8081249594688416\n","...Batch #12600 : Training Loss=0.4108497983217239, Training Accuracy=0.8174999952316284\n","...Batch #12700 : Training Loss=0.43343285359442235, Training Accuracy=0.8031249642372131\n","...Batch #12800 : Training Loss=0.42405314803123473, Training Accuracy=0.8181250095367432\n","...Batch #12900 : Training Loss=0.4380694855749607, Training Accuracy=0.8118749856948853\n","...Batch #13000 : Training Loss=0.43383492290973663, Training Accuracy=0.8062499761581421\n","...Batch #13100 : Training Loss=0.4178478042036295, Training Accuracy=0.8043749928474426\n","...Batch #13200 : Training Loss=0.44508045583963396, Training Accuracy=0.8050000071525574\n","...Batch #13300 : Training Loss=0.42476578295230866, Training Accuracy=0.8218749761581421\n","...Batch #13400 : Training Loss=0.3961586583405733, Training Accuracy=0.8318749666213989\n","...Batch #13500 : Training Loss=0.40019131943583486, Training Accuracy=0.8112499713897705\n","...Batch #13600 : Training Loss=0.4417642216384411, Training Accuracy=0.8062499761581421\n","...Batch #13700 : Training Loss=0.4285385990142822, Training Accuracy=0.8224999904632568\n","...Batch #13800 : Training Loss=0.4072654966264963, Training Accuracy=0.8237499594688416\n","...Batch #13900 : Training Loss=0.41917848512530326, Training Accuracy=0.8118749856948853\n","...Batch #14000 : Training Loss=0.43425713971257207, Training Accuracy=0.8043749928474426\n","...Batch #14100 : Training Loss=0.4090449665486813, Training Accuracy=0.8293749690055847\n","...Batch #14200 : Training Loss=0.41557747483253477, Training Accuracy=0.8043749928474426\n","...Batch #14300 : Training Loss=0.4011218908429146, Training Accuracy=0.8237499594688416\n","...Batch #14400 : Training Loss=0.4308656034618616, Training Accuracy=0.8187499642372131\n","...Batch #14500 : Training Loss=0.4217719340324402, Training Accuracy=0.8224999904632568\n","...Batch #14600 : Training Loss=0.4214506061375141, Training Accuracy=0.8149999976158142\n","...Batch #14700 : Training Loss=0.4379170489311218, Training Accuracy=0.796875\n","...Batch #14800 : Training Loss=0.42485312789678575, Training Accuracy=0.8006249666213989\n","...Batch #14900 : Training Loss=0.40341208860278127, Training Accuracy=0.8181250095367432\n","...Batch #15000 : Training Loss=0.43020757898688317, Training Accuracy=0.8031249642372131\n","...Batch #15100 : Training Loss=0.4140614801645279, Training Accuracy=0.8187499642372131\n","...Batch #15200 : Training Loss=0.4087489701807499, Training Accuracy=0.8231250047683716\n","...Batch #15300 : Training Loss=0.45549339830875396, Training Accuracy=0.7931249737739563\n","...Batch #15400 : Training Loss=0.42393179699778555, Training Accuracy=0.8087499737739563\n","...Batch #15500 : Training Loss=0.4264843738079071, Training Accuracy=0.8174999952316284\n","...Batch #15600 : Training Loss=0.42548342399299144, Training Accuracy=0.8100000023841858\n","...Batch #15700 : Training Loss=0.43171547681093214, Training Accuracy=0.8012499809265137\n","...Batch #15800 : Training Loss=0.4334136874973774, Training Accuracy=0.7987499833106995\n","...Batch #15900 : Training Loss=0.4276499669253826, Training Accuracy=0.8112499713897705\n","...Batch #16000 : Training Loss=0.4202696488797665, Training Accuracy=0.8168749809265137\n","...Batch #16100 : Training Loss=0.45755051732063295, Training Accuracy=0.7862499952316284\n","...Batch #16200 : Training Loss=0.4223869301378727, Training Accuracy=0.81312495470047\n","...Batch #16300 : Training Loss=0.44302320897579195, Training Accuracy=0.8081249594688416\n","...Batch #16400 : Training Loss=0.44407614290714265, Training Accuracy=0.8037499785423279\n","...Batch #16500 : Training Loss=0.3985660380870104, Training Accuracy=0.8268749713897705\n","...Batch #16600 : Training Loss=0.4223910737782717, Training Accuracy=0.81312495470047\n","...Batch #16700 : Training Loss=0.40509034633636476, Training Accuracy=0.8149999976158142\n","...Batch #16800 : Training Loss=0.4477906049787998, Training Accuracy=0.7962499856948853\n","...Batch #16900 : Training Loss=0.4028239367902279, Training Accuracy=0.8087499737739563\n","...Batch #17000 : Training Loss=0.4301027981936932, Training Accuracy=0.8081249594688416\n","...Batch #17100 : Training Loss=0.4216264259815216, Training Accuracy=0.8168749809265137\n","...Batch #17200 : Training Loss=0.4299393740296364, Training Accuracy=0.8056249618530273\n","...Batch #17300 : Training Loss=0.43438593968749045, Training Accuracy=0.8075000047683716\n","...Batch #17400 : Training Loss=0.4070185399055481, Training Accuracy=0.8112499713897705\n","...Batch #17500 : Training Loss=0.40654585070908067, Training Accuracy=0.8237499594688416\n","...Batch #17600 : Training Loss=0.4293086089938879, Training Accuracy=0.8112499713897705\n","...Batch #17700 : Training Loss=0.4126968991756439, Training Accuracy=0.8231250047683716\n","...Batch #17800 : Training Loss=0.41279468826949595, Training Accuracy=0.8212499618530273\n","...Batch #17900 : Training Loss=0.43792175643146036, Training Accuracy=0.8031249642372131\n","...Batch #18000 : Training Loss=0.42457494616508484, Training Accuracy=0.79749995470047\n","...Batch #18100 : Training Loss=0.42794554859399797, Training Accuracy=0.8050000071525574\n","...Batch #18200 : Training Loss=0.4204234430193901, Training Accuracy=0.8125\n","...Batch #18300 : Training Loss=0.4177739902585745, Training Accuracy=0.8174999952316284\n","...Batch #18400 : Training Loss=0.42540269494056704, Training Accuracy=0.8168749809265137\n","...Batch #18500 : Training Loss=0.40533909276127816, Training Accuracy=0.8237499594688416\n","...Batch #18600 : Training Loss=0.4238967201113701, Training Accuracy=0.8168749809265137\n","...Batch #18700 : Training Loss=0.4110812944173813, Training Accuracy=0.8112499713897705\n","...Batch #18800 : Training Loss=0.3992425928264856, Training Accuracy=0.824999988079071\n","...Batch #18900 : Training Loss=0.4564785224199295, Training Accuracy=0.7912499904632568\n","...Batch #19000 : Training Loss=0.4286008830368519, Training Accuracy=0.81312495470047\n","...Batch #19100 : Training Loss=0.4347627472877502, Training Accuracy=0.8050000071525574\n","...Batch #19200 : Training Loss=0.42490375474095343, Training Accuracy=0.8062499761581421\n","...Batch #19300 : Training Loss=0.40549150601029393, Training Accuracy=0.8149999976158142\n","...Batch #19400 : Training Loss=0.43029530480504036, Training Accuracy=0.8068749904632568\n","...Batch #19500 : Training Loss=0.41382754683494566, Training Accuracy=0.8187499642372131\n","...Batch #19600 : Training Loss=0.41212148472666743, Training Accuracy=0.8100000023841858\n","...Batch #19700 : Training Loss=0.44019407585263254, Training Accuracy=0.7931249737739563\n","...Batch #19800 : Training Loss=0.41485481917858125, Training Accuracy=0.8243749737739563\n","...Batch #19900 : Training Loss=0.43960100203752517, Training Accuracy=0.7999999523162842\n","...Batch #20000 : Training Loss=0.4424516969919205, Training Accuracy=0.8037499785423279\n","...Batch #20100 : Training Loss=0.4029587763547897, Training Accuracy=0.8212499618530273\n","...Batch #20200 : Training Loss=0.37913010381162165, Training Accuracy=0.8399999737739563\n","...Batch #20300 : Training Loss=0.42892097398638723, Training Accuracy=0.8100000023841858\n","...Batch #20400 : Training Loss=0.4221476087719202, Training Accuracy=0.8162499666213989\n","...Batch #20500 : Training Loss=0.4106551828980446, Training Accuracy=0.8174999952316284\n","...Batch #20600 : Training Loss=0.44320440709590914, Training Accuracy=0.79749995470047\n","...Batch #20700 : Training Loss=0.401530374288559, Training Accuracy=0.8143749833106995\n","...Batch #20800 : Training Loss=0.41452041149139407, Training Accuracy=0.8012499809265137\n","...Batch #20900 : Training Loss=0.43232636667788027, Training Accuracy=0.8118749856948853\n","...Batch #21000 : Training Loss=0.41206118538975717, Training Accuracy=0.8137499690055847\n","...Batch #21100 : Training Loss=0.42828869581222534, Training Accuracy=0.8162499666213989\n","...Batch #21200 : Training Loss=0.4110927488654852, Training Accuracy=0.8256250023841858\n","...Batch #21300 : Training Loss=0.3873895621299744, Training Accuracy=0.8381249904632568\n","...Batch #21400 : Training Loss=0.4078581327944994, Training Accuracy=0.8424999713897705\n","...Batch #21500 : Training Loss=0.4400738660991192, Training Accuracy=0.7943750023841858\n","...Batch #21600 : Training Loss=0.4177251781523228, Training Accuracy=0.8187499642372131\n","...Batch #21700 : Training Loss=0.4166551481187344, Training Accuracy=0.8218749761581421\n","...Batch #21800 : Training Loss=0.38531602777540686, Training Accuracy=0.8349999785423279\n","...Batch #21900 : Training Loss=0.4353054390847683, Training Accuracy=0.8100000023841858\n","...Batch #22000 : Training Loss=0.43250541791319846, Training Accuracy=0.8037499785423279\n","...Batch #22100 : Training Loss=0.4208034334331751, Training Accuracy=0.8056249618530273\n","...Batch #22200 : Training Loss=0.3967536996304989, Training Accuracy=0.8206250071525574\n","...Batch #22300 : Training Loss=0.4311156859993935, Training Accuracy=0.8106249570846558\n","...Batch #22400 : Training Loss=0.41164210632443426, Training Accuracy=0.8137499690055847\n","...Batch #22500 : Training Loss=0.4366660016775131, Training Accuracy=0.8118749856948853\n","...Batch #22600 : Training Loss=0.41521785497665403, Training Accuracy=0.8149999976158142\n","...Batch #22700 : Training Loss=0.42827872589230537, Training Accuracy=0.8137499690055847\n","...Batch #22800 : Training Loss=0.395391186773777, Training Accuracy=0.8237499594688416\n","...Batch #22900 : Training Loss=0.3892055616527796, Training Accuracy=0.8356249928474426\n","...Batch #23000 : Training Loss=0.3957752114534378, Training Accuracy=0.8062499761581421\n","...Batch #23100 : Training Loss=0.4174501006305218, Training Accuracy=0.8256250023841858\n","...Batch #23200 : Training Loss=0.4169305977225304, Training Accuracy=0.8174999952316284\n","...Batch #23300 : Training Loss=0.4182657754421234, Training Accuracy=0.809374988079071\n","...Batch #23400 : Training Loss=0.40951664358377454, Training Accuracy=0.8312499523162842\n","...Batch #23500 : Training Loss=0.4057349060475826, Training Accuracy=0.8212499618530273\n","...Batch #23600 : Training Loss=0.4326870334148407, Training Accuracy=0.8081249594688416\n","...Batch #23700 : Training Loss=0.4159544588625431, Training Accuracy=0.8181250095367432\n","...Batch #23800 : Training Loss=0.4248704884201288, Training Accuracy=0.809374988079071\n","...Batch #23900 : Training Loss=0.4391705839335918, Training Accuracy=0.809374988079071\n","...Batch #24000 : Training Loss=0.38389574371278284, Training Accuracy=0.8274999856948853\n","...Batch #24100 : Training Loss=0.40859936669468877, Training Accuracy=0.8187499642372131\n","...Batch #24200 : Training Loss=0.41406126573681834, Training Accuracy=0.8206250071525574\n","...Batch #24300 : Training Loss=0.40900366827845575, Training Accuracy=0.8168749809265137\n","...Batch #24400 : Training Loss=0.40611164551228285, Training Accuracy=0.8187499642372131\n","...Batch #24500 : Training Loss=0.44650159403681755, Training Accuracy=0.7993749976158142\n","...Batch #24600 : Training Loss=0.4337530875205994, Training Accuracy=0.81312495470047\n","...Batch #24700 : Training Loss=0.4420784622430801, Training Accuracy=0.8149999976158142\n","...Batch #24800 : Training Loss=0.42172144785523413, Training Accuracy=0.8193749785423279\n","...Batch #24900 : Training Loss=0.39356816112995147, Training Accuracy=0.8299999833106995\n","...Batch #25000 : Training Loss=0.4234616114199162, Training Accuracy=0.8149999976158142\n","...Batch #25100 : Training Loss=0.4131566655635834, Training Accuracy=0.8156249523162842\n","...Batch #25200 : Training Loss=0.43421126693487166, Training Accuracy=0.7931249737739563\n","...Batch #25300 : Training Loss=0.40251657471060753, Training Accuracy=0.8193749785423279\n","...Batch #25400 : Training Loss=0.3949339976161718, Training Accuracy=0.840624988079071\n","...Batch #25500 : Training Loss=0.4130850663036108, Training Accuracy=0.8162499666213989\n","...Batch #25600 : Training Loss=0.41595680922269823, Training Accuracy=0.8168749809265137\n","...Batch #25700 : Training Loss=0.4537480346858501, Training Accuracy=0.8031249642372131\n","...Batch #25800 : Training Loss=0.41533525958657264, Training Accuracy=0.8143749833106995\n","...Batch #25900 : Training Loss=0.41414224401116373, Training Accuracy=0.8125\n","...Batch #26000 : Training Loss=0.39953632801771166, Training Accuracy=0.8218749761581421\n","...Batch #26100 : Training Loss=0.42884315341711043, Training Accuracy=0.8118749856948853\n","...Batch #26200 : Training Loss=0.44694593697786333, Training Accuracy=0.8050000071525574\n","...Batch #26300 : Training Loss=0.3919960245490074, Training Accuracy=0.8274999856948853\n","...Batch #26400 : Training Loss=0.394664274007082, Training Accuracy=0.8274999856948853\n","...Batch #26500 : Training Loss=0.4279759056866169, Training Accuracy=0.8125\n","...Batch #26600 : Training Loss=0.4147369912266731, Training Accuracy=0.8187499642372131\n","...Batch #26700 : Training Loss=0.405118038803339, Training Accuracy=0.8137499690055847\n","...Batch #26800 : Training Loss=0.38443757086992264, Training Accuracy=0.82874995470047\n","...Batch #26900 : Training Loss=0.39027625605463984, Training Accuracy=0.8331249952316284\n","...Batch #27000 : Training Loss=0.4146915636956692, Training Accuracy=0.8243749737739563\n","...Batch #27100 : Training Loss=0.41191990323364736, Training Accuracy=0.8393749594688416\n","...Batch #27200 : Training Loss=0.3856902773678303, Training Accuracy=0.8349999785423279\n","...Batch #27300 : Training Loss=0.40456354796886446, Training Accuracy=0.8337500095367432\n","...Batch #27400 : Training Loss=0.43004099920392036, Training Accuracy=0.8137499690055847\n","...Batch #27500 : Training Loss=0.43121450178325177, Training Accuracy=0.8100000023841858\n","...Batch #27600 : Training Loss=0.41808616656810044, Training Accuracy=0.8243749737739563\n","...Batch #27700 : Training Loss=0.44063762985169885, Training Accuracy=0.8212499618530273\n","...Batch #27800 : Training Loss=0.42155263349413874, Training Accuracy=0.8137499690055847\n","...Batch #27900 : Training Loss=0.412517110183835, Training Accuracy=0.8174999952316284\n","...Batch #28000 : Training Loss=0.4336329184472561, Training Accuracy=0.8081249594688416\n","...Batch #28100 : Training Loss=0.4138334459066391, Training Accuracy=0.8174999952316284\n","...Batch #28200 : Training Loss=0.39062589205801485, Training Accuracy=0.8399999737739563\n","...Batch #28300 : Training Loss=0.39761726453900337, Training Accuracy=0.8256250023841858\n","...Batch #28400 : Training Loss=0.438031787276268, Training Accuracy=0.7931249737739563\n","...Batch #28500 : Training Loss=0.40778270080685614, Training Accuracy=0.8318749666213989\n","...Batch #28600 : Training Loss=0.39864134579896926, Training Accuracy=0.8343749642372131\n","...Batch #28700 : Training Loss=0.435104268938303, Training Accuracy=0.8012499809265137\n","...Batch #28800 : Training Loss=0.42291225895285606, Training Accuracy=0.8125\n","...Batch #28900 : Training Loss=0.39571080312132834, Training Accuracy=0.8349999785423279\n","...Batch #29000 : Training Loss=0.40621055975556375, Training Accuracy=0.8218749761581421\n","...Batch #29100 : Training Loss=0.4092519247531891, Training Accuracy=0.8187499642372131\n","...Batch #29200 : Training Loss=0.4117369616031647, Training Accuracy=0.8224999904632568\n","...Batch #29300 : Training Loss=0.40009439453482626, Training Accuracy=0.8168749809265137\n","...Batch #29400 : Training Loss=0.4089717599749565, Training Accuracy=0.8331249952316284\n","...Batch #29500 : Training Loss=0.40649327531456947, Training Accuracy=0.8174999952316284\n","...Batch #29600 : Training Loss=0.42495237588882445, Training Accuracy=0.8149999976158142\n","...Batch #29700 : Training Loss=0.4088970389217138, Training Accuracy=0.8268749713897705\n","...Batch #29800 : Training Loss=0.4076736437529325, Training Accuracy=0.8243749737739563\n","...Batch #29900 : Training Loss=0.43172961093485357, Training Accuracy=0.8031249642372131\n","...Batch #30000 : Training Loss=0.4162747332453728, Training Accuracy=0.8193749785423279\n","...Batch #30100 : Training Loss=0.4157930788397789, Training Accuracy=0.8193749785423279\n","...Batch #30200 : Training Loss=0.40982080906629564, Training Accuracy=0.8274999856948853\n","...Batch #30300 : Training Loss=0.3748008473962545, Training Accuracy=0.8362500071525574\n","...Batch #30400 : Training Loss=0.40838370747864244, Training Accuracy=0.8356249928474426\n","...Batch #30500 : Training Loss=0.4138739926367998, Training Accuracy=0.8199999928474426\n","...Batch #30600 : Training Loss=0.41335285812616346, Training Accuracy=0.8256250023841858\n","...Batch #30700 : Training Loss=0.39284716479480264, Training Accuracy=0.8293749690055847\n","...Batch #30800 : Training Loss=0.3899029628187418, Training Accuracy=0.8343749642372131\n","...Batch #30900 : Training Loss=0.41587980277836323, Training Accuracy=0.8206250071525574\n","...Batch #31000 : Training Loss=0.3939953799545765, Training Accuracy=0.8156249523162842\n","...Batch #31100 : Training Loss=0.4192772161960602, Training Accuracy=0.8075000047683716\n","...Batch #31200 : Training Loss=0.401074643433094, Training Accuracy=0.8181250095367432\n","...Batch #31300 : Training Loss=0.39554904237389565, Training Accuracy=0.8206250071525574\n","...Batch #31400 : Training Loss=0.3894198817014694, Training Accuracy=0.8293749690055847\n","...Batch #31500 : Training Loss=0.42472997374832633, Training Accuracy=0.8050000071525574\n","...Batch #31600 : Training Loss=0.421388638317585, Training Accuracy=0.8162499666213989\n","...Batch #31700 : Training Loss=0.40166295036673544, Training Accuracy=0.8193749785423279\n","...Batch #31800 : Training Loss=0.42832119271159175, Training Accuracy=0.8006249666213989\n","...Batch #31900 : Training Loss=0.40023652717471125, Training Accuracy=0.8262499570846558\n","...Batch #32000 : Training Loss=0.41049733862280846, Training Accuracy=0.81312495470047\n","...Batch #32100 : Training Loss=0.4060164225846529, Training Accuracy=0.8256250023841858\n","...Batch #32200 : Training Loss=0.41175048261880876, Training Accuracy=0.8262499570846558\n","...Batch #32300 : Training Loss=0.4234307775646448, Training Accuracy=0.8156249523162842\n","...Batch #32400 : Training Loss=0.4249137628078461, Training Accuracy=0.8206250071525574\n","...Batch #32500 : Training Loss=0.38619165688753126, Training Accuracy=0.8331249952316284\n","...Batch #32600 : Training Loss=0.4124611577391624, Training Accuracy=0.8168749809265137\n","...Batch #32700 : Training Loss=0.4009898738563061, Training Accuracy=0.8181250095367432\n","...Batch #32800 : Training Loss=0.41000102199614047, Training Accuracy=0.8218749761581421\n","...Batch #32900 : Training Loss=0.4054280576109886, Training Accuracy=0.8274999856948853\n","...Batch #33000 : Training Loss=0.40004714608192443, Training Accuracy=0.8312499523162842\n","...Batch #33100 : Training Loss=0.4086549350619316, Training Accuracy=0.8237499594688416\n","...Batch #33200 : Training Loss=0.3928757221996784, Training Accuracy=0.828125\n","...Batch #33300 : Training Loss=0.4084958669543266, Training Accuracy=0.8331249952316284\n","...Batch #33400 : Training Loss=0.414067554064095, Training Accuracy=0.8143749833106995\n","...Batch #33500 : Training Loss=0.40910069458186626, Training Accuracy=0.8212499618530273\n","...Batch #33600 : Training Loss=0.41803813993930816, Training Accuracy=0.8174999952316284\n","...Batch #33700 : Training Loss=0.3941863843053579, Training Accuracy=0.8199999928474426\n","...Batch #33800 : Training Loss=0.4139518554508686, Training Accuracy=0.8081249594688416\n","...Batch #33900 : Training Loss=0.4020562736690044, Training Accuracy=0.8174999952316284\n","...Batch #34000 : Training Loss=0.3941105116903782, Training Accuracy=0.8274999856948853\n","...Batch #34100 : Training Loss=0.3999892555177212, Training Accuracy=0.84375\n","...Batch #34200 : Training Loss=0.37545334197580815, Training Accuracy=0.8381249904632568\n","...Batch #34300 : Training Loss=0.409981495141983, Training Accuracy=0.8193749785423279\n","...Batch #34400 : Training Loss=0.4223805560171604, Training Accuracy=0.8062499761581421\n","...Batch #34500 : Training Loss=0.3924704203009605, Training Accuracy=0.8381249904632568\n","...Batch #34600 : Training Loss=0.40728387847542763, Training Accuracy=0.8299999833106995\n","...Batch #34700 : Training Loss=0.3910393962264061, Training Accuracy=0.8262499570846558\n","...Batch #34800 : Training Loss=0.3965618222951889, Training Accuracy=0.8193749785423279\n","...Batch #34900 : Training Loss=0.4103848724812269, Training Accuracy=0.8224999904632568\n","...Batch #35000 : Training Loss=0.41163193464279174, Training Accuracy=0.8112499713897705\n","...Batch #35100 : Training Loss=0.4172779208421707, Training Accuracy=0.8137499690055847\n","...Batch #35200 : Training Loss=0.4092048136889935, Training Accuracy=0.8181250095367432\n","...Batch #35300 : Training Loss=0.3949374447762966, Training Accuracy=0.8318749666213989\n","...Batch #35400 : Training Loss=0.3919093701243401, Training Accuracy=0.8349999785423279\n","...Batch #35500 : Training Loss=0.40506803408265113, Training Accuracy=0.8106249570846558\n","...Batch #35600 : Training Loss=0.40977233842015265, Training Accuracy=0.81312495470047\n","...Batch #35700 : Training Loss=0.41053264677524565, Training Accuracy=0.8312499523162842\n","...Batch #35800 : Training Loss=0.408608206808567, Training Accuracy=0.8262499570846558\n","...Batch #35900 : Training Loss=0.4034536587446928, Training Accuracy=0.8256250023841858\n","...Batch #36000 : Training Loss=0.38490584880113604, Training Accuracy=0.84375\n","...Batch #36100 : Training Loss=0.40419506177306175, Training Accuracy=0.8274999856948853\n","...Batch #36200 : Training Loss=0.4132760590314865, Training Accuracy=0.8256250023841858\n","...Batch #36300 : Training Loss=0.38573349185287953, Training Accuracy=0.8299999833106995\n","Train: loss 0.42249378572342927, accuracy 0.812901862995631\n","Valid: loss 0.3942220377985898, accuracy 0.8343129571577848, f1_ass 0.7638901814797581, f1_sweet 0.8723778862115802\n","####################################################################################################\n","Epoch 3 / 7\n","...Batch #100 : Training Loss=0.4233156082034111, Training Accuracy=0.8143749833106995\n","...Batch #200 : Training Loss=0.4020735569298267, Training Accuracy=0.8243749737739563\n","...Batch #300 : Training Loss=0.37599197559058667, Training Accuracy=0.8349999785423279\n","...Batch #400 : Training Loss=0.3966005850583315, Training Accuracy=0.8274999856948853\n","...Batch #500 : Training Loss=0.41709765009582045, Training Accuracy=0.8168749809265137\n","...Batch #600 : Training Loss=0.3944502423703671, Training Accuracy=0.8231250047683716\n","...Batch #700 : Training Loss=0.40685746282339097, Training Accuracy=0.8187499642372131\n","...Batch #800 : Training Loss=0.3909847109019756, Training Accuracy=0.8362500071525574\n","...Batch #900 : Training Loss=0.4207656392455101, Training Accuracy=0.8193749785423279\n","...Batch #1000 : Training Loss=0.40406480081379414, Training Accuracy=0.8143749833106995\n","...Batch #1100 : Training Loss=0.3909866467118263, Training Accuracy=0.8299999833106995\n","...Batch #1200 : Training Loss=0.38836210042238234, Training Accuracy=0.8237499594688416\n","...Batch #1300 : Training Loss=0.3961651346087456, Training Accuracy=0.82874995470047\n","...Batch #1400 : Training Loss=0.4157736463844776, Training Accuracy=0.81312495470047\n","...Batch #1500 : Training Loss=0.37064994268119333, Training Accuracy=0.8481249809265137\n","...Batch #1600 : Training Loss=0.384189987629652, Training Accuracy=0.8349999785423279\n","...Batch #1700 : Training Loss=0.4035351628065109, Training Accuracy=0.8243749737739563\n","...Batch #1800 : Training Loss=0.386103630438447, Training Accuracy=0.8299999833106995\n","...Batch #1900 : Training Loss=0.3925995972752571, Training Accuracy=0.824999988079071\n","...Batch #2000 : Training Loss=0.3918163301795721, Training Accuracy=0.8343749642372131\n","...Batch #2100 : Training Loss=0.3902927182614803, Training Accuracy=0.8237499594688416\n","...Batch #2200 : Training Loss=0.3899915182590485, Training Accuracy=0.82874995470047\n","...Batch #2300 : Training Loss=0.4026475627720356, Training Accuracy=0.8293749690055847\n","...Batch #2400 : Training Loss=0.3844353301823139, Training Accuracy=0.8306249976158142\n","...Batch #2500 : Training Loss=0.4004300197958946, Training Accuracy=0.8174999952316284\n","...Batch #2600 : Training Loss=0.38616874888539315, Training Accuracy=0.8243749737739563\n","...Batch #2700 : Training Loss=0.4116601663827896, Training Accuracy=0.8137499690055847\n","...Batch #2800 : Training Loss=0.406538860052824, Training Accuracy=0.8206250071525574\n","...Batch #2900 : Training Loss=0.4009133979678154, Training Accuracy=0.8268749713897705\n","...Batch #3000 : Training Loss=0.384177103638649, Training Accuracy=0.8337500095367432\n","...Batch #3100 : Training Loss=0.39220121301710603, Training Accuracy=0.8324999809265137\n","...Batch #3200 : Training Loss=0.40296371273696424, Training Accuracy=0.81312495470047\n","...Batch #3300 : Training Loss=0.3817580544948578, Training Accuracy=0.8381249904632568\n","...Batch #3400 : Training Loss=0.394290137141943, Training Accuracy=0.8262499570846558\n","...Batch #3500 : Training Loss=0.3954554394632578, Training Accuracy=0.8368749618530273\n","...Batch #3600 : Training Loss=0.3973175408691168, Training Accuracy=0.8318749666213989\n","...Batch #3700 : Training Loss=0.3821558616310358, Training Accuracy=0.8349999785423279\n","...Batch #3800 : Training Loss=0.38001911908388136, Training Accuracy=0.8368749618530273\n","...Batch #3900 : Training Loss=0.3914369641244411, Training Accuracy=0.8268749713897705\n","...Batch #4000 : Training Loss=0.4055899938195944, Training Accuracy=0.8181250095367432\n","...Batch #4100 : Training Loss=0.4118173961341381, Training Accuracy=0.8262499570846558\n","...Batch #4200 : Training Loss=0.3994443696737289, Training Accuracy=0.8274999856948853\n","...Batch #4300 : Training Loss=0.3921592545509338, Training Accuracy=0.8331249952316284\n","...Batch #4400 : Training Loss=0.36197061263024805, Training Accuracy=0.8468749523162842\n","...Batch #4500 : Training Loss=0.38148906648159026, Training Accuracy=0.8318749666213989\n","...Batch #4600 : Training Loss=0.41660334475338456, Training Accuracy=0.8243749737739563\n","...Batch #4700 : Training Loss=0.40492037624120714, Training Accuracy=0.809374988079071\n","...Batch #4800 : Training Loss=0.4239718183875084, Training Accuracy=0.8156249523162842\n","...Batch #4900 : Training Loss=0.42301151260733605, Training Accuracy=0.8075000047683716\n","...Batch #5000 : Training Loss=0.3949924419820309, Training Accuracy=0.8349999785423279\n","...Batch #5100 : Training Loss=0.4236767090857029, Training Accuracy=0.8106249570846558\n","...Batch #5200 : Training Loss=0.3900448220968247, Training Accuracy=0.8368749618530273\n","...Batch #5300 : Training Loss=0.4135742275416851, Training Accuracy=0.8149999976158142\n","...Batch #5400 : Training Loss=0.3554545099288225, Training Accuracy=0.8474999666213989\n","...Batch #5500 : Training Loss=0.3965658137202263, Training Accuracy=0.8243749737739563\n","...Batch #5600 : Training Loss=0.4081064260005951, Training Accuracy=0.8237499594688416\n","...Batch #5700 : Training Loss=0.3945686432719231, Training Accuracy=0.8312499523162842\n","...Batch #5800 : Training Loss=0.4246805823594332, Training Accuracy=0.8187499642372131\n","...Batch #5900 : Training Loss=0.3941562693566084, Training Accuracy=0.8181250095367432\n","...Batch #6000 : Training Loss=0.3933173022419214, Training Accuracy=0.82874995470047\n","...Batch #6100 : Training Loss=0.4122481197118759, Training Accuracy=0.8149999976158142\n","...Batch #6200 : Training Loss=0.4092311102896929, Training Accuracy=0.8224999904632568\n","...Batch #6300 : Training Loss=0.39305143505334855, Training Accuracy=0.8174999952316284\n","...Batch #6400 : Training Loss=0.39749163523316383, Training Accuracy=0.8231250047683716\n","...Batch #6500 : Training Loss=0.40335675828158857, Training Accuracy=0.8274999856948853\n","...Batch #6600 : Training Loss=0.3789736660569906, Training Accuracy=0.8324999809265137\n","...Batch #6700 : Training Loss=0.3988479847460985, Training Accuracy=0.8343749642372131\n","...Batch #6800 : Training Loss=0.40155582532286643, Training Accuracy=0.8312499523162842\n","...Batch #6900 : Training Loss=0.38805663645267485, Training Accuracy=0.8199999928474426\n","...Batch #7000 : Training Loss=0.3871645877510309, Training Accuracy=0.8349999785423279\n","...Batch #7100 : Training Loss=0.37639514029026033, Training Accuracy=0.8343749642372131\n","...Batch #7200 : Training Loss=0.38542303934693334, Training Accuracy=0.8306249976158142\n","...Batch #7300 : Training Loss=0.40593309499323366, Training Accuracy=0.8206250071525574\n","...Batch #7400 : Training Loss=0.379518963098526, Training Accuracy=0.8368749618530273\n","...Batch #7500 : Training Loss=0.41756892189383504, Training Accuracy=0.8112499713897705\n","...Batch #7600 : Training Loss=0.40312666654586793, Training Accuracy=0.8206250071525574\n","...Batch #7700 : Training Loss=0.4196423602104187, Training Accuracy=0.8106249570846558\n","...Batch #7800 : Training Loss=0.41134485200047494, Training Accuracy=0.824999988079071\n","...Batch #7900 : Training Loss=0.39687871739268304, Training Accuracy=0.8212499618530273\n","...Batch #8000 : Training Loss=0.3893388693034649, Training Accuracy=0.8318749666213989\n","...Batch #8100 : Training Loss=0.3868111570179462, Training Accuracy=0.8212499618530273\n","...Batch #8200 : Training Loss=0.401321968510747, Training Accuracy=0.8312499523162842\n","...Batch #8300 : Training Loss=0.3845207957923412, Training Accuracy=0.8299999833106995\n","...Batch #8400 : Training Loss=0.3891699332743883, Training Accuracy=0.8299999833106995\n","...Batch #8500 : Training Loss=0.3869193184375763, Training Accuracy=0.8256250023841858\n","...Batch #8600 : Training Loss=0.36868602767586706, Training Accuracy=0.8424999713897705\n","...Batch #8700 : Training Loss=0.3943560868501663, Training Accuracy=0.8243749737739563\n","...Batch #8800 : Training Loss=0.4151021125167608, Training Accuracy=0.8212499618530273\n","...Batch #8900 : Training Loss=0.3973997252434492, Training Accuracy=0.8224999904632568\n","...Batch #9000 : Training Loss=0.4084200125932693, Training Accuracy=0.8174999952316284\n","...Batch #9100 : Training Loss=0.4144651580601931, Training Accuracy=0.8149999976158142\n","...Batch #9200 : Training Loss=0.4071325954794884, Training Accuracy=0.8262499570846558\n","...Batch #9300 : Training Loss=0.3781512351334095, Training Accuracy=0.8462499976158142\n","...Batch #9400 : Training Loss=0.36956399597227574, Training Accuracy=0.8368749618530273\n","...Batch #9500 : Training Loss=0.4099550285935402, Training Accuracy=0.8299999833106995\n","...Batch #9600 : Training Loss=0.37118371315300464, Training Accuracy=0.84437495470047\n","...Batch #9700 : Training Loss=0.3842284397780895, Training Accuracy=0.8243749737739563\n","...Batch #9800 : Training Loss=0.42839221224188806, Training Accuracy=0.809374988079071\n","...Batch #9900 : Training Loss=0.39138787515461443, Training Accuracy=0.8224999904632568\n","...Batch #10000 : Training Loss=0.4045537971705198, Training Accuracy=0.8299999833106995\n","...Batch #10100 : Training Loss=0.41571147590875623, Training Accuracy=0.8193749785423279\n","...Batch #10200 : Training Loss=0.37706311166286466, Training Accuracy=0.8343749642372131\n","...Batch #10300 : Training Loss=0.40230558305978775, Training Accuracy=0.8299999833106995\n","...Batch #10400 : Training Loss=0.3870972585678101, Training Accuracy=0.8187499642372131\n","...Batch #10500 : Training Loss=0.3661355545371771, Training Accuracy=0.8474999666213989\n","...Batch #10600 : Training Loss=0.41688541293144227, Training Accuracy=0.8206250071525574\n","...Batch #10700 : Training Loss=0.40069086782634256, Training Accuracy=0.8212499618530273\n","...Batch #10800 : Training Loss=0.393385970890522, Training Accuracy=0.824999988079071\n","...Batch #10900 : Training Loss=0.3716843394935131, Training Accuracy=0.8393749594688416\n","...Batch #11000 : Training Loss=0.4303581906855106, Training Accuracy=0.8075000047683716\n","...Batch #11100 : Training Loss=0.39041690558195113, Training Accuracy=0.8149999976158142\n","...Batch #11200 : Training Loss=0.3915323753654957, Training Accuracy=0.8212499618530273\n","...Batch #11300 : Training Loss=0.4028013510257006, Training Accuracy=0.8168749809265137\n","...Batch #11400 : Training Loss=0.40195699721574785, Training Accuracy=0.8337500095367432\n","...Batch #11500 : Training Loss=0.3944983995705843, Training Accuracy=0.8268749713897705\n","...Batch #11600 : Training Loss=0.4081140045076609, Training Accuracy=0.8274999856948853\n","...Batch #11700 : Training Loss=0.3785352039337158, Training Accuracy=0.8324999809265137\n","...Batch #11800 : Training Loss=0.43940015584230424, Training Accuracy=0.8012499809265137\n","...Batch #11900 : Training Loss=0.4140369862318039, Training Accuracy=0.8118749856948853\n","...Batch #12000 : Training Loss=0.39902695044875147, Training Accuracy=0.8237499594688416\n","...Batch #12100 : Training Loss=0.409907997995615, Training Accuracy=0.8256250023841858\n","...Batch #12200 : Training Loss=0.3940889088064432, Training Accuracy=0.8256250023841858\n","...Batch #12300 : Training Loss=0.4186695160716772, Training Accuracy=0.8262499570846558\n","...Batch #12400 : Training Loss=0.38234970390796663, Training Accuracy=0.8387500047683716\n","...Batch #12500 : Training Loss=0.41254505135118963, Training Accuracy=0.8299999833106995\n","...Batch #12600 : Training Loss=0.3761699962615967, Training Accuracy=0.8387500047683716\n","...Batch #12700 : Training Loss=0.4163019558787346, Training Accuracy=0.7999999523162842\n","...Batch #12800 : Training Loss=0.39916254609823226, Training Accuracy=0.8324999809265137\n","...Batch #12900 : Training Loss=0.4000424501299858, Training Accuracy=0.82874995470047\n","...Batch #13000 : Training Loss=0.39616797745227816, Training Accuracy=0.8293749690055847\n","...Batch #13100 : Training Loss=0.37345531694591044, Training Accuracy=0.840624988079071\n","...Batch #13200 : Training Loss=0.3997944571822882, Training Accuracy=0.82874995470047\n","...Batch #13300 : Training Loss=0.40238682582974433, Training Accuracy=0.8368749618530273\n","...Batch #13400 : Training Loss=0.3824978116154671, Training Accuracy=0.8381249904632568\n","...Batch #13500 : Training Loss=0.36894093185663224, Training Accuracy=0.84375\n","...Batch #13600 : Training Loss=0.39672918163239956, Training Accuracy=0.82874995470047\n","...Batch #13700 : Training Loss=0.3857093419134617, Training Accuracy=0.8337500095367432\n","...Batch #13800 : Training Loss=0.3786480434238911, Training Accuracy=0.8487499952316284\n","...Batch #13900 : Training Loss=0.3863261416554451, Training Accuracy=0.8231250047683716\n","...Batch #14000 : Training Loss=0.4013860930502415, Training Accuracy=0.8274999856948853\n","...Batch #14100 : Training Loss=0.3677678383141756, Training Accuracy=0.8468749523162842\n","...Batch #14200 : Training Loss=0.36735159330070016, Training Accuracy=0.8293749690055847\n","...Batch #14300 : Training Loss=0.3919136607646942, Training Accuracy=0.8237499594688416\n","...Batch #14400 : Training Loss=0.38647469464689493, Training Accuracy=0.8256250023841858\n","...Batch #14500 : Training Loss=0.3857814884930849, Training Accuracy=0.8374999761581421\n","...Batch #14600 : Training Loss=0.3881385938823223, Training Accuracy=0.8299999833106995\n","...Batch #14700 : Training Loss=0.3919606812298298, Training Accuracy=0.8262499570846558\n","...Batch #14800 : Training Loss=0.393783066496253, Training Accuracy=0.8231250047683716\n","...Batch #14900 : Training Loss=0.38372637026011946, Training Accuracy=0.8243749737739563\n","...Batch #15000 : Training Loss=0.3985249648243189, Training Accuracy=0.8193749785423279\n","...Batch #15100 : Training Loss=0.3837930220365524, Training Accuracy=0.8368749618530273\n","...Batch #15200 : Training Loss=0.38498816788196566, Training Accuracy=0.8318749666213989\n","...Batch #15300 : Training Loss=0.41489456340670583, Training Accuracy=0.8193749785423279\n","...Batch #15400 : Training Loss=0.40561477586627004, Training Accuracy=0.8274999856948853\n","...Batch #15500 : Training Loss=0.3973756703734398, Training Accuracy=0.8306249976158142\n","...Batch #15600 : Training Loss=0.38424619659781456, Training Accuracy=0.8387500047683716\n","...Batch #15700 : Training Loss=0.3884877339750528, Training Accuracy=0.8268749713897705\n","...Batch #15800 : Training Loss=0.40323108166456223, Training Accuracy=0.8181250095367432\n","...Batch #15900 : Training Loss=0.39282257840037343, Training Accuracy=0.8293749690055847\n","...Batch #16000 : Training Loss=0.3744794575870037, Training Accuracy=0.8362500071525574\n","...Batch #16100 : Training Loss=0.41930645316839216, Training Accuracy=0.8125\n","...Batch #16200 : Training Loss=0.38744669944047927, Training Accuracy=0.8399999737739563\n","...Batch #16300 : Training Loss=0.4168911437690258, Training Accuracy=0.8212499618530273\n","...Batch #16400 : Training Loss=0.4028938892483711, Training Accuracy=0.8162499666213989\n","...Batch #16500 : Training Loss=0.3802733527868986, Training Accuracy=0.8431249856948853\n","...Batch #16600 : Training Loss=0.3980313968658447, Training Accuracy=0.8331249952316284\n","...Batch #16700 : Training Loss=0.3775330248475075, Training Accuracy=0.8399999737739563\n","...Batch #16800 : Training Loss=0.41806136690080165, Training Accuracy=0.8212499618530273\n","...Batch #16900 : Training Loss=0.3759562100842595, Training Accuracy=0.828125\n","...Batch #17000 : Training Loss=0.4106968620419502, Training Accuracy=0.8231250047683716\n","...Batch #17100 : Training Loss=0.3854230853915215, Training Accuracy=0.8362500071525574\n","...Batch #17200 : Training Loss=0.3986935679614544, Training Accuracy=0.8299999833106995\n","...Batch #17300 : Training Loss=0.38901362016797064, Training Accuracy=0.8312499523162842\n","...Batch #17400 : Training Loss=0.3676801381260157, Training Accuracy=0.8381249904632568\n","...Batch #17500 : Training Loss=0.3675312802195549, Training Accuracy=0.84437495470047\n","...Batch #17600 : Training Loss=0.3988716834038496, Training Accuracy=0.82874995470047\n","...Batch #17700 : Training Loss=0.3823534293472767, Training Accuracy=0.8356249928474426\n","...Batch #17800 : Training Loss=0.3590626635402441, Training Accuracy=0.84375\n","...Batch #17900 : Training Loss=0.40570270344614984, Training Accuracy=0.82874995470047\n","...Batch #18000 : Training Loss=0.39273218885064126, Training Accuracy=0.8187499642372131\n","...Batch #18100 : Training Loss=0.40725323334336283, Training Accuracy=0.8187499642372131\n","...Batch #18200 : Training Loss=0.40021876893937586, Training Accuracy=0.8187499642372131\n","...Batch #18300 : Training Loss=0.3805956730991602, Training Accuracy=0.8262499570846558\n","...Batch #18400 : Training Loss=0.3975762904435396, Training Accuracy=0.8337500095367432\n","...Batch #18500 : Training Loss=0.38146222434937954, Training Accuracy=0.8343749642372131\n","...Batch #18600 : Training Loss=0.3970023787021637, Training Accuracy=0.8293749690055847\n","...Batch #18700 : Training Loss=0.36416563965380194, Training Accuracy=0.84375\n","...Batch #18800 : Training Loss=0.369187383428216, Training Accuracy=0.8399999737739563\n","...Batch #18900 : Training Loss=0.4302466489374638, Training Accuracy=0.8050000071525574\n","...Batch #19000 : Training Loss=0.39827309012413026, Training Accuracy=0.8306249976158142\n","...Batch #19100 : Training Loss=0.4132830796390772, Training Accuracy=0.8125\n","...Batch #19200 : Training Loss=0.3902791254222393, Training Accuracy=0.8262499570846558\n","...Batch #19300 : Training Loss=0.37892810493707657, Training Accuracy=0.8337500095367432\n","...Batch #19400 : Training Loss=0.4106308925896883, Training Accuracy=0.8293749690055847\n","...Batch #19500 : Training Loss=0.3834900495409965, Training Accuracy=0.8418749570846558\n","...Batch #19600 : Training Loss=0.37589640624821186, Training Accuracy=0.8362500071525574\n","...Batch #19700 : Training Loss=0.4030872029066086, Training Accuracy=0.8206250071525574\n","...Batch #19800 : Training Loss=0.3958801919594407, Training Accuracy=0.8331249952316284\n","...Batch #19900 : Training Loss=0.412405833825469, Training Accuracy=0.8125\n","...Batch #20000 : Training Loss=0.40672158256173135, Training Accuracy=0.8212499618530273\n","...Batch #20100 : Training Loss=0.3705486808717251, Training Accuracy=0.84375\n","...Batch #20200 : Training Loss=0.35275322176516055, Training Accuracy=0.8556249737739563\n","...Batch #20300 : Training Loss=0.3989138588309288, Training Accuracy=0.8312499523162842\n","...Batch #20400 : Training Loss=0.40069050788879396, Training Accuracy=0.8318749666213989\n","...Batch #20500 : Training Loss=0.37776087068021297, Training Accuracy=0.8362500071525574\n","...Batch #20600 : Training Loss=0.4121674124896526, Training Accuracy=0.8106249570846558\n","...Batch #20700 : Training Loss=0.36621768698096274, Training Accuracy=0.8368749618530273\n","...Batch #20800 : Training Loss=0.3745730245858431, Training Accuracy=0.8349999785423279\n","...Batch #20900 : Training Loss=0.4033665116131306, Training Accuracy=0.8318749666213989\n","...Batch #21000 : Training Loss=0.38633316025137904, Training Accuracy=0.828125\n","...Batch #21100 : Training Loss=0.38813447803258894, Training Accuracy=0.8381249904632568\n","...Batch #21200 : Training Loss=0.37365271933376787, Training Accuracy=0.8412500023841858\n","...Batch #21300 : Training Loss=0.35485802121460436, Training Accuracy=0.8468749523162842\n","...Batch #21400 : Training Loss=0.3799869824945927, Training Accuracy=0.8487499952316284\n","...Batch #21500 : Training Loss=0.39977383218705653, Training Accuracy=0.8174999952316284\n","...Batch #21600 : Training Loss=0.39335500344634056, Training Accuracy=0.8262499570846558\n","...Batch #21700 : Training Loss=0.37636302940547467, Training Accuracy=0.8337500095367432\n","...Batch #21800 : Training Loss=0.35316633380949497, Training Accuracy=0.8568750023841858\n","...Batch #21900 : Training Loss=0.40508424818515776, Training Accuracy=0.8299999833106995\n","...Batch #22000 : Training Loss=0.3888964320719242, Training Accuracy=0.8318749666213989\n","...Batch #22100 : Training Loss=0.3976707500219345, Training Accuracy=0.8231250047683716\n","...Batch #22200 : Training Loss=0.3753060121834278, Training Accuracy=0.8349999785423279\n","...Batch #22300 : Training Loss=0.40257669135928154, Training Accuracy=0.8349999785423279\n","...Batch #22400 : Training Loss=0.37486334688961503, Training Accuracy=0.8349999785423279\n","...Batch #22500 : Training Loss=0.41942408740520476, Training Accuracy=0.8143749833106995\n","...Batch #22600 : Training Loss=0.37842262342572214, Training Accuracy=0.8399999737739563\n","...Batch #22700 : Training Loss=0.4002288316935301, Training Accuracy=0.8274999856948853\n","...Batch #22800 : Training Loss=0.3623883850872517, Training Accuracy=0.8399999737739563\n","...Batch #22900 : Training Loss=0.36139126859605314, Training Accuracy=0.8368749618530273\n","...Batch #23000 : Training Loss=0.3804747451096773, Training Accuracy=0.824999988079071\n","...Batch #23100 : Training Loss=0.3807638593763113, Training Accuracy=0.8412500023841858\n","...Batch #23200 : Training Loss=0.3939402426034212, Training Accuracy=0.8387500047683716\n","...Batch #23300 : Training Loss=0.3719738244265318, Training Accuracy=0.8418749570846558\n","...Batch #23400 : Training Loss=0.371383158788085, Training Accuracy=0.840624988079071\n","...Batch #23500 : Training Loss=0.36943559810519216, Training Accuracy=0.84375\n","...Batch #23600 : Training Loss=0.39988746989518403, Training Accuracy=0.8299999833106995\n","...Batch #23700 : Training Loss=0.384130916967988, Training Accuracy=0.8306249976158142\n","...Batch #23800 : Training Loss=0.39944349199533463, Training Accuracy=0.8299999833106995\n","...Batch #23900 : Training Loss=0.4129962131381035, Training Accuracy=0.8293749690055847\n","...Batch #24000 : Training Loss=0.3505519379675388, Training Accuracy=0.8431249856948853\n","...Batch #24100 : Training Loss=0.3694901956617832, Training Accuracy=0.8424999713897705\n","...Batch #24200 : Training Loss=0.3959410211443901, Training Accuracy=0.8349999785423279\n","...Batch #24300 : Training Loss=0.38184823576360943, Training Accuracy=0.8331249952316284\n","...Batch #24400 : Training Loss=0.3765754858404398, Training Accuracy=0.8399999737739563\n","...Batch #24500 : Training Loss=0.41829008996486666, Training Accuracy=0.8237499594688416\n","...Batch #24600 : Training Loss=0.4000154536217451, Training Accuracy=0.8293749690055847\n","...Batch #24700 : Training Loss=0.4084329442679882, Training Accuracy=0.8262499570846558\n","...Batch #24800 : Training Loss=0.39215697437524794, Training Accuracy=0.82874995470047\n","...Batch #24900 : Training Loss=0.3688085628300905, Training Accuracy=0.8387500047683716\n","...Batch #25000 : Training Loss=0.38960707016289237, Training Accuracy=0.8331249952316284\n","...Batch #25100 : Training Loss=0.38566255919635295, Training Accuracy=0.82874995470047\n","...Batch #25200 : Training Loss=0.4053397788107395, Training Accuracy=0.8106249570846558\n","...Batch #25300 : Training Loss=0.35757912769913675, Training Accuracy=0.8493750095367432\n","...Batch #25400 : Training Loss=0.3703734204173088, Training Accuracy=0.8506249785423279\n","...Batch #25500 : Training Loss=0.37414147317409513, Training Accuracy=0.8312499523162842\n","...Batch #25600 : Training Loss=0.3891910085827112, Training Accuracy=0.8318749666213989\n","...Batch #25700 : Training Loss=0.4126811029016972, Training Accuracy=0.8199999928474426\n","...Batch #25800 : Training Loss=0.3863427796214819, Training Accuracy=0.8362500071525574\n","...Batch #25900 : Training Loss=0.38567623429000375, Training Accuracy=0.8337500095367432\n","...Batch #26000 : Training Loss=0.379470052793622, Training Accuracy=0.8356249928474426\n","...Batch #26100 : Training Loss=0.3928266100585461, Training Accuracy=0.8349999785423279\n","...Batch #26200 : Training Loss=0.4118253272771835, Training Accuracy=0.8306249976158142\n","...Batch #26300 : Training Loss=0.35795872151851654, Training Accuracy=0.8499999642372131\n","...Batch #26400 : Training Loss=0.3639806967973709, Training Accuracy=0.8387500047683716\n","...Batch #26500 : Training Loss=0.39256169147789477, Training Accuracy=0.8256250023841858\n","...Batch #26600 : Training Loss=0.3850353415310383, Training Accuracy=0.8374999761581421\n","...Batch #26700 : Training Loss=0.37044997334480284, Training Accuracy=0.8349999785423279\n","...Batch #26800 : Training Loss=0.3588173854351044, Training Accuracy=0.8487499952316284\n","...Batch #26900 : Training Loss=0.3650112496316433, Training Accuracy=0.8493750095367432\n","...Batch #27000 : Training Loss=0.3889294671267271, Training Accuracy=0.8324999809265137\n","...Batch #27100 : Training Loss=0.38296597748994826, Training Accuracy=0.8468749523162842\n","...Batch #27200 : Training Loss=0.3522370610386133, Training Accuracy=0.8537499904632568\n","...Batch #27300 : Training Loss=0.36435569524765016, Training Accuracy=0.8531249761581421\n","...Batch #27400 : Training Loss=0.39491911605000496, Training Accuracy=0.8324999809265137\n","...Batch #27500 : Training Loss=0.392631433904171, Training Accuracy=0.8368749618530273\n","...Batch #27600 : Training Loss=0.3921563244611025, Training Accuracy=0.8262499570846558\n","...Batch #27700 : Training Loss=0.41232829429209233, Training Accuracy=0.8224999904632568\n","...Batch #27800 : Training Loss=0.39283922597765925, Training Accuracy=0.8181250095367432\n","...Batch #27900 : Training Loss=0.38634298272430895, Training Accuracy=0.840624988079071\n","...Batch #28000 : Training Loss=0.40464287661015985, Training Accuracy=0.8256250023841858\n","...Batch #28100 : Training Loss=0.37267756797373297, Training Accuracy=0.8468749523162842\n","...Batch #28200 : Training Loss=0.35813961770385505, Training Accuracy=0.856249988079071\n","...Batch #28300 : Training Loss=0.37482805401086805, Training Accuracy=0.8399999737739563\n","...Batch #28400 : Training Loss=0.4256918253004551, Training Accuracy=0.8025000095367432\n","...Batch #28500 : Training Loss=0.37658953547477725, Training Accuracy=0.8524999618530273\n","...Batch #28600 : Training Loss=0.3763447776436806, Training Accuracy=0.8412500023841858\n","...Batch #28700 : Training Loss=0.4124822422862053, Training Accuracy=0.8149999976158142\n","...Batch #28800 : Training Loss=0.39272110477089883, Training Accuracy=0.8231250047683716\n","...Batch #28900 : Training Loss=0.3694999948143959, Training Accuracy=0.8587499856948853\n","...Batch #29000 : Training Loss=0.3747561141103506, Training Accuracy=0.8393749594688416\n","...Batch #29100 : Training Loss=0.3567788479477167, Training Accuracy=0.84437495470047\n","...Batch #29200 : Training Loss=0.3823290603607893, Training Accuracy=0.8268749713897705\n","...Batch #29300 : Training Loss=0.3653877110034227, Training Accuracy=0.8231250047683716\n","...Batch #29400 : Training Loss=0.39181895174086095, Training Accuracy=0.8387500047683716\n","...Batch #29500 : Training Loss=0.38029402382671834, Training Accuracy=0.8318749666213989\n","...Batch #29600 : Training Loss=0.3936751784384251, Training Accuracy=0.8299999833106995\n","...Batch #29700 : Training Loss=0.38030840814113615, Training Accuracy=0.8324999809265137\n","...Batch #29800 : Training Loss=0.3909880702197552, Training Accuracy=0.8374999761581421\n","...Batch #29900 : Training Loss=0.40506951093673704, Training Accuracy=0.824999988079071\n","...Batch #30000 : Training Loss=0.389898112565279, Training Accuracy=0.8268749713897705\n","...Batch #30100 : Training Loss=0.3742541272938251, Training Accuracy=0.8356249928474426\n","...Batch #30200 : Training Loss=0.36973132111132145, Training Accuracy=0.8449999690055847\n","...Batch #30300 : Training Loss=0.33719407603144647, Training Accuracy=0.8606249690055847\n","...Batch #30400 : Training Loss=0.38780207447707654, Training Accuracy=0.8381249904632568\n","...Batch #30500 : Training Loss=0.39273347541689874, Training Accuracy=0.82874995470047\n","...Batch #30600 : Training Loss=0.377190999314189, Training Accuracy=0.8431249856948853\n","...Batch #30700 : Training Loss=0.37737064629793166, Training Accuracy=0.84375\n","...Batch #30800 : Training Loss=0.3763379367440939, Training Accuracy=0.8362500071525574\n","...Batch #30900 : Training Loss=0.3780684068053961, Training Accuracy=0.8331249952316284\n","...Batch #31000 : Training Loss=0.3502541545033455, Training Accuracy=0.840624988079071\n","...Batch #31100 : Training Loss=0.38766005367040635, Training Accuracy=0.8199999928474426\n","...Batch #31200 : Training Loss=0.3830757642537355, Training Accuracy=0.8268749713897705\n","...Batch #31300 : Training Loss=0.3801965632289648, Training Accuracy=0.8349999785423279\n","...Batch #31400 : Training Loss=0.3768514747172594, Training Accuracy=0.8387500047683716\n","...Batch #31500 : Training Loss=0.39216246619820594, Training Accuracy=0.8318749666213989\n","...Batch #31600 : Training Loss=0.39641472332179545, Training Accuracy=0.8312499523162842\n","...Batch #31700 : Training Loss=0.36197125919163226, Training Accuracy=0.8493750095367432\n","...Batch #31800 : Training Loss=0.3906947929412127, Training Accuracy=0.8224999904632568\n","...Batch #31900 : Training Loss=0.367018021568656, Training Accuracy=0.8368749618530273\n","...Batch #32000 : Training Loss=0.37611880503594874, Training Accuracy=0.8449999690055847\n","...Batch #32100 : Training Loss=0.3711076685041189, Training Accuracy=0.8418749570846558\n","...Batch #32200 : Training Loss=0.3991224832087755, Training Accuracy=0.8268749713897705\n","...Batch #32300 : Training Loss=0.38241098694503306, Training Accuracy=0.8349999785423279\n","...Batch #32400 : Training Loss=0.39251855105161665, Training Accuracy=0.8356249928474426\n","...Batch #32500 : Training Loss=0.35218477837741374, Training Accuracy=0.8424999713897705\n","...Batch #32600 : Training Loss=0.3808682245016098, Training Accuracy=0.8462499976158142\n","...Batch #32700 : Training Loss=0.3717792009562254, Training Accuracy=0.8393749594688416\n","...Batch #32800 : Training Loss=0.3798807438462973, Training Accuracy=0.8393749594688416\n","...Batch #32900 : Training Loss=0.38000541187822817, Training Accuracy=0.8412500023841858\n","...Batch #33000 : Training Loss=0.3706625020503998, Training Accuracy=0.8431249856948853\n","...Batch #33100 : Training Loss=0.391119247302413, Training Accuracy=0.840624988079071\n","...Batch #33200 : Training Loss=0.3646997258812189, Training Accuracy=0.84375\n","...Batch #33300 : Training Loss=0.37387677647173406, Training Accuracy=0.84375\n","...Batch #33400 : Training Loss=0.380592484511435, Training Accuracy=0.82874995470047\n","...Batch #33500 : Training Loss=0.3743206413835287, Training Accuracy=0.8387500047683716\n","...Batch #33600 : Training Loss=0.37132019974291325, Training Accuracy=0.8412500023841858\n","...Batch #33700 : Training Loss=0.36361482571810483, Training Accuracy=0.8387500047683716\n","...Batch #33800 : Training Loss=0.37269698902964593, Training Accuracy=0.8318749666213989\n","...Batch #33900 : Training Loss=0.38005900122225283, Training Accuracy=0.8293749690055847\n","...Batch #34000 : Training Loss=0.3710903833806515, Training Accuracy=0.8399999737739563\n","...Batch #34100 : Training Loss=0.3977443639189005, Training Accuracy=0.8362500071525574\n","...Batch #34200 : Training Loss=0.34974090024828913, Training Accuracy=0.8499999642372131\n","...Batch #34300 : Training Loss=0.3783006490767002, Training Accuracy=0.8387500047683716\n","...Batch #34400 : Training Loss=0.38537935830652714, Training Accuracy=0.8293749690055847\n","...Batch #34500 : Training Loss=0.37039205580949786, Training Accuracy=0.8481249809265137\n","...Batch #34600 : Training Loss=0.3786391196399927, Training Accuracy=0.8374999761581421\n","...Batch #34700 : Training Loss=0.35983875587582587, Training Accuracy=0.8312499523162842\n","...Batch #34800 : Training Loss=0.37512521870434284, Training Accuracy=0.8331249952316284\n","...Batch #34900 : Training Loss=0.3785793972760439, Training Accuracy=0.840624988079071\n","...Batch #35000 : Training Loss=0.38169315956532957, Training Accuracy=0.8224999904632568\n","...Batch #35100 : Training Loss=0.387852396517992, Training Accuracy=0.8256250023841858\n","...Batch #35200 : Training Loss=0.3869069568067789, Training Accuracy=0.8268749713897705\n","...Batch #35300 : Training Loss=0.36742545053362846, Training Accuracy=0.8393749594688416\n","...Batch #35400 : Training Loss=0.3708161524683237, Training Accuracy=0.84375\n","...Batch #35500 : Training Loss=0.37739427998661995, Training Accuracy=0.8262499570846558\n","...Batch #35600 : Training Loss=0.3904641085118055, Training Accuracy=0.8268749713897705\n","...Batch #35700 : Training Loss=0.36941722251474857, Training Accuracy=0.8531249761581421\n","...Batch #35800 : Training Loss=0.37963429890573025, Training Accuracy=0.840624988079071\n","...Batch #35900 : Training Loss=0.38281649358570574, Training Accuracy=0.8356249928474426\n","...Batch #36000 : Training Loss=0.344496768116951, Training Accuracy=0.856249988079071\n","...Batch #36100 : Training Loss=0.37865059174597265, Training Accuracy=0.8424999713897705\n","...Batch #36200 : Training Loss=0.3823822209239006, Training Accuracy=0.8418749570846558\n","...Batch #36300 : Training Loss=0.36278711274266245, Training Accuracy=0.8343749642372131\n","Train: loss 0.38889996962200024, accuracy 0.8312570411892397\n","Valid: loss 0.37622328129058846, accuracy 0.8441091954022989, f1_ass 0.7786740843764488, f1_sweet 0.8796814355562276\n","####################################################################################################\n","Epoch 4 / 7\n","...Batch #100 : Training Loss=0.3881844466924667, Training Accuracy=0.8318749666213989\n","...Batch #200 : Training Loss=0.37711792908608915, Training Accuracy=0.840624988079071\n","...Batch #300 : Training Loss=0.350902279317379, Training Accuracy=0.8481249809265137\n","...Batch #400 : Training Loss=0.38130549795925617, Training Accuracy=0.8368749618530273\n","...Batch #500 : Training Loss=0.3831897358596325, Training Accuracy=0.8374999761581421\n","...Batch #600 : Training Loss=0.3551696491241455, Training Accuracy=0.8512499928474426\n","...Batch #700 : Training Loss=0.38933291643857953, Training Accuracy=0.8293749690055847\n","...Batch #800 : Training Loss=0.3709259787201881, Training Accuracy=0.8474999666213989\n","...Batch #900 : Training Loss=0.3869448281824589, Training Accuracy=0.8256250023841858\n","...Batch #1000 : Training Loss=0.383395438939333, Training Accuracy=0.8318749666213989\n","...Batch #1100 : Training Loss=0.36281220719218255, Training Accuracy=0.8549999594688416\n","...Batch #1200 : Training Loss=0.3631947672739625, Training Accuracy=0.8343749642372131\n","...Batch #1300 : Training Loss=0.36395615682005883, Training Accuracy=0.8481249809265137\n","...Batch #1400 : Training Loss=0.3877743022143841, Training Accuracy=0.8274999856948853\n","...Batch #1500 : Training Loss=0.3479603195935488, Training Accuracy=0.85999995470047\n","...Batch #1600 : Training Loss=0.35976640924811365, Training Accuracy=0.8531249761581421\n","...Batch #1700 : Training Loss=0.37305158920586107, Training Accuracy=0.8368749618530273\n","...Batch #1800 : Training Loss=0.3529019898921251, Training Accuracy=0.8493750095367432\n","...Batch #1900 : Training Loss=0.37425846137106417, Training Accuracy=0.8481249809265137\n","...Batch #2000 : Training Loss=0.363118846192956, Training Accuracy=0.840624988079071\n","...Batch #2100 : Training Loss=0.36096638277173043, Training Accuracy=0.8387500047683716\n","...Batch #2200 : Training Loss=0.37839514672756197, Training Accuracy=0.8387500047683716\n","...Batch #2300 : Training Loss=0.3732949712127447, Training Accuracy=0.8456249833106995\n","...Batch #2400 : Training Loss=0.35505439080297946, Training Accuracy=0.8524999618530273\n","...Batch #2500 : Training Loss=0.3857970491051674, Training Accuracy=0.8374999761581421\n","...Batch #2600 : Training Loss=0.36617940969765184, Training Accuracy=0.8412500023841858\n","...Batch #2700 : Training Loss=0.38875518541783094, Training Accuracy=0.8324999809265137\n","...Batch #2800 : Training Loss=0.36918582908809183, Training Accuracy=0.8449999690055847\n","...Batch #2900 : Training Loss=0.3899188134074211, Training Accuracy=0.824999988079071\n","...Batch #3000 : Training Loss=0.3426248325407505, Training Accuracy=0.8493750095367432\n","...Batch #3100 : Training Loss=0.3691940180212259, Training Accuracy=0.8431249856948853\n","...Batch #3200 : Training Loss=0.3763324201107025, Training Accuracy=0.8268749713897705\n","...Batch #3300 : Training Loss=0.35838029090315104, Training Accuracy=0.8456249833106995\n","...Batch #3400 : Training Loss=0.3648460700362921, Training Accuracy=0.8531249761581421\n","...Batch #3500 : Training Loss=0.38118317514657973, Training Accuracy=0.8349999785423279\n","...Batch #3600 : Training Loss=0.36543414913117883, Training Accuracy=0.84375\n","...Batch #3700 : Training Loss=0.344556016176939, Training Accuracy=0.8556249737739563\n","...Batch #3800 : Training Loss=0.35220743134617805, Training Accuracy=0.8456249833106995\n","...Batch #3900 : Training Loss=0.373450952321291, Training Accuracy=0.8306249976158142\n","...Batch #4000 : Training Loss=0.37632416762411597, Training Accuracy=0.8456249833106995\n","...Batch #4100 : Training Loss=0.3656518591940403, Training Accuracy=0.8506249785423279\n","...Batch #4200 : Training Loss=0.3671929584443569, Training Accuracy=0.8374999761581421\n","...Batch #4300 : Training Loss=0.35746423244476316, Training Accuracy=0.8587499856948853\n","...Batch #4400 : Training Loss=0.34264498345553873, Training Accuracy=0.8549999594688416\n","...Batch #4500 : Training Loss=0.35998478546738627, Training Accuracy=0.8456249833106995\n","...Batch #4600 : Training Loss=0.37807861298322676, Training Accuracy=0.8393749594688416\n","...Batch #4700 : Training Loss=0.38246470376849173, Training Accuracy=0.8243749737739563\n","...Batch #4800 : Training Loss=0.4052199739962816, Training Accuracy=0.828125\n","...Batch #4900 : Training Loss=0.3814052381366491, Training Accuracy=0.8293749690055847\n","...Batch #5000 : Training Loss=0.3602390617132187, Training Accuracy=0.84375\n","...Batch #5100 : Training Loss=0.4020078490674496, Training Accuracy=0.8337500095367432\n","...Batch #5200 : Training Loss=0.3667579893767834, Training Accuracy=0.8468749523162842\n","...Batch #5300 : Training Loss=0.3767541189491749, Training Accuracy=0.8374999761581421\n","...Batch #5400 : Training Loss=0.3227466700226069, Training Accuracy=0.8637499809265137\n","...Batch #5500 : Training Loss=0.37363222062587736, Training Accuracy=0.8393749594688416\n","...Batch #5600 : Training Loss=0.3731084745377302, Training Accuracy=0.8362500071525574\n","...Batch #5700 : Training Loss=0.3734111054241657, Training Accuracy=0.8343749642372131\n","...Batch #5800 : Training Loss=0.39363888032734395, Training Accuracy=0.8306249976158142\n","...Batch #5900 : Training Loss=0.38183699652552605, Training Accuracy=0.8299999833106995\n","...Batch #6000 : Training Loss=0.3665655118227005, Training Accuracy=0.8393749594688416\n","...Batch #6100 : Training Loss=0.372514366954565, Training Accuracy=0.8362500071525574\n","...Batch #6200 : Training Loss=0.3896579065173864, Training Accuracy=0.8449999690055847\n","...Batch #6300 : Training Loss=0.3612780641019344, Training Accuracy=0.828125\n","...Batch #6400 : Training Loss=0.3712151838093996, Training Accuracy=0.8387500047683716\n","...Batch #6500 : Training Loss=0.36576862625777723, Training Accuracy=0.8474999666213989\n","...Batch #6600 : Training Loss=0.3510630140453577, Training Accuracy=0.8537499904632568\n","...Batch #6700 : Training Loss=0.3707404710352421, Training Accuracy=0.8449999690055847\n","...Batch #6800 : Training Loss=0.37861102357506754, Training Accuracy=0.8356249928474426\n","...Batch #6900 : Training Loss=0.3608499453961849, Training Accuracy=0.8399999737739563\n","...Batch #7000 : Training Loss=0.35708669990301134, Training Accuracy=0.8518750071525574\n","...Batch #7100 : Training Loss=0.3540287213027477, Training Accuracy=0.8487499952316284\n","...Batch #7200 : Training Loss=0.3472195503488183, Training Accuracy=0.8418749570846558\n","...Batch #7300 : Training Loss=0.36808867573738097, Training Accuracy=0.8349999785423279\n","...Batch #7400 : Training Loss=0.35884023532271386, Training Accuracy=0.8618749976158142\n","...Batch #7500 : Training Loss=0.37814666286110876, Training Accuracy=0.8206250071525574\n","...Batch #7600 : Training Loss=0.37210427537560464, Training Accuracy=0.8449999690055847\n","...Batch #7700 : Training Loss=0.3947191723436117, Training Accuracy=0.8224999904632568\n","...Batch #7800 : Training Loss=0.3901622176915407, Training Accuracy=0.8374999761581421\n","...Batch #7900 : Training Loss=0.38956560753285885, Training Accuracy=0.8293749690055847\n","...Batch #8000 : Training Loss=0.3542901214212179, Training Accuracy=0.8468749523162842\n","...Batch #8100 : Training Loss=0.3629122968018055, Training Accuracy=0.8493750095367432\n","...Batch #8200 : Training Loss=0.36125132206827404, Training Accuracy=0.8393749594688416\n","...Batch #8300 : Training Loss=0.3617143949866295, Training Accuracy=0.8456249833106995\n","...Batch #8400 : Training Loss=0.3545357272401452, Training Accuracy=0.856249988079071\n","...Batch #8500 : Training Loss=0.3714358219504356, Training Accuracy=0.840624988079071\n","...Batch #8600 : Training Loss=0.3524592682719231, Training Accuracy=0.8543750047683716\n","...Batch #8700 : Training Loss=0.36688189014792444, Training Accuracy=0.8431249856948853\n","...Batch #8800 : Training Loss=0.38664090633392334, Training Accuracy=0.8349999785423279\n","...Batch #8900 : Training Loss=0.3586792839318514, Training Accuracy=0.856249988079071\n","...Batch #9000 : Training Loss=0.4100066953152418, Training Accuracy=0.8199999928474426\n","...Batch #9100 : Training Loss=0.36857579842209814, Training Accuracy=0.8449999690055847\n","...Batch #9200 : Training Loss=0.3800382449477911, Training Accuracy=0.8324999809265137\n","...Batch #9300 : Training Loss=0.35512310527265073, Training Accuracy=0.8568750023841858\n","...Batch #9400 : Training Loss=0.3450512636452913, Training Accuracy=0.8518750071525574\n","...Batch #9500 : Training Loss=0.3836256532371044, Training Accuracy=0.8306249976158142\n","...Batch #9600 : Training Loss=0.3259140419960022, Training Accuracy=0.8656249642372131\n","...Batch #9700 : Training Loss=0.3465408595651388, Training Accuracy=0.8468749523162842\n","...Batch #9800 : Training Loss=0.40535303331911565, Training Accuracy=0.8218749761581421\n","...Batch #9900 : Training Loss=0.3653347394242883, Training Accuracy=0.8356249928474426\n","...Batch #10000 : Training Loss=0.37439099393785, Training Accuracy=0.8374999761581421\n","...Batch #10100 : Training Loss=0.40376756101846695, Training Accuracy=0.824999988079071\n","...Batch #10200 : Training Loss=0.3508553159236908, Training Accuracy=0.8431249856948853\n","...Batch #10300 : Training Loss=0.38738484650850297, Training Accuracy=0.8343749642372131\n","...Batch #10400 : Training Loss=0.36162634164094926, Training Accuracy=0.8424999713897705\n","...Batch #10500 : Training Loss=0.34698152117431164, Training Accuracy=0.8499999642372131\n","...Batch #10600 : Training Loss=0.40321535013616083, Training Accuracy=0.8256250023841858\n","...Batch #10700 : Training Loss=0.357910678088665, Training Accuracy=0.8499999642372131\n","...Batch #10800 : Training Loss=0.3593668334186077, Training Accuracy=0.8362500071525574\n","...Batch #10900 : Training Loss=0.342351016625762, Training Accuracy=0.8499999642372131\n","...Batch #11000 : Training Loss=0.41615834916010497, Training Accuracy=0.8174999952316284\n","...Batch #11100 : Training Loss=0.37506141655147074, Training Accuracy=0.840624988079071\n","...Batch #11200 : Training Loss=0.37165293902158736, Training Accuracy=0.8393749594688416\n","...Batch #11300 : Training Loss=0.37807236589491366, Training Accuracy=0.8337500095367432\n","...Batch #11400 : Training Loss=0.3534646029770374, Training Accuracy=0.8456249833106995\n","...Batch #11500 : Training Loss=0.36124431967735293, Training Accuracy=0.8537499904632568\n","...Batch #11600 : Training Loss=0.37439115896821024, Training Accuracy=0.8368749618530273\n","...Batch #11700 : Training Loss=0.3572501692920923, Training Accuracy=0.8493750095367432\n","...Batch #11800 : Training Loss=0.3893048167228699, Training Accuracy=0.8337500095367432\n","...Batch #11900 : Training Loss=0.3885543708503246, Training Accuracy=0.8343749642372131\n","...Batch #12000 : Training Loss=0.3683923376351595, Training Accuracy=0.8418749570846558\n","...Batch #12100 : Training Loss=0.38430192224681375, Training Accuracy=0.8418749570846558\n","...Batch #12200 : Training Loss=0.38810408756136894, Training Accuracy=0.824999988079071\n","...Batch #12300 : Training Loss=0.4015014009177685, Training Accuracy=0.82874995470047\n","...Batch #12400 : Training Loss=0.34477910451591015, Training Accuracy=0.8499999642372131\n","...Batch #12500 : Training Loss=0.38663338020443916, Training Accuracy=0.8387500047683716\n","...Batch #12600 : Training Loss=0.34436557825654746, Training Accuracy=0.8456249833106995\n","...Batch #12700 : Training Loss=0.3824631769955158, Training Accuracy=0.8387500047683716\n","...Batch #12800 : Training Loss=0.38343375876545904, Training Accuracy=0.84437495470047\n","...Batch #12900 : Training Loss=0.385075898244977, Training Accuracy=0.8431249856948853\n","...Batch #13000 : Training Loss=0.3768541026860476, Training Accuracy=0.8393749594688416\n","...Batch #13100 : Training Loss=0.3519484657049179, Training Accuracy=0.8499999642372131\n","...Batch #13200 : Training Loss=0.3873523487895727, Training Accuracy=0.8387500047683716\n","...Batch #13300 : Training Loss=0.3724659229815006, Training Accuracy=0.8468749523162842\n","...Batch #13400 : Training Loss=0.34065712951123717, Training Accuracy=0.8581249713897705\n","...Batch #13500 : Training Loss=0.34392592653632165, Training Accuracy=0.8537499904632568\n","...Batch #13600 : Training Loss=0.37450044125318527, Training Accuracy=0.8456249833106995\n","...Batch #13700 : Training Loss=0.3613702362403274, Training Accuracy=0.8537499904632568\n","...Batch #13800 : Training Loss=0.3606191247701645, Training Accuracy=0.8524999618530273\n","...Batch #13900 : Training Loss=0.3646375503391027, Training Accuracy=0.8449999690055847\n","...Batch #14000 : Training Loss=0.3719690230488777, Training Accuracy=0.8318749666213989\n","...Batch #14100 : Training Loss=0.3324518097937107, Training Accuracy=0.8737499713897705\n","...Batch #14200 : Training Loss=0.34389267094433307, Training Accuracy=0.8462499976158142\n","...Batch #14300 : Training Loss=0.3591719734668732, Training Accuracy=0.8449999690055847\n","...Batch #14400 : Training Loss=0.3691850624978542, Training Accuracy=0.84375\n","...Batch #14500 : Training Loss=0.3528328945115209, Training Accuracy=0.8506249785423279\n","...Batch #14600 : Training Loss=0.3628712198883295, Training Accuracy=0.8387500047683716\n","...Batch #14700 : Training Loss=0.36721812169998885, Training Accuracy=0.8424999713897705\n","...Batch #14800 : Training Loss=0.36538549393415454, Training Accuracy=0.840624988079071\n","...Batch #14900 : Training Loss=0.3458111292868853, Training Accuracy=0.8574999570846558\n","...Batch #15000 : Training Loss=0.38003643080592153, Training Accuracy=0.82874995470047\n","...Batch #15100 : Training Loss=0.3485697239637375, Training Accuracy=0.8481249809265137\n","...Batch #15200 : Training Loss=0.3568609255552292, Training Accuracy=0.84437495470047\n","...Batch #15300 : Training Loss=0.38877388179302214, Training Accuracy=0.8331249952316284\n","...Batch #15400 : Training Loss=0.3723490506410599, Training Accuracy=0.8306249976158142\n","...Batch #15500 : Training Loss=0.37100550018250944, Training Accuracy=0.8418749570846558\n","...Batch #15600 : Training Loss=0.36700226575136186, Training Accuracy=0.8474999666213989\n","...Batch #15700 : Training Loss=0.35899038359522817, Training Accuracy=0.8487499952316284\n","...Batch #15800 : Training Loss=0.3848758790269494, Training Accuracy=0.8293749690055847\n","...Batch #15900 : Training Loss=0.37454324431717395, Training Accuracy=0.8399999737739563\n","...Batch #16000 : Training Loss=0.35470377907156947, Training Accuracy=0.8543750047683716\n","...Batch #16100 : Training Loss=0.3853678482025862, Training Accuracy=0.8374999761581421\n","...Batch #16200 : Training Loss=0.3604858111590147, Training Accuracy=0.84375\n","...Batch #16300 : Training Loss=0.4052863561362028, Training Accuracy=0.8387500047683716\n","...Batch #16400 : Training Loss=0.38661636084318163, Training Accuracy=0.8331249952316284\n","...Batch #16500 : Training Loss=0.34323694102466107, Training Accuracy=0.8574999570846558\n","...Batch #16600 : Training Loss=0.36455654591321945, Training Accuracy=0.8549999594688416\n","...Batch #16700 : Training Loss=0.35388069100677966, Training Accuracy=0.8399999737739563\n","...Batch #16800 : Training Loss=0.39856428109109404, Training Accuracy=0.8306249976158142\n","...Batch #16900 : Training Loss=0.3652714826539159, Training Accuracy=0.8493750095367432\n","...Batch #17000 : Training Loss=0.3795390834659338, Training Accuracy=0.8462499976158142\n","...Batch #17100 : Training Loss=0.35459960967302323, Training Accuracy=0.84375\n","...Batch #17200 : Training Loss=0.37971982002258303, Training Accuracy=0.8362500071525574\n","...Batch #17300 : Training Loss=0.3633839276432991, Training Accuracy=0.8449999690055847\n","...Batch #17400 : Training Loss=0.3438869567215443, Training Accuracy=0.8481249809265137\n","...Batch #17500 : Training Loss=0.34100726261734965, Training Accuracy=0.8568750023841858\n","...Batch #17600 : Training Loss=0.3779500424861908, Training Accuracy=0.840624988079071\n","...Batch #17700 : Training Loss=0.35211464695632455, Training Accuracy=0.8493750095367432\n","...Batch #17800 : Training Loss=0.34195522747933865, Training Accuracy=0.8537499904632568\n","...Batch #17900 : Training Loss=0.38427564449608326, Training Accuracy=0.8381249904632568\n","...Batch #18000 : Training Loss=0.3571948233991861, Training Accuracy=0.8431249856948853\n","...Batch #18100 : Training Loss=0.3743421023339033, Training Accuracy=0.8349999785423279\n","...Batch #18200 : Training Loss=0.3845482099056244, Training Accuracy=0.8331249952316284\n","...Batch #18300 : Training Loss=0.3592061970755458, Training Accuracy=0.8468749523162842\n","...Batch #18400 : Training Loss=0.37769607193768023, Training Accuracy=0.8431249856948853\n","...Batch #18500 : Training Loss=0.34817281618714335, Training Accuracy=0.8481249809265137\n","...Batch #18600 : Training Loss=0.36711270466446877, Training Accuracy=0.84375\n","...Batch #18700 : Training Loss=0.34778816640377047, Training Accuracy=0.8512499928474426\n","...Batch #18800 : Training Loss=0.3602525885403156, Training Accuracy=0.8474999666213989\n","...Batch #18900 : Training Loss=0.4010214353352785, Training Accuracy=0.8168749809265137\n","...Batch #19000 : Training Loss=0.3664607289433479, Training Accuracy=0.8574999570846558\n","...Batch #19100 : Training Loss=0.3856018322706223, Training Accuracy=0.8399999737739563\n","...Batch #19200 : Training Loss=0.3645706273987889, Training Accuracy=0.8418749570846558\n","...Batch #19300 : Training Loss=0.35531071700155736, Training Accuracy=0.84375\n","...Batch #19400 : Training Loss=0.3859356770664453, Training Accuracy=0.8474999666213989\n","...Batch #19500 : Training Loss=0.36453729070723057, Training Accuracy=0.84437495470047\n","...Batch #19600 : Training Loss=0.35570080153644085, Training Accuracy=0.8449999690055847\n","...Batch #19700 : Training Loss=0.38031041629612444, Training Accuracy=0.8274999856948853\n","...Batch #19800 : Training Loss=0.3755829594656825, Training Accuracy=0.8499999642372131\n","...Batch #19900 : Training Loss=0.3695510237663984, Training Accuracy=0.8324999809265137\n","...Batch #20000 : Training Loss=0.3820862952619791, Training Accuracy=0.8318749666213989\n","...Batch #20100 : Training Loss=0.34491932041943074, Training Accuracy=0.8537499904632568\n","...Batch #20200 : Training Loss=0.32767678253352645, Training Accuracy=0.8643749952316284\n","...Batch #20300 : Training Loss=0.3633610478788614, Training Accuracy=0.8524999618530273\n","...Batch #20400 : Training Loss=0.3941002672165632, Training Accuracy=0.8362500071525574\n","...Batch #20500 : Training Loss=0.35394507758319377, Training Accuracy=0.8481249809265137\n","...Batch #20600 : Training Loss=0.3790007954090834, Training Accuracy=0.8318749666213989\n","...Batch #20700 : Training Loss=0.3375619816407561, Training Accuracy=0.84375\n","...Batch #20800 : Training Loss=0.35485797695815563, Training Accuracy=0.8449999690055847\n","...Batch #20900 : Training Loss=0.38014730632305144, Training Accuracy=0.8387500047683716\n","...Batch #21000 : Training Loss=0.3652720806747675, Training Accuracy=0.8418749570846558\n","...Batch #21100 : Training Loss=0.36755810633301733, Training Accuracy=0.8456249833106995\n","...Batch #21200 : Training Loss=0.3389542504400015, Training Accuracy=0.8574999570846558\n","...Batch #21300 : Training Loss=0.3334402298927307, Training Accuracy=0.8612499833106995\n","...Batch #21400 : Training Loss=0.3446162759885192, Training Accuracy=0.8556249737739563\n","...Batch #21500 : Training Loss=0.359321014136076, Training Accuracy=0.84437495470047\n","...Batch #21600 : Training Loss=0.37152267158031466, Training Accuracy=0.8456249833106995\n","...Batch #21700 : Training Loss=0.36352894492447374, Training Accuracy=0.8512499928474426\n","...Batch #21800 : Training Loss=0.3184284941852093, Training Accuracy=0.871874988079071\n","...Batch #21900 : Training Loss=0.37912700202316046, Training Accuracy=0.8393749594688416\n","...Batch #22000 : Training Loss=0.37000066459178926, Training Accuracy=0.8399999737739563\n","...Batch #22100 : Training Loss=0.37397624224424364, Training Accuracy=0.8324999809265137\n","...Batch #22200 : Training Loss=0.34426298923790455, Training Accuracy=0.840624988079071\n","...Batch #22300 : Training Loss=0.3728441626578569, Training Accuracy=0.840624988079071\n","...Batch #22400 : Training Loss=0.34995332438498733, Training Accuracy=0.8424999713897705\n","...Batch #22500 : Training Loss=0.39922370508313176, Training Accuracy=0.8306249976158142\n","...Batch #22600 : Training Loss=0.34739011868834496, Training Accuracy=0.8493750095367432\n","...Batch #22700 : Training Loss=0.3808359334617853, Training Accuracy=0.8274999856948853\n","...Batch #22800 : Training Loss=0.34475479774177076, Training Accuracy=0.8512499928474426\n","...Batch #22900 : Training Loss=0.33120595291256905, Training Accuracy=0.8568750023841858\n","...Batch #23000 : Training Loss=0.35019040361046794, Training Accuracy=0.8487499952316284\n","...Batch #23100 : Training Loss=0.3678499027341604, Training Accuracy=0.8462499976158142\n","...Batch #23200 : Training Loss=0.3678023501113057, Training Accuracy=0.8487499952316284\n","...Batch #23300 : Training Loss=0.33198189213871954, Training Accuracy=0.859375\n","...Batch #23400 : Training Loss=0.34769092977046967, Training Accuracy=0.8531249761581421\n","...Batch #23500 : Training Loss=0.34383120745420453, Training Accuracy=0.8537499904632568\n","...Batch #23600 : Training Loss=0.378963378444314, Training Accuracy=0.8374999761581421\n","...Batch #23700 : Training Loss=0.3601885984838009, Training Accuracy=0.8449999690055847\n","...Batch #23800 : Training Loss=0.3589710967242718, Training Accuracy=0.8487499952316284\n","...Batch #23900 : Training Loss=0.38614937126636506, Training Accuracy=0.8493750095367432\n","...Batch #24000 : Training Loss=0.33334739282727244, Training Accuracy=0.859375\n","...Batch #24100 : Training Loss=0.3485643570125103, Training Accuracy=0.8531249761581421\n","...Batch #24200 : Training Loss=0.3499209223687649, Training Accuracy=0.8549999594688416\n","...Batch #24300 : Training Loss=0.3616500458866358, Training Accuracy=0.8462499976158142\n","...Batch #24400 : Training Loss=0.341013500392437, Training Accuracy=0.8518750071525574\n","...Batch #24500 : Training Loss=0.3953791771829128, Training Accuracy=0.8293749690055847\n","...Batch #24600 : Training Loss=0.3902408728003502, Training Accuracy=0.8243749737739563\n","...Batch #24700 : Training Loss=0.3738121296465397, Training Accuracy=0.8387500047683716\n","...Batch #24800 : Training Loss=0.37421096101403234, Training Accuracy=0.84437495470047\n","...Batch #24900 : Training Loss=0.3373958607017994, Training Accuracy=0.8543750047683716\n","...Batch #25000 : Training Loss=0.3502429936826229, Training Accuracy=0.8468749523162842\n","...Batch #25100 : Training Loss=0.3684956131130457, Training Accuracy=0.8431249856948853\n","...Batch #25200 : Training Loss=0.377834692671895, Training Accuracy=0.828125\n","...Batch #25300 : Training Loss=0.3386513850837946, Training Accuracy=0.8549999594688416\n","...Batch #25400 : Training Loss=0.3381687431782484, Training Accuracy=0.8643749952316284\n","...Batch #25500 : Training Loss=0.3448966195434332, Training Accuracy=0.8493750095367432\n","...Batch #25600 : Training Loss=0.36834298580884933, Training Accuracy=0.8412500023841858\n","...Batch #25700 : Training Loss=0.3857304540276527, Training Accuracy=0.84437495470047\n","...Batch #25800 : Training Loss=0.3584008927643299, Training Accuracy=0.8506249785423279\n","...Batch #25900 : Training Loss=0.3656346853077412, Training Accuracy=0.8412500023841858\n","...Batch #26000 : Training Loss=0.35385687433183194, Training Accuracy=0.8549999594688416\n","...Batch #26100 : Training Loss=0.37286299854516985, Training Accuracy=0.8456249833106995\n","...Batch #26200 : Training Loss=0.36955241687595847, Training Accuracy=0.8374999761581421\n","...Batch #26300 : Training Loss=0.3152749066799879, Training Accuracy=0.8774999976158142\n","...Batch #26400 : Training Loss=0.3417488807439804, Training Accuracy=0.8574999570846558\n","...Batch #26500 : Training Loss=0.3554854126274586, Training Accuracy=0.84375\n","...Batch #26600 : Training Loss=0.3623564375936985, Training Accuracy=0.84375\n","...Batch #26700 : Training Loss=0.3438826335221529, Training Accuracy=0.8468749523162842\n","...Batch #26800 : Training Loss=0.3441674429178238, Training Accuracy=0.8531249761581421\n","...Batch #26900 : Training Loss=0.33956743821501734, Training Accuracy=0.859375\n","...Batch #27000 : Training Loss=0.35154842790216206, Training Accuracy=0.8506249785423279\n","...Batch #27100 : Training Loss=0.36803516156971455, Training Accuracy=0.8512499928474426\n","...Batch #27200 : Training Loss=0.3342442299425602, Training Accuracy=0.8618749976158142\n","...Batch #27300 : Training Loss=0.3445767048001289, Training Accuracy=0.859375\n","...Batch #27400 : Training Loss=0.3692149319499731, Training Accuracy=0.8431249856948853\n","...Batch #27500 : Training Loss=0.3681453329324722, Training Accuracy=0.8481249809265137\n","...Batch #27600 : Training Loss=0.3787574468925595, Training Accuracy=0.8474999666213989\n","...Batch #27700 : Training Loss=0.38552889086306097, Training Accuracy=0.8362500071525574\n","...Batch #27800 : Training Loss=0.3699552583694458, Training Accuracy=0.8368749618530273\n","...Batch #27900 : Training Loss=0.36104841660708187, Training Accuracy=0.8424999713897705\n","...Batch #28000 : Training Loss=0.3684782601892948, Training Accuracy=0.8493750095367432\n","...Batch #28100 : Training Loss=0.3551641125977039, Training Accuracy=0.8506249785423279\n","...Batch #28200 : Training Loss=0.3382041526585817, Training Accuracy=0.8606249690055847\n","...Batch #28300 : Training Loss=0.33835071317851545, Training Accuracy=0.8568750023841858\n","...Batch #28400 : Training Loss=0.39456720143556595, Training Accuracy=0.8318749666213989\n","...Batch #28500 : Training Loss=0.3438038641214371, Training Accuracy=0.859375\n","...Batch #28600 : Training Loss=0.33117753952741624, Training Accuracy=0.8574999570846558\n","...Batch #28700 : Training Loss=0.3788957007229328, Training Accuracy=0.8243749737739563\n","...Batch #28800 : Training Loss=0.3714371547847986, Training Accuracy=0.8337500095367432\n","...Batch #28900 : Training Loss=0.3382718949764967, Training Accuracy=0.8650000095367432\n","...Batch #29000 : Training Loss=0.35215287551283836, Training Accuracy=0.8462499976158142\n","...Batch #29100 : Training Loss=0.351143903657794, Training Accuracy=0.8581249713897705\n","...Batch #29200 : Training Loss=0.3605300222337246, Training Accuracy=0.8431249856948853\n","...Batch #29300 : Training Loss=0.34762127738445997, Training Accuracy=0.8424999713897705\n","...Batch #29400 : Training Loss=0.3570058470964432, Training Accuracy=0.8518750071525574\n","...Batch #29500 : Training Loss=0.35223188921809195, Training Accuracy=0.8412500023841858\n","...Batch #29600 : Training Loss=0.36577396348118785, Training Accuracy=0.8399999737739563\n","...Batch #29700 : Training Loss=0.3572386734932661, Training Accuracy=0.840624988079071\n","...Batch #29800 : Training Loss=0.3720049698650837, Training Accuracy=0.8468749523162842\n","...Batch #29900 : Training Loss=0.37123178340494634, Training Accuracy=0.840624988079071\n","...Batch #30000 : Training Loss=0.3610042899101973, Training Accuracy=0.8549999594688416\n","...Batch #30100 : Training Loss=0.35118608742952345, Training Accuracy=0.8506249785423279\n","...Batch #30200 : Training Loss=0.3452669996023178, Training Accuracy=0.8549999594688416\n","...Batch #30300 : Training Loss=0.31217317659407856, Training Accuracy=0.8706249594688416\n","...Batch #30400 : Training Loss=0.36395217292010784, Training Accuracy=0.84375\n","...Batch #30500 : Training Loss=0.3685526406019926, Training Accuracy=0.8518750071525574\n","...Batch #30600 : Training Loss=0.3613806823641062, Training Accuracy=0.8412500023841858\n","...Batch #30700 : Training Loss=0.34952353209257125, Training Accuracy=0.8568750023841858\n","...Batch #30800 : Training Loss=0.3487470040470362, Training Accuracy=0.8499999642372131\n","...Batch #30900 : Training Loss=0.36385542873293164, Training Accuracy=0.8462499976158142\n","...Batch #31000 : Training Loss=0.33149802722036836, Training Accuracy=0.8524999618530273\n","...Batch #31100 : Training Loss=0.35803709633648395, Training Accuracy=0.8387500047683716\n","...Batch #31200 : Training Loss=0.3608810897171497, Training Accuracy=0.8418749570846558\n","...Batch #31300 : Training Loss=0.3546635486930609, Training Accuracy=0.8424999713897705\n","...Batch #31400 : Training Loss=0.3469801029935479, Training Accuracy=0.8568750023841858\n","...Batch #31500 : Training Loss=0.36638660199940204, Training Accuracy=0.8456249833106995\n","...Batch #31600 : Training Loss=0.36460480868816375, Training Accuracy=0.8431249856948853\n","...Batch #31700 : Training Loss=0.33576615385711195, Training Accuracy=0.8518750071525574\n","...Batch #31800 : Training Loss=0.36806521221995353, Training Accuracy=0.8399999737739563\n","...Batch #31900 : Training Loss=0.3393626745045185, Training Accuracy=0.8518750071525574\n","...Batch #32000 : Training Loss=0.35589752189815044, Training Accuracy=0.8456249833106995\n","...Batch #32100 : Training Loss=0.35912351958453653, Training Accuracy=0.8318749666213989\n","...Batch #32200 : Training Loss=0.367438093945384, Training Accuracy=0.8493750095367432\n","...Batch #32300 : Training Loss=0.3660709459334612, Training Accuracy=0.8456249833106995\n","...Batch #32400 : Training Loss=0.3810475032031536, Training Accuracy=0.84437495470047\n","...Batch #32500 : Training Loss=0.3274691005796194, Training Accuracy=0.8581249713897705\n","...Batch #32600 : Training Loss=0.35777613319456575, Training Accuracy=0.8637499809265137\n","...Batch #32700 : Training Loss=0.3511522778123617, Training Accuracy=0.8531249761581421\n","...Batch #32800 : Training Loss=0.36357651025056836, Training Accuracy=0.8412500023841858\n","...Batch #32900 : Training Loss=0.35088736928999426, Training Accuracy=0.859375\n","...Batch #33000 : Training Loss=0.35120608311146495, Training Accuracy=0.85999995470047\n","...Batch #33100 : Training Loss=0.37426438204944135, Training Accuracy=0.8493750095367432\n","...Batch #33200 : Training Loss=0.336735712364316, Training Accuracy=0.8549999594688416\n","...Batch #33300 : Training Loss=0.34017670802772043, Training Accuracy=0.8624999523162842\n","...Batch #33400 : Training Loss=0.36229473024606706, Training Accuracy=0.8418749570846558\n","...Batch #33500 : Training Loss=0.35694794714450834, Training Accuracy=0.8568750023841858\n","...Batch #33600 : Training Loss=0.35361298188567164, Training Accuracy=0.8512499928474426\n","...Batch #33700 : Training Loss=0.3415565679594874, Training Accuracy=0.8518750071525574\n","...Batch #33800 : Training Loss=0.3492308715730906, Training Accuracy=0.8368749618530273\n","...Batch #33900 : Training Loss=0.35521929010748865, Training Accuracy=0.8493750095367432\n","...Batch #34000 : Training Loss=0.35558041870594026, Training Accuracy=0.8499999642372131\n","...Batch #34100 : Training Loss=0.3774632681906223, Training Accuracy=0.8481249809265137\n","...Batch #34200 : Training Loss=0.3194778874143958, Training Accuracy=0.8675000071525574\n","...Batch #34300 : Training Loss=0.3602193243429065, Training Accuracy=0.8493750095367432\n","...Batch #34400 : Training Loss=0.36095201417803763, Training Accuracy=0.8418749570846558\n","...Batch #34500 : Training Loss=0.3527558445930481, Training Accuracy=0.8618749976158142\n","...Batch #34600 : Training Loss=0.3668745850771666, Training Accuracy=0.8537499904632568\n","...Batch #34700 : Training Loss=0.3336130478978157, Training Accuracy=0.859375\n","...Batch #34800 : Training Loss=0.34558953061699865, Training Accuracy=0.8381249904632568\n","...Batch #34900 : Training Loss=0.36196120485663413, Training Accuracy=0.8524999618530273\n","...Batch #35000 : Training Loss=0.36785709045827386, Training Accuracy=0.8412500023841858\n","...Batch #35100 : Training Loss=0.37064174070954325, Training Accuracy=0.8399999737739563\n","...Batch #35200 : Training Loss=0.38094085730612276, Training Accuracy=0.8418749570846558\n","...Batch #35300 : Training Loss=0.3565015384554863, Training Accuracy=0.8487499952316284\n","...Batch #35400 : Training Loss=0.33632688254117965, Training Accuracy=0.8512499928474426\n","...Batch #35500 : Training Loss=0.3511237258464098, Training Accuracy=0.8499999642372131\n","...Batch #35600 : Training Loss=0.3612498830258846, Training Accuracy=0.8499999642372131\n","...Batch #35700 : Training Loss=0.3436694823578, Training Accuracy=0.8612499833106995\n","...Batch #35800 : Training Loss=0.35560249716043474, Training Accuracy=0.8549999594688416\n","...Batch #35900 : Training Loss=0.34863641403615475, Training Accuracy=0.8506249785423279\n","...Batch #36000 : Training Loss=0.33621734142303467, Training Accuracy=0.8668749928474426\n","...Batch #36100 : Training Loss=0.3666614340990782, Training Accuracy=0.8493750095367432\n","...Batch #36200 : Training Loss=0.35077287800610063, Training Accuracy=0.8556249737739563\n","...Batch #36300 : Training Loss=0.35362458452582357, Training Accuracy=0.8506249785423279\n","Train: loss 0.3634278669922535, accuracy 0.8449392740362157\n","Valid: loss 0.36200924179163474, accuracy 0.8510318704284221, f1_ass 0.7872796791942553, f1_sweet 0.8853826440882367\n","####################################################################################################\n","Epoch 5 / 7\n","...Batch #100 : Training Loss=0.3592259100079536, Training Accuracy=0.8449999690055847\n","...Batch #200 : Training Loss=0.36085267961025236, Training Accuracy=0.840624988079071\n","...Batch #300 : Training Loss=0.3279538647085428, Training Accuracy=0.8631249666213989\n","...Batch #400 : Training Loss=0.348422113135457, Training Accuracy=0.8618749976158142\n","...Batch #500 : Training Loss=0.35459272399544717, Training Accuracy=0.8481249809265137\n","...Batch #600 : Training Loss=0.3411702229827642, Training Accuracy=0.8531249761581421\n","...Batch #700 : Training Loss=0.36117546536028383, Training Accuracy=0.8399999737739563\n","...Batch #800 : Training Loss=0.3455579891055822, Training Accuracy=0.8662499785423279\n","...Batch #900 : Training Loss=0.36985681161284445, Training Accuracy=0.8399999737739563\n","...Batch #1000 : Training Loss=0.3643106647580862, Training Accuracy=0.8431249856948853\n","...Batch #1100 : Training Loss=0.34122658796608446, Training Accuracy=0.8631249666213989\n","...Batch #1200 : Training Loss=0.34382218956947325, Training Accuracy=0.8481249809265137\n","...Batch #1300 : Training Loss=0.3464184519276023, Training Accuracy=0.8643749952316284\n","...Batch #1400 : Training Loss=0.3750078395009041, Training Accuracy=0.8387500047683716\n","...Batch #1500 : Training Loss=0.3222779082506895, Training Accuracy=0.8706249594688416\n","...Batch #1600 : Training Loss=0.33455168940126895, Training Accuracy=0.8574999570846558\n","...Batch #1700 : Training Loss=0.36024205304682255, Training Accuracy=0.8487499952316284\n","...Batch #1800 : Training Loss=0.34097003877162935, Training Accuracy=0.8512499928474426\n","...Batch #1900 : Training Loss=0.34956094563007356, Training Accuracy=0.8474999666213989\n","...Batch #2000 : Training Loss=0.3448652518540621, Training Accuracy=0.8543750047683716\n","...Batch #2100 : Training Loss=0.3391528214886785, Training Accuracy=0.8549999594688416\n","...Batch #2200 : Training Loss=0.34036884166300296, Training Accuracy=0.8618749976158142\n","...Batch #2300 : Training Loss=0.3554443979263306, Training Accuracy=0.8549999594688416\n","...Batch #2400 : Training Loss=0.3310127428174019, Training Accuracy=0.8581249713897705\n","...Batch #2500 : Training Loss=0.35922054298222067, Training Accuracy=0.8512499928474426\n","...Batch #2600 : Training Loss=0.33519116681069133, Training Accuracy=0.856249988079071\n","...Batch #2700 : Training Loss=0.37903757456690074, Training Accuracy=0.8449999690055847\n","...Batch #2800 : Training Loss=0.3461289287358522, Training Accuracy=0.8531249761581421\n","...Batch #2900 : Training Loss=0.38240099623799323, Training Accuracy=0.8343749642372131\n","...Batch #3000 : Training Loss=0.3272684195637703, Training Accuracy=0.8543750047683716\n","...Batch #3100 : Training Loss=0.3399786642566323, Training Accuracy=0.8474999666213989\n","...Batch #3200 : Training Loss=0.37154860407114026, Training Accuracy=0.82874995470047\n","...Batch #3300 : Training Loss=0.34142373103648427, Training Accuracy=0.8612499833106995\n","...Batch #3400 : Training Loss=0.3460108199715614, Training Accuracy=0.8499999642372131\n","...Batch #3500 : Training Loss=0.3438855780661106, Training Accuracy=0.8499999642372131\n","...Batch #3600 : Training Loss=0.34544939517974854, Training Accuracy=0.859375\n","...Batch #3700 : Training Loss=0.32381694674491884, Training Accuracy=0.8725000023841858\n","...Batch #3800 : Training Loss=0.3503880876302719, Training Accuracy=0.8487499952316284\n","...Batch #3900 : Training Loss=0.35603748582303524, Training Accuracy=0.840624988079071\n","...Batch #4000 : Training Loss=0.3621675511449575, Training Accuracy=0.8531249761581421\n","...Batch #4100 : Training Loss=0.36413093976676464, Training Accuracy=0.8506249785423279\n","...Batch #4200 : Training Loss=0.33298469193279745, Training Accuracy=0.8606249690055847\n","...Batch #4300 : Training Loss=0.3437744053453207, Training Accuracy=0.8624999523162842\n","...Batch #4400 : Training Loss=0.32105807941406966, Training Accuracy=0.8662499785423279\n","...Batch #4500 : Training Loss=0.3348139728978276, Training Accuracy=0.8643749952316284\n","...Batch #4600 : Training Loss=0.37427446857094765, Training Accuracy=0.8474999666213989\n","...Batch #4700 : Training Loss=0.3630269193649292, Training Accuracy=0.8268749713897705\n","...Batch #4800 : Training Loss=0.3887848475575447, Training Accuracy=0.8324999809265137\n","...Batch #4900 : Training Loss=0.36521099478006364, Training Accuracy=0.8431249856948853\n","...Batch #5000 : Training Loss=0.3427580218017101, Training Accuracy=0.8531249761581421\n","...Batch #5100 : Training Loss=0.37804530970752237, Training Accuracy=0.8424999713897705\n","...Batch #5200 : Training Loss=0.33771188046783207, Training Accuracy=0.856249988079071\n","...Batch #5300 : Training Loss=0.35005673073232174, Training Accuracy=0.8462499976158142\n","...Batch #5400 : Training Loss=0.3141615181043744, Training Accuracy=0.8700000047683716\n","...Batch #5500 : Training Loss=0.3513925552368164, Training Accuracy=0.8637499809265137\n","...Batch #5600 : Training Loss=0.36303179532289503, Training Accuracy=0.8537499904632568\n","...Batch #5700 : Training Loss=0.3423746280372143, Training Accuracy=0.8543750047683716\n","...Batch #5800 : Training Loss=0.3815006560832262, Training Accuracy=0.8343749642372131\n","...Batch #5900 : Training Loss=0.35238021440804007, Training Accuracy=0.8487499952316284\n","...Batch #6000 : Training Loss=0.35786136366426946, Training Accuracy=0.8506249785423279\n","...Batch #6100 : Training Loss=0.35464310020208356, Training Accuracy=0.8499999642372131\n","...Batch #6200 : Training Loss=0.3528814635425806, Training Accuracy=0.8556249737739563\n","...Batch #6300 : Training Loss=0.33883874729275704, Training Accuracy=0.8499999642372131\n","...Batch #6400 : Training Loss=0.3548550598323345, Training Accuracy=0.8412500023841858\n","...Batch #6500 : Training Loss=0.3454666816443205, Training Accuracy=0.859375\n","...Batch #6600 : Training Loss=0.33736611645668746, Training Accuracy=0.8618749976158142\n","...Batch #6700 : Training Loss=0.35082048624753953, Training Accuracy=0.8581249713897705\n","...Batch #6800 : Training Loss=0.3638594002276659, Training Accuracy=0.8499999642372131\n","...Batch #6900 : Training Loss=0.3361712582409382, Training Accuracy=0.8456249833106995\n","...Batch #7000 : Training Loss=0.33644968941807746, Training Accuracy=0.8618749976158142\n","...Batch #7100 : Training Loss=0.3367349757254124, Training Accuracy=0.8587499856948853\n","...Batch #7200 : Training Loss=0.32923568423837424, Training Accuracy=0.8712499737739563\n","...Batch #7300 : Training Loss=0.35535723231732846, Training Accuracy=0.8431249856948853\n","...Batch #7400 : Training Loss=0.3253481702506542, Training Accuracy=0.8737499713897705\n","...Batch #7500 : Training Loss=0.3566786640137434, Training Accuracy=0.8468749523162842\n","...Batch #7600 : Training Loss=0.3462492353469133, Training Accuracy=0.8587499856948853\n","...Batch #7700 : Training Loss=0.37000649973750116, Training Accuracy=0.8393749594688416\n","...Batch #7800 : Training Loss=0.36319458201527594, Training Accuracy=0.84437495470047\n","...Batch #7900 : Training Loss=0.3623836186900735, Training Accuracy=0.8474999666213989\n","...Batch #8000 : Training Loss=0.3409971544146538, Training Accuracy=0.8624999523162842\n","...Batch #8100 : Training Loss=0.3529733406379819, Training Accuracy=0.8524999618530273\n","...Batch #8200 : Training Loss=0.3495363043248653, Training Accuracy=0.8531249761581421\n","...Batch #8300 : Training Loss=0.3441064359247685, Training Accuracy=0.8556249737739563\n","...Batch #8400 : Training Loss=0.3444119318574667, Training Accuracy=0.8618749976158142\n","...Batch #8500 : Training Loss=0.3420464368164539, Training Accuracy=0.8556249737739563\n","...Batch #8600 : Training Loss=0.3242906032502651, Training Accuracy=0.8624999523162842\n","...Batch #8700 : Training Loss=0.34447397239506244, Training Accuracy=0.8474999666213989\n","...Batch #8800 : Training Loss=0.36199893862009047, Training Accuracy=0.8481249809265137\n","...Batch #8900 : Training Loss=0.3425386245921254, Training Accuracy=0.8624999523162842\n","...Batch #9000 : Training Loss=0.3786810703575611, Training Accuracy=0.8318749666213989\n","...Batch #9100 : Training Loss=0.3515112344175577, Training Accuracy=0.8499999642372131\n","...Batch #9200 : Training Loss=0.36761157512664794, Training Accuracy=0.8474999666213989\n","...Batch #9300 : Training Loss=0.3329468707740307, Training Accuracy=0.8687499761581421\n","...Batch #9400 : Training Loss=0.31016213208436966, Training Accuracy=0.8631249666213989\n","...Batch #9500 : Training Loss=0.3658122538775206, Training Accuracy=0.8481249809265137\n","...Batch #9600 : Training Loss=0.31439438747242093, Training Accuracy=0.8681249618530273\n","...Batch #9700 : Training Loss=0.3429660688340664, Training Accuracy=0.8543750047683716\n","...Batch #9800 : Training Loss=0.39290285244584083, Training Accuracy=0.8381249904632568\n","...Batch #9900 : Training Loss=0.34413376957178116, Training Accuracy=0.84437495470047\n","...Batch #10000 : Training Loss=0.35322294481098654, Training Accuracy=0.8549999594688416\n","...Batch #10100 : Training Loss=0.3685208740830421, Training Accuracy=0.8393749594688416\n","...Batch #10200 : Training Loss=0.3451578046381474, Training Accuracy=0.8424999713897705\n","...Batch #10300 : Training Loss=0.3634791835397482, Training Accuracy=0.8499999642372131\n","...Batch #10400 : Training Loss=0.33160326108336446, Training Accuracy=0.859375\n","...Batch #10500 : Training Loss=0.3318431270122528, Training Accuracy=0.8612499833106995\n","...Batch #10600 : Training Loss=0.3803395229578018, Training Accuracy=0.840624988079071\n","...Batch #10700 : Training Loss=0.34854468077421186, Training Accuracy=0.8481249809265137\n","...Batch #10800 : Training Loss=0.34731214709579944, Training Accuracy=0.8462499976158142\n","...Batch #10900 : Training Loss=0.31409270942211154, Training Accuracy=0.8681249618530273\n","...Batch #11000 : Training Loss=0.38761802405118945, Training Accuracy=0.8343749642372131\n","...Batch #11100 : Training Loss=0.3540026888251305, Training Accuracy=0.8524999618530273\n","...Batch #11200 : Training Loss=0.3377430460602045, Training Accuracy=0.8549999594688416\n","...Batch #11300 : Training Loss=0.3647840891033411, Training Accuracy=0.84437495470047\n","...Batch #11400 : Training Loss=0.3520802742615342, Training Accuracy=0.8549999594688416\n","...Batch #11500 : Training Loss=0.33488842073827985, Training Accuracy=0.8618749976158142\n","...Batch #11600 : Training Loss=0.34902379728853705, Training Accuracy=0.8506249785423279\n","...Batch #11700 : Training Loss=0.3288529372215271, Training Accuracy=0.8612499833106995\n","...Batch #11800 : Training Loss=0.3927981377393007, Training Accuracy=0.824999988079071\n","...Batch #11900 : Training Loss=0.36237561121582984, Training Accuracy=0.8512499928474426\n","...Batch #12000 : Training Loss=0.3530988645553589, Training Accuracy=0.8531249761581421\n","...Batch #12100 : Training Loss=0.36024124927818774, Training Accuracy=0.8462499976158142\n","...Batch #12200 : Training Loss=0.34915798224508765, Training Accuracy=0.8493750095367432\n","...Batch #12300 : Training Loss=0.3877547011896968, Training Accuracy=0.8312499523162842\n","...Batch #12400 : Training Loss=0.32356023840606213, Training Accuracy=0.8687499761581421\n","...Batch #12500 : Training Loss=0.3749136970937252, Training Accuracy=0.8449999690055847\n","...Batch #12600 : Training Loss=0.32078709580004217, Training Accuracy=0.8581249713897705\n","...Batch #12700 : Training Loss=0.3710828069038689, Training Accuracy=0.8412500023841858\n","...Batch #12800 : Training Loss=0.3549924781918526, Training Accuracy=0.8531249761581421\n","...Batch #12900 : Training Loss=0.3574111671745777, Training Accuracy=0.8481249809265137\n","...Batch #13000 : Training Loss=0.3514160401374102, Training Accuracy=0.8543750047683716\n","...Batch #13100 : Training Loss=0.32598024982959034, Training Accuracy=0.8656249642372131\n","...Batch #13200 : Training Loss=0.35737707920372486, Training Accuracy=0.8487499952316284\n","...Batch #13300 : Training Loss=0.3596430805325508, Training Accuracy=0.85999995470047\n","...Batch #13400 : Training Loss=0.3295220927894115, Training Accuracy=0.8624999523162842\n","...Batch #13500 : Training Loss=0.31701142348349093, Training Accuracy=0.8774999976158142\n","...Batch #13600 : Training Loss=0.35275136940181256, Training Accuracy=0.8487499952316284\n","...Batch #13700 : Training Loss=0.3437429588288069, Training Accuracy=0.8624999523162842\n","...Batch #13800 : Training Loss=0.34724185258150103, Training Accuracy=0.8637499809265137\n","...Batch #13900 : Training Loss=0.3434210047870874, Training Accuracy=0.8518750071525574\n","...Batch #14000 : Training Loss=0.32591469667851924, Training Accuracy=0.8574999570846558\n","...Batch #14100 : Training Loss=0.32132769383490084, Training Accuracy=0.8681249618530273\n","...Batch #14200 : Training Loss=0.32483481854200363, Training Accuracy=0.8631249666213989\n","...Batch #14300 : Training Loss=0.34160402975976467, Training Accuracy=0.8531249761581421\n","...Batch #14400 : Training Loss=0.3498137565329671, Training Accuracy=0.8568750023841858\n","...Batch #14500 : Training Loss=0.33303532674908637, Training Accuracy=0.8662499785423279\n","...Batch #14600 : Training Loss=0.3370032648742199, Training Accuracy=0.8512499928474426\n","...Batch #14700 : Training Loss=0.33135827362537384, Training Accuracy=0.8537499904632568\n","...Batch #14800 : Training Loss=0.3414886815845966, Training Accuracy=0.8531249761581421\n","...Batch #14900 : Training Loss=0.3254736179858446, Training Accuracy=0.8581249713897705\n","...Batch #15000 : Training Loss=0.34785069063305857, Training Accuracy=0.8487499952316284\n","...Batch #15100 : Training Loss=0.3478729021549225, Training Accuracy=0.8537499904632568\n","...Batch #15200 : Training Loss=0.33574931368231775, Training Accuracy=0.8606249690055847\n","...Batch #15300 : Training Loss=0.3664998283237219, Training Accuracy=0.8474999666213989\n","...Batch #15400 : Training Loss=0.35796427451074125, Training Accuracy=0.8449999690055847\n","...Batch #15500 : Training Loss=0.3340736179053783, Training Accuracy=0.8637499809265137\n","...Batch #15600 : Training Loss=0.33811406277120115, Training Accuracy=0.8556249737739563\n","...Batch #15700 : Training Loss=0.3386303836852312, Training Accuracy=0.8675000071525574\n","...Batch #15800 : Training Loss=0.36021921582520006, Training Accuracy=0.8481249809265137\n","...Batch #15900 : Training Loss=0.3523043655604124, Training Accuracy=0.840624988079071\n","...Batch #16000 : Training Loss=0.331759010180831, Training Accuracy=0.8618749976158142\n","...Batch #16100 : Training Loss=0.3905124405026436, Training Accuracy=0.8362500071525574\n","...Batch #16200 : Training Loss=0.33966799594461916, Training Accuracy=0.856249988079071\n","...Batch #16300 : Training Loss=0.3806081534177065, Training Accuracy=0.8481249809265137\n","...Batch #16400 : Training Loss=0.3595713759213686, Training Accuracy=0.8431249856948853\n","...Batch #16500 : Training Loss=0.32924207270145417, Training Accuracy=0.8700000047683716\n","...Batch #16600 : Training Loss=0.34404694229364396, Training Accuracy=0.8675000071525574\n","...Batch #16700 : Training Loss=0.33382857631891966, Training Accuracy=0.8581249713897705\n","...Batch #16800 : Training Loss=0.3910108868777752, Training Accuracy=0.8293749690055847\n","...Batch #16900 : Training Loss=0.3338657915964723, Training Accuracy=0.8524999618530273\n","...Batch #17000 : Training Loss=0.37556816086173056, Training Accuracy=0.8499999642372131\n","...Batch #17100 : Training Loss=0.33774612590670583, Training Accuracy=0.8537499904632568\n","...Batch #17200 : Training Loss=0.3569081012904644, Training Accuracy=0.8399999737739563\n","...Batch #17300 : Training Loss=0.3431087926402688, Training Accuracy=0.8581249713897705\n","...Batch #17400 : Training Loss=0.3246427467465401, Training Accuracy=0.8574999570846558\n","...Batch #17500 : Training Loss=0.32320653591305015, Training Accuracy=0.8581249713897705\n","...Batch #17600 : Training Loss=0.3639581776410341, Training Accuracy=0.8549999594688416\n","...Batch #17700 : Training Loss=0.32461924873292447, Training Accuracy=0.871874988079071\n","...Batch #17800 : Training Loss=0.3298927427828312, Training Accuracy=0.8549999594688416\n","...Batch #17900 : Training Loss=0.36249913930892946, Training Accuracy=0.8506249785423279\n","...Batch #18000 : Training Loss=0.34475816305726764, Training Accuracy=0.8524999618530273\n","...Batch #18100 : Training Loss=0.3610397505015135, Training Accuracy=0.84375\n","...Batch #18200 : Training Loss=0.36262071900069714, Training Accuracy=0.8387500047683716\n","...Batch #18300 : Training Loss=0.328250322714448, Training Accuracy=0.8612499833106995\n","...Batch #18400 : Training Loss=0.3511761114001274, Training Accuracy=0.8643749952316284\n","...Batch #18500 : Training Loss=0.3277584157139063, Training Accuracy=0.8643749952316284\n","...Batch #18600 : Training Loss=0.341042078435421, Training Accuracy=0.8606249690055847\n","...Batch #18700 : Training Loss=0.3233282831311226, Training Accuracy=0.856249988079071\n","...Batch #18800 : Training Loss=0.3236661372333765, Training Accuracy=0.8643749952316284\n","...Batch #18900 : Training Loss=0.3900897828862071, Training Accuracy=0.8231250047683716\n","...Batch #19000 : Training Loss=0.34762909322977065, Training Accuracy=0.8656249642372131\n","...Batch #19100 : Training Loss=0.3759968000650406, Training Accuracy=0.8468749523162842\n","...Batch #19200 : Training Loss=0.3373332500085235, Training Accuracy=0.8549999594688416\n","...Batch #19300 : Training Loss=0.34800139561295507, Training Accuracy=0.8524999618530273\n","...Batch #19400 : Training Loss=0.36242353796958926, Training Accuracy=0.8556249737739563\n","...Batch #19500 : Training Loss=0.3419339636713266, Training Accuracy=0.8524999618530273\n","...Batch #19600 : Training Loss=0.3398434489220381, Training Accuracy=0.856249988079071\n","...Batch #19700 : Training Loss=0.3628608614951372, Training Accuracy=0.8481249809265137\n","...Batch #19800 : Training Loss=0.36013617672026155, Training Accuracy=0.8518750071525574\n","...Batch #19900 : Training Loss=0.3494961005449295, Training Accuracy=0.8424999713897705\n","...Batch #20000 : Training Loss=0.35191061697900294, Training Accuracy=0.8518750071525574\n","...Batch #20100 : Training Loss=0.32097047448158267, Training Accuracy=0.8675000071525574\n","...Batch #20200 : Training Loss=0.3210475983470678, Training Accuracy=0.8675000071525574\n","...Batch #20300 : Training Loss=0.34063141606748104, Training Accuracy=0.8606249690055847\n","...Batch #20400 : Training Loss=0.35099202491343023, Training Accuracy=0.8581249713897705\n","...Batch #20500 : Training Loss=0.33911939814686776, Training Accuracy=0.8624999523162842\n","...Batch #20600 : Training Loss=0.37816223219037054, Training Accuracy=0.8343749642372131\n","...Batch #20700 : Training Loss=0.3234048643708229, Training Accuracy=0.8656249642372131\n","...Batch #20800 : Training Loss=0.34152245268225667, Training Accuracy=0.8518750071525574\n","...Batch #20900 : Training Loss=0.36428765542805197, Training Accuracy=0.8474999666213989\n","...Batch #21000 : Training Loss=0.35046751067042353, Training Accuracy=0.8537499904632568\n","...Batch #21100 : Training Loss=0.330334809422493, Training Accuracy=0.8631249666213989\n","...Batch #21200 : Training Loss=0.3224053755030036, Training Accuracy=0.8662499785423279\n","...Batch #21300 : Training Loss=0.3136150112748146, Training Accuracy=0.8731249570846558\n","...Batch #21400 : Training Loss=0.332768685631454, Training Accuracy=0.871874988079071\n","...Batch #21500 : Training Loss=0.34956951953470705, Training Accuracy=0.8543750047683716\n","...Batch #21600 : Training Loss=0.36078871600329876, Training Accuracy=0.84375\n","...Batch #21700 : Training Loss=0.33924985386431217, Training Accuracy=0.8581249713897705\n","...Batch #21800 : Training Loss=0.29638183657079936, Training Accuracy=0.8806250095367432\n","...Batch #21900 : Training Loss=0.3538311542198062, Training Accuracy=0.8574999570846558\n","...Batch #22000 : Training Loss=0.355731566734612, Training Accuracy=0.8531249761581421\n","...Batch #22100 : Training Loss=0.3512035738304257, Training Accuracy=0.8468749523162842\n","...Batch #22200 : Training Loss=0.328175562992692, Training Accuracy=0.8531249761581421\n","...Batch #22300 : Training Loss=0.3423715590685606, Training Accuracy=0.8462499976158142\n","...Batch #22400 : Training Loss=0.34492191709578035, Training Accuracy=0.8543750047683716\n","...Batch #22500 : Training Loss=0.3812565379589796, Training Accuracy=0.8399999737739563\n","...Batch #22600 : Training Loss=0.34021378044039013, Training Accuracy=0.8537499904632568\n","...Batch #22700 : Training Loss=0.37176733020693065, Training Accuracy=0.84437495470047\n","...Batch #22800 : Training Loss=0.3305231815576553, Training Accuracy=0.8631249666213989\n","...Batch #22900 : Training Loss=0.32599262975156307, Training Accuracy=0.8618749976158142\n","...Batch #23000 : Training Loss=0.3313914982974529, Training Accuracy=0.8587499856948853\n","...Batch #23100 : Training Loss=0.3376551831886172, Training Accuracy=0.8587499856948853\n","...Batch #23200 : Training Loss=0.35692287895828484, Training Accuracy=0.8531249761581421\n","...Batch #23300 : Training Loss=0.3336001696437597, Training Accuracy=0.8537499904632568\n","...Batch #23400 : Training Loss=0.3387964956462383, Training Accuracy=0.8650000095367432\n","...Batch #23500 : Training Loss=0.31144979313015936, Training Accuracy=0.8650000095367432\n","...Batch #23600 : Training Loss=0.3786995851993561, Training Accuracy=0.8424999713897705\n","...Batch #23700 : Training Loss=0.33895212054252627, Training Accuracy=0.8624999523162842\n","...Batch #23800 : Training Loss=0.34005174282938244, Training Accuracy=0.8574999570846558\n","...Batch #23900 : Training Loss=0.36318055860698223, Training Accuracy=0.8656249642372131\n","...Batch #24000 : Training Loss=0.31881660103797915, Training Accuracy=0.8606249690055847\n","...Batch #24100 : Training Loss=0.3239569480344653, Training Accuracy=0.8681249618530273\n","...Batch #24200 : Training Loss=0.3329890010505915, Training Accuracy=0.8556249737739563\n","...Batch #24300 : Training Loss=0.3346818058937788, Training Accuracy=0.8631249666213989\n","...Batch #24400 : Training Loss=0.3263908990100026, Training Accuracy=0.8618749976158142\n","...Batch #24500 : Training Loss=0.38092468239367006, Training Accuracy=0.8431249856948853\n","...Batch #24600 : Training Loss=0.3643219730257988, Training Accuracy=0.8399999737739563\n","...Batch #24700 : Training Loss=0.3744006425887346, Training Accuracy=0.84437495470047\n","...Batch #24800 : Training Loss=0.34290989898145197, Training Accuracy=0.8606249690055847\n","...Batch #24900 : Training Loss=0.3152940769866109, Training Accuracy=0.8706249594688416\n","...Batch #25000 : Training Loss=0.34412348724901676, Training Accuracy=0.8650000095367432\n","...Batch #25100 : Training Loss=0.33762092761695384, Training Accuracy=0.8612499833106995\n","...Batch #25200 : Training Loss=0.36462850935757163, Training Accuracy=0.8412500023841858\n","...Batch #25300 : Training Loss=0.32761839784681795, Training Accuracy=0.8650000095367432\n","...Batch #25400 : Training Loss=0.31972895193845036, Training Accuracy=0.8687499761581421\n","...Batch #25500 : Training Loss=0.33265458393841985, Training Accuracy=0.8506249785423279\n","...Batch #25600 : Training Loss=0.36323173172771933, Training Accuracy=0.8481249809265137\n","...Batch #25700 : Training Loss=0.3640622216463089, Training Accuracy=0.8381249904632568\n","...Batch #25800 : Training Loss=0.3419863686710596, Training Accuracy=0.8512499928474426\n","...Batch #25900 : Training Loss=0.345179925635457, Training Accuracy=0.8474999666213989\n","...Batch #26000 : Training Loss=0.327628725245595, Training Accuracy=0.8537499904632568\n","...Batch #26100 : Training Loss=0.347906626611948, Training Accuracy=0.8637499809265137\n","...Batch #26200 : Training Loss=0.36070604413747787, Training Accuracy=0.8431249856948853\n","...Batch #26300 : Training Loss=0.30697649493813517, Training Accuracy=0.8725000023841858\n","...Batch #26400 : Training Loss=0.32291101790964605, Training Accuracy=0.8631249666213989\n","...Batch #26500 : Training Loss=0.35044718213379383, Training Accuracy=0.8556249737739563\n","...Batch #26600 : Training Loss=0.34413861904293297, Training Accuracy=0.8512499928474426\n","...Batch #26700 : Training Loss=0.3197823765873909, Training Accuracy=0.8581249713897705\n","...Batch #26800 : Training Loss=0.30937461111694575, Training Accuracy=0.8812499642372131\n","...Batch #26900 : Training Loss=0.3347694469988346, Training Accuracy=0.8643749952316284\n","...Batch #27000 : Training Loss=0.3316385168209672, Training Accuracy=0.8618749976158142\n","...Batch #27100 : Training Loss=0.338638164922595, Training Accuracy=0.85999995470047\n","...Batch #27200 : Training Loss=0.3195885031670332, Training Accuracy=0.87562495470047\n","...Batch #27300 : Training Loss=0.327367278970778, Training Accuracy=0.87562495470047\n","...Batch #27400 : Training Loss=0.36207287810742855, Training Accuracy=0.8537499904632568\n","...Batch #27500 : Training Loss=0.36024833012372254, Training Accuracy=0.8524999618530273\n","...Batch #27600 : Training Loss=0.3600398737192154, Training Accuracy=0.8493750095367432\n","...Batch #27700 : Training Loss=0.3621098244190216, Training Accuracy=0.8587499856948853\n","...Batch #27800 : Training Loss=0.35195882722735405, Training Accuracy=0.8518750071525574\n","...Batch #27900 : Training Loss=0.34416242107748984, Training Accuracy=0.8543750047683716\n","...Batch #28000 : Training Loss=0.35042736895382404, Training Accuracy=0.8506249785423279\n","...Batch #28100 : Training Loss=0.32981095395982263, Training Accuracy=0.8581249713897705\n","...Batch #28200 : Training Loss=0.3296685845404863, Training Accuracy=0.8612499833106995\n","...Batch #28300 : Training Loss=0.34301008090376855, Training Accuracy=0.8587499856948853\n","...Batch #28400 : Training Loss=0.3817403896898031, Training Accuracy=0.8412500023841858\n","...Batch #28500 : Training Loss=0.3432513286173344, Training Accuracy=0.8700000047683716\n","...Batch #28600 : Training Loss=0.3165287324786186, Training Accuracy=0.8693749904632568\n","...Batch #28700 : Training Loss=0.36402394860982895, Training Accuracy=0.8381249904632568\n","...Batch #28800 : Training Loss=0.3524573303759098, Training Accuracy=0.8412500023841858\n","...Batch #28900 : Training Loss=0.3277841128408909, Training Accuracy=0.8700000047683716\n","...Batch #29000 : Training Loss=0.3339019212126732, Training Accuracy=0.8587499856948853\n","...Batch #29100 : Training Loss=0.3345025485008955, Training Accuracy=0.8643749952316284\n","...Batch #29200 : Training Loss=0.3535362705588341, Training Accuracy=0.8474999666213989\n","...Batch #29300 : Training Loss=0.31531160436570643, Training Accuracy=0.8643749952316284\n","...Batch #29400 : Training Loss=0.34076795253902675, Training Accuracy=0.8637499809265137\n","...Batch #29500 : Training Loss=0.3394935543835163, Training Accuracy=0.8574999570846558\n","...Batch #29600 : Training Loss=0.3576318646594882, Training Accuracy=0.8468749523162842\n","...Batch #29700 : Training Loss=0.33598360151052475, Training Accuracy=0.8650000095367432\n","...Batch #29800 : Training Loss=0.3531235999614, Training Accuracy=0.8556249737739563\n","...Batch #29900 : Training Loss=0.3403641214221716, Training Accuracy=0.8606249690055847\n","...Batch #30000 : Training Loss=0.3382949440553784, Training Accuracy=0.8650000095367432\n","...Batch #30100 : Training Loss=0.33694943569600583, Training Accuracy=0.8624999523162842\n","...Batch #30200 : Training Loss=0.33791976653039457, Training Accuracy=0.8662499785423279\n","...Batch #30300 : Training Loss=0.3048134123906493, Training Accuracy=0.875\n","...Batch #30400 : Training Loss=0.33377396292984485, Training Accuracy=0.8650000095367432\n","...Batch #30500 : Training Loss=0.3574689017236233, Training Accuracy=0.8449999690055847\n","...Batch #30600 : Training Loss=0.34004170410335066, Training Accuracy=0.85999995470047\n","...Batch #30700 : Training Loss=0.34254307992756367, Training Accuracy=0.8568750023841858\n","...Batch #30800 : Training Loss=0.3266741918027401, Training Accuracy=0.8637499809265137\n","...Batch #30900 : Training Loss=0.3472501604259014, Training Accuracy=0.8493750095367432\n","...Batch #31000 : Training Loss=0.32217805113643405, Training Accuracy=0.8631249666213989\n","...Batch #31100 : Training Loss=0.34692517302930354, Training Accuracy=0.8568750023841858\n","...Batch #31200 : Training Loss=0.3382272865623236, Training Accuracy=0.8556249737739563\n","...Batch #31300 : Training Loss=0.336928636059165, Training Accuracy=0.8493750095367432\n","...Batch #31400 : Training Loss=0.3231777486950159, Training Accuracy=0.8650000095367432\n","...Batch #31500 : Training Loss=0.3430728525668383, Training Accuracy=0.8549999594688416\n","...Batch #31600 : Training Loss=0.3556248654425144, Training Accuracy=0.84375\n","...Batch #31700 : Training Loss=0.32154771164059637, Training Accuracy=0.856249988079071\n","...Batch #31800 : Training Loss=0.3423109661787748, Training Accuracy=0.8481249809265137\n","...Batch #31900 : Training Loss=0.3336118617281318, Training Accuracy=0.8624999523162842\n","...Batch #32000 : Training Loss=0.3328415126353502, Training Accuracy=0.8650000095367432\n","...Batch #32100 : Training Loss=0.34133693419396877, Training Accuracy=0.8587499856948853\n","...Batch #32200 : Training Loss=0.3496164271235466, Training Accuracy=0.8531249761581421\n","...Batch #32300 : Training Loss=0.35611705631017687, Training Accuracy=0.856249988079071\n","...Batch #32400 : Training Loss=0.35217803925275804, Training Accuracy=0.8556249737739563\n","...Batch #32500 : Training Loss=0.3090270604938269, Training Accuracy=0.8606249690055847\n","...Batch #32600 : Training Loss=0.34151168167591095, Training Accuracy=0.8606249690055847\n","...Batch #32700 : Training Loss=0.3377585946023464, Training Accuracy=0.8624999523162842\n","...Batch #32800 : Training Loss=0.33485051944851874, Training Accuracy=0.85999995470047\n","...Batch #32900 : Training Loss=0.3348630503565073, Training Accuracy=0.8556249737739563\n","...Batch #33000 : Training Loss=0.33840973421931264, Training Accuracy=0.8656249642372131\n","...Batch #33100 : Training Loss=0.3574433422088623, Training Accuracy=0.8449999690055847\n","...Batch #33200 : Training Loss=0.3267165495082736, Training Accuracy=0.871874988079071\n","...Batch #33300 : Training Loss=0.3368998381122947, Training Accuracy=0.8650000095367432\n","...Batch #33400 : Training Loss=0.3433002281934023, Training Accuracy=0.8631249666213989\n","...Batch #33500 : Training Loss=0.32959016181528566, Training Accuracy=0.8624999523162842\n","...Batch #33600 : Training Loss=0.33102335792034865, Training Accuracy=0.8624999523162842\n","...Batch #33700 : Training Loss=0.318984206430614, Training Accuracy=0.8568750023841858\n","...Batch #33800 : Training Loss=0.3231912036240101, Training Accuracy=0.8537499904632568\n","...Batch #33900 : Training Loss=0.3422107976302505, Training Accuracy=0.8581249713897705\n","...Batch #34000 : Training Loss=0.31575684651732444, Training Accuracy=0.8681249618530273\n","...Batch #34100 : Training Loss=0.36086118064820766, Training Accuracy=0.8506249785423279\n","...Batch #34200 : Training Loss=0.295766604244709, Training Accuracy=0.8787499666213989\n","...Batch #34300 : Training Loss=0.3382118917629123, Training Accuracy=0.8662499785423279\n","...Batch #34400 : Training Loss=0.3459760408848524, Training Accuracy=0.8512499928474426\n","...Batch #34500 : Training Loss=0.3530400931835175, Training Accuracy=0.8581249713897705\n","...Batch #34600 : Training Loss=0.3484088682383299, Training Accuracy=0.8612499833106995\n","...Batch #34700 : Training Loss=0.3172282252833247, Training Accuracy=0.8637499809265137\n","...Batch #34800 : Training Loss=0.3326243395730853, Training Accuracy=0.8518750071525574\n","...Batch #34900 : Training Loss=0.3281996855139732, Training Accuracy=0.8587499856948853\n","...Batch #35000 : Training Loss=0.3453010790795088, Training Accuracy=0.8549999594688416\n","...Batch #35100 : Training Loss=0.3519576333835721, Training Accuracy=0.8456249833106995\n","...Batch #35200 : Training Loss=0.3466118489950895, Training Accuracy=0.8481249809265137\n","...Batch #35300 : Training Loss=0.33991042025387286, Training Accuracy=0.8650000095367432\n","...Batch #35400 : Training Loss=0.327211345769465, Training Accuracy=0.8581249713897705\n","...Batch #35500 : Training Loss=0.3397508358582854, Training Accuracy=0.8518750071525574\n","...Batch #35600 : Training Loss=0.3375415747612715, Training Accuracy=0.8606249690055847\n","...Batch #35700 : Training Loss=0.33136872444301846, Training Accuracy=0.85999995470047\n","...Batch #35800 : Training Loss=0.3430150230973959, Training Accuracy=0.859375\n","...Batch #35900 : Training Loss=0.33780607908964155, Training Accuracy=0.8587499856948853\n","...Batch #36000 : Training Loss=0.30441769044846295, Training Accuracy=0.8812499642372131\n","...Batch #36100 : Training Loss=0.333895061314106, Training Accuracy=0.8650000095367432\n","...Batch #36200 : Training Loss=0.3343785512447357, Training Accuracy=0.8687499761581421\n","...Batch #36300 : Training Loss=0.3251257072389126, Training Accuracy=0.8700000047683716\n","Train: loss 0.3444807001785477, accuracy 0.8553241694831424\n","Valid: loss 0.35209949555745507, accuracy 0.8550156739811913, f1_ass 0.7955424571744336, f1_sweet 0.8876859253263178\n","####################################################################################################\n","Epoch 6 / 7\n","...Batch #100 : Training Loss=0.3403664299100637, Training Accuracy=0.8531249761581421\n","...Batch #200 : Training Loss=0.3397111924737692, Training Accuracy=0.8581249713897705\n","...Batch #300 : Training Loss=0.31015147127211096, Training Accuracy=0.871874988079071\n","...Batch #400 : Training Loss=0.33303582467138765, Training Accuracy=0.8706249594688416\n","...Batch #500 : Training Loss=0.3472342376410961, Training Accuracy=0.8637499809265137\n","...Batch #600 : Training Loss=0.32543012000620364, Training Accuracy=0.8656249642372131\n","...Batch #700 : Training Loss=0.353805827498436, Training Accuracy=0.8568750023841858\n","...Batch #800 : Training Loss=0.3234569110162556, Training Accuracy=0.8731249570846558\n","...Batch #900 : Training Loss=0.3383755841851234, Training Accuracy=0.8581249713897705\n","...Batch #1000 : Training Loss=0.3323880007863045, Training Accuracy=0.8656249642372131\n","...Batch #1100 : Training Loss=0.31710266970098017, Training Accuracy=0.87562495470047\n","...Batch #1200 : Training Loss=0.3275079672038555, Training Accuracy=0.8662499785423279\n","...Batch #1300 : Training Loss=0.3337121643126011, Training Accuracy=0.8731249570846558\n","...Batch #1400 : Training Loss=0.3567745089530945, Training Accuracy=0.8549999594688416\n","...Batch #1500 : Training Loss=0.29882912192493677, Training Accuracy=0.8856250047683716\n","...Batch #1600 : Training Loss=0.3107847948744893, Training Accuracy=0.8768749833106995\n","...Batch #1700 : Training Loss=0.3419049904868007, Training Accuracy=0.8537499904632568\n","...Batch #1800 : Training Loss=0.32970764342695474, Training Accuracy=0.8656249642372131\n","...Batch #1900 : Training Loss=0.34271749943494795, Training Accuracy=0.8581249713897705\n","...Batch #2000 : Training Loss=0.3246512671560049, Training Accuracy=0.8587499856948853\n","...Batch #2100 : Training Loss=0.32470347814261913, Training Accuracy=0.8668749928474426\n","...Batch #2200 : Training Loss=0.3299364960566163, Training Accuracy=0.859375\n","...Batch #2300 : Training Loss=0.3262718188762665, Training Accuracy=0.8681249618530273\n","...Batch #2400 : Training Loss=0.31448733765631914, Training Accuracy=0.8743749856948853\n","...Batch #2500 : Training Loss=0.36269719652831556, Training Accuracy=0.8518750071525574\n","...Batch #2600 : Training Loss=0.3293937583640218, Training Accuracy=0.8568750023841858\n","...Batch #2700 : Training Loss=0.3459518674388528, Training Accuracy=0.856249988079071\n","...Batch #2800 : Training Loss=0.32516740173101427, Training Accuracy=0.8687499761581421\n","...Batch #2900 : Training Loss=0.3403423422947526, Training Accuracy=0.8606249690055847\n","...Batch #3000 : Training Loss=0.30430833108723165, Training Accuracy=0.8681249618530273\n","...Batch #3100 : Training Loss=0.32633353244513275, Training Accuracy=0.8668749928474426\n","...Batch #3200 : Training Loss=0.3408056198805571, Training Accuracy=0.8506249785423279\n","...Batch #3300 : Training Loss=0.3225584051758051, Training Accuracy=0.8675000071525574\n","...Batch #3400 : Training Loss=0.32548066474497317, Training Accuracy=0.8637499809265137\n","...Batch #3500 : Training Loss=0.32411239385604856, Training Accuracy=0.8606249690055847\n","...Batch #3600 : Training Loss=0.33606305837631223, Training Accuracy=0.8531249761581421\n","...Batch #3700 : Training Loss=0.29724438540637493, Training Accuracy=0.8812499642372131\n","...Batch #3800 : Training Loss=0.33031895697116853, Training Accuracy=0.8612499833106995\n","...Batch #3900 : Training Loss=0.34591485366225244, Training Accuracy=0.8431249856948853\n","...Batch #4000 : Training Loss=0.3409677255153656, Training Accuracy=0.8650000095367432\n","...Batch #4100 : Training Loss=0.3631664152443409, Training Accuracy=0.8474999666213989\n","...Batch #4200 : Training Loss=0.3184467189759016, Training Accuracy=0.8668749928474426\n","...Batch #4300 : Training Loss=0.3299306918680668, Training Accuracy=0.8706249594688416\n","...Batch #4400 : Training Loss=0.29109913282096384, Training Accuracy=0.8831250071525574\n","...Batch #4500 : Training Loss=0.32128037337213755, Training Accuracy=0.875\n","...Batch #4600 : Training Loss=0.35743835486471653, Training Accuracy=0.8462499976158142\n","...Batch #4700 : Training Loss=0.33116732552647593, Training Accuracy=0.8493750095367432\n","...Batch #4800 : Training Loss=0.3647913855686784, Training Accuracy=0.8399999737739563\n","...Batch #4900 : Training Loss=0.34750576205551625, Training Accuracy=0.8537499904632568\n","...Batch #5000 : Training Loss=0.31451423175632953, Training Accuracy=0.8656249642372131\n","...Batch #5100 : Training Loss=0.34820441998541357, Training Accuracy=0.8662499785423279\n","...Batch #5200 : Training Loss=0.336474871262908, Training Accuracy=0.8681249618530273\n","...Batch #5300 : Training Loss=0.3169079333543777, Training Accuracy=0.8675000071525574\n","...Batch #5400 : Training Loss=0.2962504278868437, Training Accuracy=0.8856250047683716\n","...Batch #5500 : Training Loss=0.3283636173605919, Training Accuracy=0.8681249618530273\n","...Batch #5600 : Training Loss=0.3375479042902589, Training Accuracy=0.8643749952316284\n","...Batch #5700 : Training Loss=0.32878008857369423, Training Accuracy=0.8643749952316284\n","...Batch #5800 : Training Loss=0.35358660720288754, Training Accuracy=0.8487499952316284\n","...Batch #5900 : Training Loss=0.3359737441316247, Training Accuracy=0.8549999594688416\n","...Batch #6000 : Training Loss=0.330220608599484, Training Accuracy=0.8656249642372131\n","...Batch #6100 : Training Loss=0.33163517989218233, Training Accuracy=0.856249988079071\n","...Batch #6200 : Training Loss=0.344121700078249, Training Accuracy=0.8637499809265137\n","...Batch #6300 : Training Loss=0.3348005386069417, Training Accuracy=0.8499999642372131\n","...Batch #6400 : Training Loss=0.3392945042997599, Training Accuracy=0.8518750071525574\n","...Batch #6500 : Training Loss=0.3301373355090618, Training Accuracy=0.8656249642372131\n","...Batch #6600 : Training Loss=0.32994603529572486, Training Accuracy=0.8612499833106995\n","...Batch #6700 : Training Loss=0.3304506583511829, Training Accuracy=0.8675000071525574\n","...Batch #6800 : Training Loss=0.344094788171351, Training Accuracy=0.8587499856948853\n","...Batch #6900 : Training Loss=0.3187732101231813, Training Accuracy=0.8606249690055847\n","...Batch #7000 : Training Loss=0.3105365676060319, Training Accuracy=0.8774999976158142\n","...Batch #7100 : Training Loss=0.33616061754524706, Training Accuracy=0.8662499785423279\n","...Batch #7200 : Training Loss=0.33232779156416653, Training Accuracy=0.8537499904632568\n","...Batch #7300 : Training Loss=0.3388769107684493, Training Accuracy=0.8556249737739563\n","...Batch #7400 : Training Loss=0.3199412719905376, Training Accuracy=0.8643749952316284\n","...Batch #7500 : Training Loss=0.3396496343612671, Training Accuracy=0.8543750047683716\n","...Batch #7600 : Training Loss=0.3493263743072748, Training Accuracy=0.856249988079071\n","...Batch #7700 : Training Loss=0.3521702363342047, Training Accuracy=0.8493750095367432\n","...Batch #7800 : Training Loss=0.3445386640354991, Training Accuracy=0.8618749976158142\n","...Batch #7900 : Training Loss=0.35118549585342407, Training Accuracy=0.8499999642372131\n","...Batch #8000 : Training Loss=0.3268257042020559, Training Accuracy=0.8656249642372131\n","...Batch #8100 : Training Loss=0.3399619226902723, Training Accuracy=0.8606249690055847\n","...Batch #8200 : Training Loss=0.3319367950037122, Training Accuracy=0.8618749976158142\n","...Batch #8300 : Training Loss=0.33714347004890444, Training Accuracy=0.8612499833106995\n","...Batch #8400 : Training Loss=0.31244476556777956, Training Accuracy=0.8668749928474426\n","...Batch #8500 : Training Loss=0.32627553679049015, Training Accuracy=0.8650000095367432\n","...Batch #8600 : Training Loss=0.3202680103480816, Training Accuracy=0.87562495470047\n","...Batch #8700 : Training Loss=0.32072978254407647, Training Accuracy=0.8637499809265137\n","...Batch #8800 : Training Loss=0.33663926616311074, Training Accuracy=0.8506249785423279\n","...Batch #8900 : Training Loss=0.325009603574872, Training Accuracy=0.8812499642372131\n","...Batch #9000 : Training Loss=0.3540370454639196, Training Accuracy=0.8524999618530273\n","...Batch #9100 : Training Loss=0.33133973509073256, Training Accuracy=0.8700000047683716\n","...Batch #9200 : Training Loss=0.3486941702663898, Training Accuracy=0.8543750047683716\n","...Batch #9300 : Training Loss=0.3214592570066452, Training Accuracy=0.8687499761581421\n","...Batch #9400 : Training Loss=0.2981491516530514, Training Accuracy=0.8743749856948853\n","...Batch #9500 : Training Loss=0.33457686584442853, Training Accuracy=0.8568750023841858\n","...Batch #9600 : Training Loss=0.29937046734616163, Training Accuracy=0.8768749833106995\n","...Batch #9700 : Training Loss=0.32314450956881047, Training Accuracy=0.8606249690055847\n","...Batch #9800 : Training Loss=0.36695643812417983, Training Accuracy=0.8449999690055847\n","...Batch #9900 : Training Loss=0.3331263006478548, Training Accuracy=0.8650000095367432\n","...Batch #10000 : Training Loss=0.33856629878282546, Training Accuracy=0.8581249713897705\n","...Batch #10100 : Training Loss=0.3488988271355629, Training Accuracy=0.8568750023841858\n","...Batch #10200 : Training Loss=0.33096876233816147, Training Accuracy=0.8518750071525574\n","...Batch #10300 : Training Loss=0.34231579713523386, Training Accuracy=0.8556249737739563\n","...Batch #10400 : Training Loss=0.32696440305560825, Training Accuracy=0.8568750023841858\n","...Batch #10500 : Training Loss=0.3005953311547637, Training Accuracy=0.8799999952316284\n","...Batch #10600 : Training Loss=0.36886253133416175, Training Accuracy=0.8418749570846558\n","...Batch #10700 : Training Loss=0.33360583178699016, Training Accuracy=0.8581249713897705\n","...Batch #10800 : Training Loss=0.3319906136393547, Training Accuracy=0.8556249737739563\n","...Batch #10900 : Training Loss=0.29047712743282317, Training Accuracy=0.8706249594688416\n","...Batch #11000 : Training Loss=0.37564530711621047, Training Accuracy=0.84437495470047\n","...Batch #11100 : Training Loss=0.33974162861704826, Training Accuracy=0.8568750023841858\n","...Batch #11200 : Training Loss=0.3210911186784506, Training Accuracy=0.8631249666213989\n","...Batch #11300 : Training Loss=0.36091766133904457, Training Accuracy=0.8512499928474426\n","...Batch #11400 : Training Loss=0.33848564833402633, Training Accuracy=0.8631249666213989\n","...Batch #11500 : Training Loss=0.325406686887145, Training Accuracy=0.8687499761581421\n","...Batch #11600 : Training Loss=0.3345624450594187, Training Accuracy=0.8574999570846558\n","...Batch #11700 : Training Loss=0.3191605614125729, Training Accuracy=0.8668749928474426\n","...Batch #11800 : Training Loss=0.3686177699267864, Training Accuracy=0.8481249809265137\n","...Batch #11900 : Training Loss=0.3364083607494831, Training Accuracy=0.85999995470047\n","...Batch #12000 : Training Loss=0.3409436679631472, Training Accuracy=0.8512499928474426\n","...Batch #12100 : Training Loss=0.3410702598839998, Training Accuracy=0.8637499809265137\n","...Batch #12200 : Training Loss=0.34545898869633673, Training Accuracy=0.8543750047683716\n","...Batch #12300 : Training Loss=0.38055555932223795, Training Accuracy=0.8462499976158142\n","...Batch #12400 : Training Loss=0.3088222298398614, Training Accuracy=0.8706249594688416\n","...Batch #12500 : Training Loss=0.3475474908202887, Training Accuracy=0.8512499928474426\n","...Batch #12600 : Training Loss=0.31019123300909995, Training Accuracy=0.8675000071525574\n","...Batch #12700 : Training Loss=0.3514091337844729, Training Accuracy=0.8493750095367432\n","...Batch #12800 : Training Loss=0.36204719454050066, Training Accuracy=0.8543750047683716\n","...Batch #12900 : Training Loss=0.35005078330636025, Training Accuracy=0.8543750047683716\n","...Batch #13000 : Training Loss=0.33553347043693066, Training Accuracy=0.8637499809265137\n","...Batch #13100 : Training Loss=0.31864640895277263, Training Accuracy=0.8643749952316284\n","...Batch #13200 : Training Loss=0.34325991533696654, Training Accuracy=0.8499999642372131\n","...Batch #13300 : Training Loss=0.35598680201917887, Training Accuracy=0.85999995470047\n","...Batch #13400 : Training Loss=0.3191527758166194, Training Accuracy=0.8793749809265137\n","...Batch #13500 : Training Loss=0.3089889829605818, Training Accuracy=0.871874988079071\n","...Batch #13600 : Training Loss=0.34098786855116486, Training Accuracy=0.8581249713897705\n","...Batch #13700 : Training Loss=0.3281626044213772, Training Accuracy=0.8681249618530273\n","...Batch #13800 : Training Loss=0.3262444357573986, Training Accuracy=0.8725000023841858\n","...Batch #13900 : Training Loss=0.3250625499151647, Training Accuracy=0.8587499856948853\n","...Batch #14000 : Training Loss=0.32762609906494616, Training Accuracy=0.8656249642372131\n","...Batch #14100 : Training Loss=0.3063770279288292, Training Accuracy=0.875\n","...Batch #14200 : Training Loss=0.3095571207255125, Training Accuracy=0.8731249570846558\n","...Batch #14300 : Training Loss=0.33505664609372615, Training Accuracy=0.8643749952316284\n","...Batch #14400 : Training Loss=0.318903239145875, Training Accuracy=0.8637499809265137\n","...Batch #14500 : Training Loss=0.3138536824658513, Training Accuracy=0.8806250095367432\n","...Batch #14600 : Training Loss=0.3214786392450333, Training Accuracy=0.8662499785423279\n","...Batch #14700 : Training Loss=0.3251814186573029, Training Accuracy=0.8618749976158142\n","...Batch #14800 : Training Loss=0.31444779597222805, Training Accuracy=0.8656249642372131\n","...Batch #14900 : Training Loss=0.3235770436748862, Training Accuracy=0.8631249666213989\n","...Batch #15000 : Training Loss=0.3263201756030321, Training Accuracy=0.8543750047683716\n","...Batch #15100 : Training Loss=0.33653465457260606, Training Accuracy=0.8587499856948853\n","...Batch #15200 : Training Loss=0.32529568411409854, Training Accuracy=0.8631249666213989\n","...Batch #15300 : Training Loss=0.359196602627635, Training Accuracy=0.8524999618530273\n","...Batch #15400 : Training Loss=0.34323582373559475, Training Accuracy=0.8581249713897705\n","...Batch #15500 : Training Loss=0.3314778239279985, Training Accuracy=0.8643749952316284\n","...Batch #15600 : Training Loss=0.31761900033801793, Training Accuracy=0.8587499856948853\n","...Batch #15700 : Training Loss=0.32515427574515343, Training Accuracy=0.8650000095367432\n","...Batch #15800 : Training Loss=0.3523350210487843, Training Accuracy=0.8412500023841858\n","...Batch #15900 : Training Loss=0.3289064499735832, Training Accuracy=0.8624999523162842\n","...Batch #16000 : Training Loss=0.29976798422634604, Training Accuracy=0.8824999928474426\n","...Batch #16100 : Training Loss=0.3629329364001751, Training Accuracy=0.840624988079071\n","...Batch #16200 : Training Loss=0.33406390830874444, Training Accuracy=0.8662499785423279\n","...Batch #16300 : Training Loss=0.362460572719574, Training Accuracy=0.8574999570846558\n","...Batch #16400 : Training Loss=0.34174642980098724, Training Accuracy=0.8543750047683716\n","...Batch #16500 : Training Loss=0.3172613549977541, Training Accuracy=0.8731249570846558\n","...Batch #16600 : Training Loss=0.32912422496825455, Training Accuracy=0.8662499785423279\n","...Batch #16700 : Training Loss=0.3260547690093517, Training Accuracy=0.8631249666213989\n","...Batch #16800 : Training Loss=0.3733605358377099, Training Accuracy=0.8456249833106995\n","...Batch #16900 : Training Loss=0.3120736987516284, Training Accuracy=0.8618749976158142\n","...Batch #17000 : Training Loss=0.34525241881608965, Training Accuracy=0.8631249666213989\n","...Batch #17100 : Training Loss=0.3310501243546605, Training Accuracy=0.8612499833106995\n","...Batch #17200 : Training Loss=0.3457206827402115, Training Accuracy=0.8574999570846558\n","...Batch #17300 : Training Loss=0.32335560116916895, Training Accuracy=0.8624999523162842\n","...Batch #17400 : Training Loss=0.3337368947267532, Training Accuracy=0.8668749928474426\n","...Batch #17500 : Training Loss=0.3074761152639985, Training Accuracy=0.8712499737739563\n","...Batch #17600 : Training Loss=0.3363790501654148, Training Accuracy=0.85999995470047\n","...Batch #17700 : Training Loss=0.32312783937901257, Training Accuracy=0.871874988079071\n","...Batch #17800 : Training Loss=0.3107159289345145, Training Accuracy=0.8687499761581421\n","...Batch #17900 : Training Loss=0.34649078171700237, Training Accuracy=0.859375\n","...Batch #18000 : Training Loss=0.32762660447508096, Training Accuracy=0.8568750023841858\n","...Batch #18100 : Training Loss=0.3493161937594414, Training Accuracy=0.8424999713897705\n","...Batch #18200 : Training Loss=0.33605129301548003, Training Accuracy=0.8531249761581421\n","...Batch #18300 : Training Loss=0.3165440222993493, Training Accuracy=0.8618749976158142\n","...Batch #18400 : Training Loss=0.3379131107404828, Training Accuracy=0.8700000047683716\n","...Batch #18500 : Training Loss=0.3185612753778696, Training Accuracy=0.8675000071525574\n","...Batch #18600 : Training Loss=0.3302276562899351, Training Accuracy=0.8693749904632568\n","...Batch #18700 : Training Loss=0.3109246098250151, Training Accuracy=0.8693749904632568\n","...Batch #18800 : Training Loss=0.30018856648355724, Training Accuracy=0.8725000023841858\n","...Batch #18900 : Training Loss=0.3602193845435977, Training Accuracy=0.8393749594688416\n","...Batch #19000 : Training Loss=0.32791995964944365, Training Accuracy=0.8693749904632568\n","...Batch #19100 : Training Loss=0.35637397699058054, Training Accuracy=0.8493750095367432\n","...Batch #19200 : Training Loss=0.326950068064034, Training Accuracy=0.8700000047683716\n","...Batch #19300 : Training Loss=0.3277130465954542, Training Accuracy=0.85999995470047\n","...Batch #19400 : Training Loss=0.35934058506041766, Training Accuracy=0.8543750047683716\n","...Batch #19500 : Training Loss=0.3277369625121355, Training Accuracy=0.8574999570846558\n","...Batch #19600 : Training Loss=0.3113352185487747, Training Accuracy=0.8637499809265137\n","...Batch #19700 : Training Loss=0.3323106171190739, Training Accuracy=0.859375\n","...Batch #19800 : Training Loss=0.34466633141040803, Training Accuracy=0.8662499785423279\n","...Batch #19900 : Training Loss=0.3387448533624411, Training Accuracy=0.8487499952316284\n","...Batch #20000 : Training Loss=0.3531174433603883, Training Accuracy=0.8456249833106995\n","...Batch #20100 : Training Loss=0.3057237725704908, Training Accuracy=0.8743749856948853\n","...Batch #20200 : Training Loss=0.3176998391747475, Training Accuracy=0.8743749856948853\n","...Batch #20300 : Training Loss=0.32437333673238755, Training Accuracy=0.8668749928474426\n","...Batch #20400 : Training Loss=0.33840513780713083, Training Accuracy=0.8587499856948853\n","...Batch #20500 : Training Loss=0.33501891292631625, Training Accuracy=0.8668749928474426\n","...Batch #20600 : Training Loss=0.35923608459532264, Training Accuracy=0.8468749523162842\n","...Batch #20700 : Training Loss=0.3161567881330848, Training Accuracy=0.8631249666213989\n","...Batch #20800 : Training Loss=0.3228977572917938, Training Accuracy=0.8606249690055847\n","...Batch #20900 : Training Loss=0.34112063847482205, Training Accuracy=0.8693749904632568\n","...Batch #21000 : Training Loss=0.33192005347460507, Training Accuracy=0.8556249737739563\n","...Batch #21100 : Training Loss=0.31892642702907326, Training Accuracy=0.8731249570846558\n","...Batch #21200 : Training Loss=0.30915358029305934, Training Accuracy=0.8706249594688416\n","...Batch #21300 : Training Loss=0.2984204810857773, Training Accuracy=0.8818749785423279\n","...Batch #21400 : Training Loss=0.3074186469241977, Training Accuracy=0.8837499618530273\n","...Batch #21500 : Training Loss=0.3361978446692228, Training Accuracy=0.8581249713897705\n","...Batch #21600 : Training Loss=0.3281899520754814, Training Accuracy=0.8606249690055847\n","...Batch #21700 : Training Loss=0.32559227146208286, Training Accuracy=0.8662499785423279\n","...Batch #21800 : Training Loss=0.28269117772579194, Training Accuracy=0.8974999785423279\n","...Batch #21900 : Training Loss=0.330847057364881, Training Accuracy=0.8643749952316284\n","...Batch #22000 : Training Loss=0.34972362592816353, Training Accuracy=0.8537499904632568\n","...Batch #22100 : Training Loss=0.3341182554513216, Training Accuracy=0.8518750071525574\n","...Batch #22200 : Training Loss=0.3314850014820695, Training Accuracy=0.8531249761581421\n","...Batch #22300 : Training Loss=0.3280671104043722, Training Accuracy=0.8681249618530273\n","...Batch #22400 : Training Loss=0.3185324832051992, Training Accuracy=0.8668749928474426\n","...Batch #22500 : Training Loss=0.36202451281249526, Training Accuracy=0.8481249809265137\n","...Batch #22600 : Training Loss=0.3320817448198795, Training Accuracy=0.8574999570846558\n","...Batch #22700 : Training Loss=0.34457090206444263, Training Accuracy=0.8487499952316284\n","...Batch #22800 : Training Loss=0.31024491533637044, Training Accuracy=0.8687499761581421\n","...Batch #22900 : Training Loss=0.3093028473481536, Training Accuracy=0.8662499785423279\n","...Batch #23000 : Training Loss=0.3166565354540944, Training Accuracy=0.8650000095367432\n","...Batch #23100 : Training Loss=0.32641358476132154, Training Accuracy=0.8675000071525574\n","...Batch #23200 : Training Loss=0.33359481997787954, Training Accuracy=0.8737499713897705\n","...Batch #23300 : Training Loss=0.31798227719962596, Training Accuracy=0.8681249618530273\n","...Batch #23400 : Training Loss=0.32776824206113814, Training Accuracy=0.8706249594688416\n","...Batch #23500 : Training Loss=0.3175695539265871, Training Accuracy=0.85999995470047\n","...Batch #23600 : Training Loss=0.3450312785245478, Training Accuracy=0.8574999570846558\n","...Batch #23700 : Training Loss=0.3191077867522836, Training Accuracy=0.8693749904632568\n","...Batch #23800 : Training Loss=0.322775555383414, Training Accuracy=0.8706249594688416\n","...Batch #23900 : Training Loss=0.3453090316057205, Training Accuracy=0.8781249523162842\n","...Batch #24000 : Training Loss=0.2986877013370395, Training Accuracy=0.8762499690055847\n","...Batch #24100 : Training Loss=0.3071408349275589, Training Accuracy=0.8793749809265137\n","...Batch #24200 : Training Loss=0.31501949805766344, Training Accuracy=0.8706249594688416\n","...Batch #24300 : Training Loss=0.34905464340001346, Training Accuracy=0.8537499904632568\n","...Batch #24400 : Training Loss=0.3150863283127546, Training Accuracy=0.8693749904632568\n","...Batch #24500 : Training Loss=0.36434926390647887, Training Accuracy=0.8537499904632568\n","...Batch #24600 : Training Loss=0.3478916870057583, Training Accuracy=0.8531249761581421\n","...Batch #24700 : Training Loss=0.35895725518465044, Training Accuracy=0.8531249761581421\n","...Batch #24800 : Training Loss=0.3162470363080502, Training Accuracy=0.8662499785423279\n","...Batch #24900 : Training Loss=0.30876366414129736, Training Accuracy=0.8656249642372131\n","...Batch #25000 : Training Loss=0.324521479010582, Training Accuracy=0.8637499809265137\n","...Batch #25100 : Training Loss=0.3339209711551666, Training Accuracy=0.8668749928474426\n","...Batch #25200 : Training Loss=0.3387361904233694, Training Accuracy=0.8518750071525574\n","...Batch #25300 : Training Loss=0.31419502295553686, Training Accuracy=0.8712499737739563\n","...Batch #25400 : Training Loss=0.3131837577372789, Training Accuracy=0.8781249523162842\n","...Batch #25500 : Training Loss=0.32211372815072536, Training Accuracy=0.8668749928474426\n","...Batch #25600 : Training Loss=0.35708199597895146, Training Accuracy=0.8487499952316284\n","...Batch #25700 : Training Loss=0.35967364825308323, Training Accuracy=0.8481249809265137\n","...Batch #25800 : Training Loss=0.3252687195688486, Training Accuracy=0.8668749928474426\n","...Batch #25900 : Training Loss=0.3343538335710764, Training Accuracy=0.8587499856948853\n","...Batch #26000 : Training Loss=0.3215108584612608, Training Accuracy=0.8725000023841858\n","...Batch #26100 : Training Loss=0.34834497079253196, Training Accuracy=0.8637499809265137\n","...Batch #26200 : Training Loss=0.34061931185424327, Training Accuracy=0.8612499833106995\n","...Batch #26300 : Training Loss=0.3023998797684908, Training Accuracy=0.8799999952316284\n","...Batch #26400 : Training Loss=0.3041630020737648, Training Accuracy=0.8762499690055847\n","...Batch #26500 : Training Loss=0.3233523742109537, Training Accuracy=0.8650000095367432\n","...Batch #26600 : Training Loss=0.33620010189712046, Training Accuracy=0.8568750023841858\n","...Batch #26700 : Training Loss=0.3042846842110157, Training Accuracy=0.87562495470047\n","...Batch #26800 : Training Loss=0.31608954060822725, Training Accuracy=0.8737499713897705\n","...Batch #26900 : Training Loss=0.3181023483723402, Training Accuracy=0.8743749856948853\n","...Batch #27000 : Training Loss=0.3242558704316616, Training Accuracy=0.8706249594688416\n","...Batch #27100 : Training Loss=0.32111931189894677, Training Accuracy=0.8725000023841858\n","...Batch #27200 : Training Loss=0.29232564605772493, Training Accuracy=0.8843749761581421\n","...Batch #27300 : Training Loss=0.31353411588817837, Training Accuracy=0.8743749856948853\n","...Batch #27400 : Training Loss=0.3422277491539717, Training Accuracy=0.8612499833106995\n","...Batch #27500 : Training Loss=0.34776409681886433, Training Accuracy=0.8731249570846558\n","...Batch #27600 : Training Loss=0.34165629152208565, Training Accuracy=0.859375\n","...Batch #27700 : Training Loss=0.3680539334192872, Training Accuracy=0.856249988079071\n","...Batch #27800 : Training Loss=0.33529327511787416, Training Accuracy=0.85999995470047\n","...Batch #27900 : Training Loss=0.3227369935438037, Training Accuracy=0.8643749952316284\n","...Batch #28000 : Training Loss=0.33841956868767736, Training Accuracy=0.8612499833106995\n","...Batch #28100 : Training Loss=0.3178909410536289, Training Accuracy=0.8624999523162842\n","...Batch #28200 : Training Loss=0.31217528704553843, Training Accuracy=0.8700000047683716\n","...Batch #28300 : Training Loss=0.3213780242949724, Training Accuracy=0.859375\n","...Batch #28400 : Training Loss=0.3509117469936609, Training Accuracy=0.8524999618530273\n","...Batch #28500 : Training Loss=0.3274003125727177, Training Accuracy=0.875\n","...Batch #28600 : Training Loss=0.307302389331162, Training Accuracy=0.8706249594688416\n","...Batch #28700 : Training Loss=0.364050153195858, Training Accuracy=0.8381249904632568\n","...Batch #28800 : Training Loss=0.3438327553868294, Training Accuracy=0.8543750047683716\n","...Batch #28900 : Training Loss=0.30590498208999634, Training Accuracy=0.8793749809265137\n","...Batch #29000 : Training Loss=0.322642757371068, Training Accuracy=0.8731249570846558\n","...Batch #29100 : Training Loss=0.3212390962988138, Training Accuracy=0.8675000071525574\n","...Batch #29200 : Training Loss=0.3195250276476145, Training Accuracy=0.8643749952316284\n","...Batch #29300 : Training Loss=0.3093250825256109, Training Accuracy=0.875\n","...Batch #29400 : Training Loss=0.33298502407968045, Training Accuracy=0.8787499666213989\n","...Batch #29500 : Training Loss=0.3134344731271267, Training Accuracy=0.8731249570846558\n","...Batch #29600 : Training Loss=0.34029691230505704, Training Accuracy=0.8631249666213989\n","...Batch #29700 : Training Loss=0.31602456752210856, Training Accuracy=0.8662499785423279\n","...Batch #29800 : Training Loss=0.3438817232847214, Training Accuracy=0.8656249642372131\n","...Batch #29900 : Training Loss=0.34513432037085295, Training Accuracy=0.8624999523162842\n","...Batch #30000 : Training Loss=0.3433094120770693, Training Accuracy=0.8656249642372131\n","...Batch #30100 : Training Loss=0.30760895133018495, Training Accuracy=0.8706249594688416\n","...Batch #30200 : Training Loss=0.3089139627665281, Training Accuracy=0.8831250071525574\n","...Batch #30300 : Training Loss=0.28797879692167044, Training Accuracy=0.8856250047683716\n","...Batch #30400 : Training Loss=0.3197774911671877, Training Accuracy=0.8787499666213989\n","...Batch #30500 : Training Loss=0.3300551855564117, Training Accuracy=0.8618749976158142\n","...Batch #30600 : Training Loss=0.32573125265538694, Training Accuracy=0.8700000047683716\n","...Batch #30700 : Training Loss=0.32945240624248984, Training Accuracy=0.8725000023841858\n","...Batch #30800 : Training Loss=0.3103846460022032, Training Accuracy=0.8681249618530273\n","...Batch #30900 : Training Loss=0.3312392231822014, Training Accuracy=0.8606249690055847\n","...Batch #31000 : Training Loss=0.29930647913366554, Training Accuracy=0.875\n","...Batch #31100 : Training Loss=0.33031246419996024, Training Accuracy=0.8687499761581421\n","...Batch #31200 : Training Loss=0.3251194325834513, Training Accuracy=0.8624999523162842\n","...Batch #31300 : Training Loss=0.3222623658925295, Training Accuracy=0.8662499785423279\n","...Batch #31400 : Training Loss=0.3230174342542887, Training Accuracy=0.8687499761581421\n","...Batch #31500 : Training Loss=0.3363006108626723, Training Accuracy=0.8693749904632568\n","...Batch #31600 : Training Loss=0.34486160360276696, Training Accuracy=0.8637499809265137\n","...Batch #31700 : Training Loss=0.31218050740659237, Training Accuracy=0.8687499761581421\n","...Batch #31800 : Training Loss=0.34083044953644276, Training Accuracy=0.8481249809265137\n","...Batch #31900 : Training Loss=0.3115748998522758, Training Accuracy=0.875\n","...Batch #32000 : Training Loss=0.31319709673523904, Training Accuracy=0.8725000023841858\n","...Batch #32100 : Training Loss=0.323637687638402, Training Accuracy=0.8662499785423279\n","...Batch #32200 : Training Loss=0.33630490563809873, Training Accuracy=0.8643749952316284\n","...Batch #32300 : Training Loss=0.33775385782122613, Training Accuracy=0.8537499904632568\n","...Batch #32400 : Training Loss=0.3360694873332977, Training Accuracy=0.8618749976158142\n","...Batch #32500 : Training Loss=0.2822263064980507, Training Accuracy=0.8700000047683716\n","...Batch #32600 : Training Loss=0.32702374290674924, Training Accuracy=0.8700000047683716\n","...Batch #32700 : Training Loss=0.3178557828813791, Training Accuracy=0.8637499809265137\n","...Batch #32800 : Training Loss=0.3387768369540572, Training Accuracy=0.8568750023841858\n","...Batch #32900 : Training Loss=0.3315667602047324, Training Accuracy=0.8624999523162842\n","...Batch #33000 : Training Loss=0.3158471405506134, Training Accuracy=0.8650000095367432\n","...Batch #33100 : Training Loss=0.3530830144882202, Training Accuracy=0.8606249690055847\n","...Batch #33200 : Training Loss=0.3111426951736212, Training Accuracy=0.8725000023841858\n","...Batch #33300 : Training Loss=0.3160030905157328, Training Accuracy=0.8681249618530273\n","...Batch #33400 : Training Loss=0.3427930883318186, Training Accuracy=0.8612499833106995\n","...Batch #33500 : Training Loss=0.3315093532204628, Training Accuracy=0.8631249666213989\n","...Batch #33600 : Training Loss=0.30789151050150393, Training Accuracy=0.8731249570846558\n","...Batch #33700 : Training Loss=0.3015530318021774, Training Accuracy=0.8662499785423279\n","...Batch #33800 : Training Loss=0.3024210836738348, Training Accuracy=0.8693749904632568\n","...Batch #33900 : Training Loss=0.332614611685276, Training Accuracy=0.8643749952316284\n","...Batch #34000 : Training Loss=0.3089178014546633, Training Accuracy=0.8731249570846558\n","...Batch #34100 : Training Loss=0.3443913599103689, Training Accuracy=0.859375\n","...Batch #34200 : Training Loss=0.297410047352314, Training Accuracy=0.8787499666213989\n","...Batch #34300 : Training Loss=0.3283619804680347, Training Accuracy=0.8725000023841858\n","...Batch #34400 : Training Loss=0.3306499457731843, Training Accuracy=0.8543750047683716\n","...Batch #34500 : Training Loss=0.33420349564403296, Training Accuracy=0.8725000023841858\n","...Batch #34600 : Training Loss=0.3344932470843196, Training Accuracy=0.8624999523162842\n","...Batch #34700 : Training Loss=0.3113939866423607, Training Accuracy=0.8700000047683716\n","...Batch #34800 : Training Loss=0.3103747526742518, Training Accuracy=0.8675000071525574\n","...Batch #34900 : Training Loss=0.3273712529242039, Training Accuracy=0.8631249666213989\n","...Batch #35000 : Training Loss=0.3225224727392197, Training Accuracy=0.8693749904632568\n","...Batch #35100 : Training Loss=0.3441165743395686, Training Accuracy=0.8506249785423279\n","...Batch #35200 : Training Loss=0.34552532248198986, Training Accuracy=0.8543750047683716\n","...Batch #35300 : Training Loss=0.32642022635787726, Training Accuracy=0.8768749833106995\n","...Batch #35400 : Training Loss=0.3183459015190601, Training Accuracy=0.8675000071525574\n","...Batch #35500 : Training Loss=0.32253468185663225, Training Accuracy=0.8612499833106995\n","...Batch #35600 : Training Loss=0.32579852394759656, Training Accuracy=0.8574999570846558\n","...Batch #35700 : Training Loss=0.3131854609400034, Training Accuracy=0.8725000023841858\n","...Batch #35800 : Training Loss=0.3275308867171407, Training Accuracy=0.8693749904632568\n","...Batch #35900 : Training Loss=0.3162705870717764, Training Accuracy=0.8624999523162842\n","...Batch #36000 : Training Loss=0.2971621200442314, Training Accuracy=0.8824999928474426\n","...Batch #36100 : Training Loss=0.3213964675366878, Training Accuracy=0.8700000047683716\n","...Batch #36200 : Training Loss=0.33185213565826416, Training Accuracy=0.8612499833106995\n","...Batch #36300 : Training Loss=0.29760274540632964, Training Accuracy=0.8793749809265137\n","Train: loss 0.32931905030805825, accuracy 0.863746118759102\n","Valid: loss 0.35401538030371893, accuracy 0.8585423197492164, f1_ass 0.7993329627570872, f1_sweet 0.8907715582450833\n","####################################################################################################\n","Epoch 7 / 7\n","...Batch #100 : Training Loss=0.3363429207354784, Training Accuracy=0.8449999690055847\n","...Batch #200 : Training Loss=0.33704176984727385, Training Accuracy=0.8624999523162842\n","...Batch #300 : Training Loss=0.30750960744917394, Training Accuracy=0.8725000023841858\n","...Batch #400 : Training Loss=0.31886131316423416, Training Accuracy=0.8737499713897705\n","...Batch #500 : Training Loss=0.3264626605063677, Training Accuracy=0.8675000071525574\n","...Batch #600 : Training Loss=0.3085238715261221, Training Accuracy=0.8774999976158142\n","...Batch #700 : Training Loss=0.3283650892227888, Training Accuracy=0.8618749976158142\n","...Batch #800 : Training Loss=0.31357214581221343, Training Accuracy=0.8737499713897705\n","...Batch #900 : Training Loss=0.3377143090963364, Training Accuracy=0.8531249761581421\n","...Batch #1000 : Training Loss=0.33230058923363687, Training Accuracy=0.8537499904632568\n","...Batch #1100 : Training Loss=0.30237452261149883, Training Accuracy=0.8806250095367432\n","...Batch #1200 : Training Loss=0.3219559698365629, Training Accuracy=0.8675000071525574\n","...Batch #1300 : Training Loss=0.308016737755388, Training Accuracy=0.8737499713897705\n","...Batch #1400 : Training Loss=0.34403418354690074, Training Accuracy=0.8543750047683716\n","...Batch #1500 : Training Loss=0.2953784615546465, Training Accuracy=0.8768749833106995\n","...Batch #1600 : Training Loss=0.3116835176944733, Training Accuracy=0.8793749809265137\n","...Batch #1700 : Training Loss=0.32018494265154, Training Accuracy=0.8687499761581421\n","...Batch #1800 : Training Loss=0.3140998170524836, Training Accuracy=0.8712499737739563\n","...Batch #1900 : Training Loss=0.3176450561732054, Training Accuracy=0.8706249594688416\n","...Batch #2000 : Training Loss=0.3106221111118794, Training Accuracy=0.8768749833106995\n","...Batch #2100 : Training Loss=0.3065512106567621, Training Accuracy=0.8687499761581421\n","...Batch #2200 : Training Loss=0.3137639869377017, Training Accuracy=0.8637499809265137\n","...Batch #2300 : Training Loss=0.3316609349474311, Training Accuracy=0.8650000095367432\n","...Batch #2400 : Training Loss=0.3019120007753372, Training Accuracy=0.875\n","...Batch #2500 : Training Loss=0.3423223502933979, Training Accuracy=0.8662499785423279\n","...Batch #2600 : Training Loss=0.30384217523038387, Training Accuracy=0.8693749904632568\n","...Batch #2700 : Training Loss=0.34995385516434907, Training Accuracy=0.8606249690055847\n","...Batch #2800 : Training Loss=0.3097085876762867, Training Accuracy=0.8787499666213989\n","...Batch #2900 : Training Loss=0.3514928888902068, Training Accuracy=0.8493750095367432\n","...Batch #3000 : Training Loss=0.2965210514515638, Training Accuracy=0.8768749833106995\n","...Batch #3100 : Training Loss=0.2982768056169152, Training Accuracy=0.8687499761581421\n","...Batch #3200 : Training Loss=0.33117292311042545, Training Accuracy=0.8556249737739563\n","...Batch #3300 : Training Loss=0.30038743738085033, Training Accuracy=0.8725000023841858\n","...Batch #3400 : Training Loss=0.3071622744202614, Training Accuracy=0.875\n","...Batch #3500 : Training Loss=0.3228726477921009, Training Accuracy=0.8650000095367432\n","...Batch #3600 : Training Loss=0.31646193489432334, Training Accuracy=0.8712499737739563\n","...Batch #3700 : Training Loss=0.29630566217005255, Training Accuracy=0.8881250023841858\n","...Batch #3800 : Training Loss=0.32148779256269333, Training Accuracy=0.8662499785423279\n","...Batch #3900 : Training Loss=0.32630784893408415, Training Accuracy=0.8612499833106995\n","...Batch #4000 : Training Loss=0.335801594145596, Training Accuracy=0.8725000023841858\n","...Batch #4100 : Training Loss=0.3400275214016438, Training Accuracy=0.8668749928474426\n","...Batch #4200 : Training Loss=0.3073283340781927, Training Accuracy=0.87562495470047\n","...Batch #4300 : Training Loss=0.3063174032047391, Training Accuracy=0.8831250071525574\n","...Batch #4400 : Training Loss=0.2855431364476681, Training Accuracy=0.8868749737739563\n","...Batch #4500 : Training Loss=0.2966415512561798, Training Accuracy=0.8781249523162842\n","...Batch #4600 : Training Loss=0.3600403591990471, Training Accuracy=0.8493750095367432\n","...Batch #4700 : Training Loss=0.315535064227879, Training Accuracy=0.8581249713897705\n","...Batch #4800 : Training Loss=0.35584188766777514, Training Accuracy=0.856249988079071\n","...Batch #4900 : Training Loss=0.33133575432002543, Training Accuracy=0.8612499833106995\n","...Batch #5000 : Training Loss=0.30699109852313994, Training Accuracy=0.8618749976158142\n","...Batch #5100 : Training Loss=0.3288549347966909, Training Accuracy=0.8668749928474426\n","...Batch #5200 : Training Loss=0.31340400595217943, Training Accuracy=0.8818749785423279\n","...Batch #5300 : Training Loss=0.3270956552773714, Training Accuracy=0.8656249642372131\n","...Batch #5400 : Training Loss=0.2853766992688179, Training Accuracy=0.8849999904632568\n","...Batch #5500 : Training Loss=0.3227046997472644, Training Accuracy=0.8656249642372131\n","...Batch #5600 : Training Loss=0.32191134367138147, Training Accuracy=0.8693749904632568\n","...Batch #5700 : Training Loss=0.30665913481265306, Training Accuracy=0.8668749928474426\n","...Batch #5800 : Training Loss=0.3374955769442022, Training Accuracy=0.8574999570846558\n","...Batch #5900 : Training Loss=0.3378215953707695, Training Accuracy=0.8531249761581421\n","...Batch #6000 : Training Loss=0.32966377936303615, Training Accuracy=0.8693749904632568\n","...Batch #6100 : Training Loss=0.33935760289430617, Training Accuracy=0.8687499761581421\n","...Batch #6200 : Training Loss=0.32852671027183533, Training Accuracy=0.8656249642372131\n","...Batch #6300 : Training Loss=0.3156815594434738, Training Accuracy=0.8662499785423279\n","...Batch #6400 : Training Loss=0.3233147457242012, Training Accuracy=0.85999995470047\n","...Batch #6500 : Training Loss=0.315564367249608, Training Accuracy=0.8737499713897705\n","...Batch #6600 : Training Loss=0.30835994239896536, Training Accuracy=0.8799999952316284\n","...Batch #6700 : Training Loss=0.3223879295215011, Training Accuracy=0.8681249618530273\n","...Batch #6800 : Training Loss=0.3207414445281029, Training Accuracy=0.8693749904632568\n","...Batch #6900 : Training Loss=0.29523145396262407, Training Accuracy=0.8725000023841858\n","...Batch #7000 : Training Loss=0.3150897899642587, Training Accuracy=0.8768749833106995\n","...Batch #7100 : Training Loss=0.3051751510426402, Training Accuracy=0.8774999976158142\n","...Batch #7200 : Training Loss=0.30858900628983976, Training Accuracy=0.87562495470047\n","...Batch #7300 : Training Loss=0.33097975943237545, Training Accuracy=0.8650000095367432\n","...Batch #7400 : Training Loss=0.305115872323513, Training Accuracy=0.8700000047683716\n","...Batch #7500 : Training Loss=0.3277412673085928, Training Accuracy=0.8618749976158142\n","...Batch #7600 : Training Loss=0.3264699647575617, Training Accuracy=0.8662499785423279\n","...Batch #7700 : Training Loss=0.3308421029150486, Training Accuracy=0.8493750095367432\n","...Batch #7800 : Training Loss=0.3401445234566927, Training Accuracy=0.8549999594688416\n","...Batch #7900 : Training Loss=0.33538217110559343, Training Accuracy=0.8637499809265137\n","...Batch #8000 : Training Loss=0.31539107762277124, Training Accuracy=0.8656249642372131\n","...Batch #8100 : Training Loss=0.31924797892570494, Training Accuracy=0.8650000095367432\n","...Batch #8200 : Training Loss=0.3265050713345408, Training Accuracy=0.8762499690055847\n","...Batch #8300 : Training Loss=0.3207167027518153, Training Accuracy=0.8700000047683716\n","...Batch #8400 : Training Loss=0.31432688649743795, Training Accuracy=0.8693749904632568\n","...Batch #8500 : Training Loss=0.3143711484968662, Training Accuracy=0.87562495470047\n","...Batch #8600 : Training Loss=0.30122917752712963, Training Accuracy=0.8831250071525574\n","...Batch #8700 : Training Loss=0.31300320744514465, Training Accuracy=0.8687499761581421\n","...Batch #8800 : Training Loss=0.33349829910323026, Training Accuracy=0.8618749976158142\n","...Batch #8900 : Training Loss=0.3135428658500314, Training Accuracy=0.8812499642372131\n","...Batch #9000 : Training Loss=0.3402834375202656, Training Accuracy=0.8549999594688416\n","...Batch #9100 : Training Loss=0.3252769347280264, Training Accuracy=0.8681249618530273\n","...Batch #9200 : Training Loss=0.34022140242159365, Training Accuracy=0.8675000071525574\n","...Batch #9300 : Training Loss=0.3089808415994048, Training Accuracy=0.8712499737739563\n","...Batch #9400 : Training Loss=0.2859536559507251, Training Accuracy=0.8799999952316284\n","...Batch #9500 : Training Loss=0.3266359298489988, Training Accuracy=0.8668749928474426\n","...Batch #9600 : Training Loss=0.2782314127497375, Training Accuracy=0.887499988079071\n","...Batch #9700 : Training Loss=0.32085032768547533, Training Accuracy=0.8668749928474426\n","...Batch #9800 : Training Loss=0.365960760191083, Training Accuracy=0.8487499952316284\n","...Batch #9900 : Training Loss=0.3135411884635687, Training Accuracy=0.8675000071525574\n","...Batch #10000 : Training Loss=0.32872383780777453, Training Accuracy=0.8656249642372131\n","...Batch #10100 : Training Loss=0.3460423563793302, Training Accuracy=0.8612499833106995\n","...Batch #10200 : Training Loss=0.31564802292734384, Training Accuracy=0.8581249713897705\n","...Batch #10300 : Training Loss=0.33313125096261503, Training Accuracy=0.8700000047683716\n","...Batch #10400 : Training Loss=0.30848712772130965, Training Accuracy=0.8687499761581421\n","...Batch #10500 : Training Loss=0.3120498850941658, Training Accuracy=0.8812499642372131\n","...Batch #10600 : Training Loss=0.35388347422704103, Training Accuracy=0.8481249809265137\n","...Batch #10700 : Training Loss=0.31103314595296977, Training Accuracy=0.8675000071525574\n","...Batch #10800 : Training Loss=0.3107622704282403, Training Accuracy=0.8662499785423279\n","...Batch #10900 : Training Loss=0.2916862423717976, Training Accuracy=0.8787499666213989\n","...Batch #11000 : Training Loss=0.35164257595315573, Training Accuracy=0.8499999642372131\n","...Batch #11100 : Training Loss=0.3177289481088519, Training Accuracy=0.8656249642372131\n","...Batch #11200 : Training Loss=0.3082571564614773, Training Accuracy=0.8712499737739563\n","...Batch #11300 : Training Loss=0.3460450535267591, Training Accuracy=0.8549999594688416\n","...Batch #11400 : Training Loss=0.32642753593623636, Training Accuracy=0.8631249666213989\n","...Batch #11500 : Training Loss=0.3127209158241749, Training Accuracy=0.8731249570846558\n","...Batch #11600 : Training Loss=0.311348677482456, Training Accuracy=0.8631249666213989\n","...Batch #11700 : Training Loss=0.30897980365902183, Training Accuracy=0.8824999928474426\n","...Batch #11800 : Training Loss=0.34402770245447756, Training Accuracy=0.8524999618530273\n","...Batch #11900 : Training Loss=0.3300085065513849, Training Accuracy=0.8543750047683716\n","...Batch #12000 : Training Loss=0.3268372060358524, Training Accuracy=0.8650000095367432\n","...Batch #12100 : Training Loss=0.3257492340356112, Training Accuracy=0.8624999523162842\n","...Batch #12200 : Training Loss=0.3300889588147402, Training Accuracy=0.85999995470047\n","...Batch #12300 : Training Loss=0.3670363287255168, Training Accuracy=0.8556249737739563\n","...Batch #12400 : Training Loss=0.30158299587666987, Training Accuracy=0.8687499761581421\n","...Batch #12500 : Training Loss=0.35693021163344385, Training Accuracy=0.8568750023841858\n","...Batch #12600 : Training Loss=0.2846021957695484, Training Accuracy=0.8831250071525574\n","...Batch #12700 : Training Loss=0.33649354545399546, Training Accuracy=0.8624999523162842\n","...Batch #12800 : Training Loss=0.3325943990051746, Training Accuracy=0.8631249666213989\n","...Batch #12900 : Training Loss=0.3288284491747618, Training Accuracy=0.8637499809265137\n","...Batch #13000 : Training Loss=0.3254734820500016, Training Accuracy=0.8725000023841858\n","...Batch #13100 : Training Loss=0.312148707266897, Training Accuracy=0.8700000047683716\n","...Batch #13200 : Training Loss=0.33205603674054146, Training Accuracy=0.8643749952316284\n","...Batch #13300 : Training Loss=0.32433647103607655, Training Accuracy=0.8687499761581421\n","...Batch #13400 : Training Loss=0.30605977568775417, Training Accuracy=0.8781249523162842\n","...Batch #13500 : Training Loss=0.3126856084167957, Training Accuracy=0.8799999952316284\n","...Batch #13600 : Training Loss=0.3208696900308132, Training Accuracy=0.8725000023841858\n","...Batch #13700 : Training Loss=0.30055237289518116, Training Accuracy=0.8837499618530273\n","...Batch #13800 : Training Loss=0.3113792125508189, Training Accuracy=0.8774999976158142\n","...Batch #13900 : Training Loss=0.32988101974129674, Training Accuracy=0.871874988079071\n","...Batch #14000 : Training Loss=0.3113471820950508, Training Accuracy=0.8731249570846558\n","...Batch #14100 : Training Loss=0.29816877387464047, Training Accuracy=0.8824999928474426\n","...Batch #14200 : Training Loss=0.30321662858128545, Training Accuracy=0.871874988079071\n","...Batch #14300 : Training Loss=0.3208725703135133, Training Accuracy=0.8643749952316284\n","...Batch #14400 : Training Loss=0.3157967147603631, Training Accuracy=0.87562495470047\n","...Batch #14500 : Training Loss=0.3148285309225321, Training Accuracy=0.8818749785423279\n","...Batch #14600 : Training Loss=0.3098352641239762, Training Accuracy=0.875\n","...Batch #14700 : Training Loss=0.3178214156255126, Training Accuracy=0.8631249666213989\n","...Batch #14800 : Training Loss=0.31369314804673193, Training Accuracy=0.8662499785423279\n","...Batch #14900 : Training Loss=0.3151634747162461, Training Accuracy=0.8725000023841858\n","...Batch #15000 : Training Loss=0.33152517952024935, Training Accuracy=0.8581249713897705\n","...Batch #15100 : Training Loss=0.3335565236583352, Training Accuracy=0.8643749952316284\n","...Batch #15200 : Training Loss=0.30606154501438143, Training Accuracy=0.8712499737739563\n","...Batch #15300 : Training Loss=0.3502000865340233, Training Accuracy=0.8624999523162842\n","...Batch #15400 : Training Loss=0.32600931756198404, Training Accuracy=0.8662499785423279\n","...Batch #15500 : Training Loss=0.30944358967244623, Training Accuracy=0.875\n","...Batch #15600 : Training Loss=0.31767420694231985, Training Accuracy=0.8693749904632568\n","...Batch #15700 : Training Loss=0.3277705356478691, Training Accuracy=0.8681249618530273\n","...Batch #15800 : Training Loss=0.326797938272357, Training Accuracy=0.8631249666213989\n","...Batch #15900 : Training Loss=0.30857882104814055, Training Accuracy=0.8837499618530273\n","...Batch #16000 : Training Loss=0.2959262216836214, Training Accuracy=0.8837499618530273\n","...Batch #16100 : Training Loss=0.34397471651434897, Training Accuracy=0.8574999570846558\n","...Batch #16200 : Training Loss=0.3120940561592579, Training Accuracy=0.8768749833106995\n","...Batch #16300 : Training Loss=0.3444742479547858, Training Accuracy=0.8656249642372131\n","...Batch #16400 : Training Loss=0.33395309176295995, Training Accuracy=0.8631249666213989\n","...Batch #16500 : Training Loss=0.3013982856273651, Training Accuracy=0.8799999952316284\n","...Batch #16600 : Training Loss=0.3335291877016425, Training Accuracy=0.8737499713897705\n","...Batch #16700 : Training Loss=0.30703950103372335, Training Accuracy=0.8806250095367432\n","...Batch #16800 : Training Loss=0.36400520183146, Training Accuracy=0.8499999642372131\n","...Batch #16900 : Training Loss=0.30503099158406255, Training Accuracy=0.8687499761581421\n","...Batch #17000 : Training Loss=0.32540577217936517, Training Accuracy=0.871874988079071\n","...Batch #17100 : Training Loss=0.3077629067748785, Training Accuracy=0.8774999976158142\n","...Batch #17200 : Training Loss=0.3194157671928406, Training Accuracy=0.871874988079071\n","...Batch #17300 : Training Loss=0.33063440676778555, Training Accuracy=0.859375\n","...Batch #17400 : Training Loss=0.32160270981490613, Training Accuracy=0.8712499737739563\n","...Batch #17500 : Training Loss=0.2841477721184492, Training Accuracy=0.8787499666213989\n","...Batch #17600 : Training Loss=0.33489260643720625, Training Accuracy=0.8656249642372131\n","...Batch #17700 : Training Loss=0.3070187869668007, Training Accuracy=0.8731249570846558\n","...Batch #17800 : Training Loss=0.290102145075798, Training Accuracy=0.8774999976158142\n","...Batch #17900 : Training Loss=0.33630873769521713, Training Accuracy=0.8631249666213989\n","...Batch #18000 : Training Loss=0.3225084267556667, Training Accuracy=0.8618749976158142\n","...Batch #18100 : Training Loss=0.35502601467072964, Training Accuracy=0.8499999642372131\n","...Batch #18200 : Training Loss=0.33129489459097383, Training Accuracy=0.85999995470047\n","...Batch #18300 : Training Loss=0.3067939853854477, Training Accuracy=0.8681249618530273\n","...Batch #18400 : Training Loss=0.33796978373080494, Training Accuracy=0.8675000071525574\n","...Batch #18500 : Training Loss=0.2905901612341404, Training Accuracy=0.8818749785423279\n","...Batch #18600 : Training Loss=0.32757050897926093, Training Accuracy=0.8693749904632568\n","...Batch #18700 : Training Loss=0.2968316285312176, Training Accuracy=0.8774999976158142\n","...Batch #18800 : Training Loss=0.30163874983787536, Training Accuracy=0.8731249570846558\n","...Batch #18900 : Training Loss=0.34799045119434596, Training Accuracy=0.8506249785423279\n","...Batch #19000 : Training Loss=0.3146599959209561, Training Accuracy=0.8762499690055847\n","...Batch #19100 : Training Loss=0.34475578024983405, Training Accuracy=0.8487499952316284\n","...Batch #19200 : Training Loss=0.31599791184067727, Training Accuracy=0.8781249523162842\n","...Batch #19300 : Training Loss=0.32808168526738885, Training Accuracy=0.8650000095367432\n","...Batch #19400 : Training Loss=0.34409218847751616, Training Accuracy=0.8662499785423279\n","...Batch #19500 : Training Loss=0.29593849156051877, Training Accuracy=0.87562495470047\n","...Batch #19600 : Training Loss=0.2951865157485008, Training Accuracy=0.8793749809265137\n","...Batch #19700 : Training Loss=0.3229922076314688, Training Accuracy=0.8637499809265137\n","...Batch #19800 : Training Loss=0.3411083503440022, Training Accuracy=0.8662499785423279\n","...Batch #19900 : Training Loss=0.3241308527626097, Training Accuracy=0.8687499761581421\n","...Batch #20000 : Training Loss=0.3348196569830179, Training Accuracy=0.8574999570846558\n","...Batch #20100 : Training Loss=0.29818905718624594, Training Accuracy=0.8725000023841858\n","...Batch #20200 : Training Loss=0.3043831865489483, Training Accuracy=0.8768749833106995\n","...Batch #20300 : Training Loss=0.31562950886785984, Training Accuracy=0.8768749833106995\n","...Batch #20400 : Training Loss=0.32636178478598593, Training Accuracy=0.8675000071525574\n","...Batch #20500 : Training Loss=0.3147346951067448, Training Accuracy=0.8731249570846558\n","...Batch #20600 : Training Loss=0.34912287842482326, Training Accuracy=0.8587499856948853\n","...Batch #20700 : Training Loss=0.2921938506513834, Training Accuracy=0.8725000023841858\n","...Batch #20800 : Training Loss=0.3088984470441937, Training Accuracy=0.8668749928474426\n","...Batch #20900 : Training Loss=0.33501490376889703, Training Accuracy=0.8668749928474426\n","...Batch #21000 : Training Loss=0.331263667345047, Training Accuracy=0.8675000071525574\n","...Batch #21100 : Training Loss=0.3063104959577322, Training Accuracy=0.8806250095367432\n","...Batch #21200 : Training Loss=0.30793042555451394, Training Accuracy=0.8687499761581421\n","...Batch #21300 : Training Loss=0.28723178815096617, Training Accuracy=0.8893749713897705\n","...Batch #21400 : Training Loss=0.30527955800294876, Training Accuracy=0.8856250047683716\n","...Batch #21500 : Training Loss=0.32235739719122647, Training Accuracy=0.8656249642372131\n","...Batch #21600 : Training Loss=0.3429669187217951, Training Accuracy=0.8637499809265137\n","...Batch #21700 : Training Loss=0.32267251662909985, Training Accuracy=0.871874988079071\n","...Batch #21800 : Training Loss=0.27680778451263904, Training Accuracy=0.8962500095367432\n","...Batch #21900 : Training Loss=0.32694082632660865, Training Accuracy=0.871874988079071\n","...Batch #22000 : Training Loss=0.3288233336806297, Training Accuracy=0.8668749928474426\n","...Batch #22100 : Training Loss=0.33799343664199116, Training Accuracy=0.859375\n","...Batch #22200 : Training Loss=0.31355204686522486, Training Accuracy=0.8637499809265137\n","...Batch #22300 : Training Loss=0.3061983226984739, Training Accuracy=0.8824999928474426\n","...Batch #22400 : Training Loss=0.2967740819975734, Training Accuracy=0.8787499666213989\n","...Batch #22500 : Training Loss=0.34825444128364325, Training Accuracy=0.8612499833106995\n","...Batch #22600 : Training Loss=0.3126578528434038, Training Accuracy=0.8700000047683716\n","...Batch #22700 : Training Loss=0.3469213302433491, Training Accuracy=0.8556249737739563\n","...Batch #22800 : Training Loss=0.3127443351596594, Training Accuracy=0.87562495470047\n","...Batch #22900 : Training Loss=0.30268745820969345, Training Accuracy=0.8681249618530273\n","...Batch #23000 : Training Loss=0.3012077805399895, Training Accuracy=0.8700000047683716\n","...Batch #23100 : Training Loss=0.32295734487473965, Training Accuracy=0.8693749904632568\n","...Batch #23200 : Training Loss=0.30596189700067045, Training Accuracy=0.8799999952316284\n","...Batch #23300 : Training Loss=0.30073355704545973, Training Accuracy=0.8731249570846558\n","...Batch #23400 : Training Loss=0.3073113368079066, Training Accuracy=0.887499988079071\n","...Batch #23500 : Training Loss=0.2923952158540487, Training Accuracy=0.8743749856948853\n","...Batch #23600 : Training Loss=0.33985804580152035, Training Accuracy=0.8587499856948853\n","...Batch #23700 : Training Loss=0.30988512374460697, Training Accuracy=0.8725000023841858\n","...Batch #23800 : Training Loss=0.3158888914436102, Training Accuracy=0.8762499690055847\n","...Batch #23900 : Training Loss=0.3348074809461832, Training Accuracy=0.87562495470047\n","...Batch #24000 : Training Loss=0.2852098076790571, Training Accuracy=0.8768749833106995\n","...Batch #24100 : Training Loss=0.29243646875023843, Training Accuracy=0.8806250095367432\n","...Batch #24200 : Training Loss=0.3016399474814534, Training Accuracy=0.8768749833106995\n","...Batch #24300 : Training Loss=0.32856175016611816, Training Accuracy=0.8643749952316284\n","...Batch #24400 : Training Loss=0.308146297596395, Training Accuracy=0.8687499761581421\n","...Batch #24500 : Training Loss=0.35783698976039885, Training Accuracy=0.8587499856948853\n","...Batch #24600 : Training Loss=0.31933362245559693, Training Accuracy=0.8725000023841858\n","...Batch #24700 : Training Loss=0.35104328982532024, Training Accuracy=0.8512499928474426\n","...Batch #24800 : Training Loss=0.3231663401052356, Training Accuracy=0.8706249594688416\n","...Batch #24900 : Training Loss=0.28350973762571813, Training Accuracy=0.8774999976158142\n","...Batch #25000 : Training Loss=0.32629728078842163, Training Accuracy=0.8675000071525574\n","...Batch #25100 : Training Loss=0.3165643769875169, Training Accuracy=0.875\n","...Batch #25200 : Training Loss=0.34532864272594455, Training Accuracy=0.8556249737739563\n","...Batch #25300 : Training Loss=0.30241824507713316, Training Accuracy=0.87562495470047\n","...Batch #25400 : Training Loss=0.28284171875566244, Training Accuracy=0.8924999833106995\n","...Batch #25500 : Training Loss=0.3095712091121823, Training Accuracy=0.8675000071525574\n","...Batch #25600 : Training Loss=0.34115392439067366, Training Accuracy=0.8587499856948853\n","...Batch #25700 : Training Loss=0.34541845180094244, Training Accuracy=0.8631249666213989\n","...Batch #25800 : Training Loss=0.3240343690291047, Training Accuracy=0.8712499737739563\n","...Batch #25900 : Training Loss=0.31417017538100483, Training Accuracy=0.8700000047683716\n","...Batch #26000 : Training Loss=0.3061178747564554, Training Accuracy=0.8768749833106995\n","...Batch #26100 : Training Loss=0.32350586123764513, Training Accuracy=0.87562495470047\n","...Batch #26200 : Training Loss=0.3495361060276628, Training Accuracy=0.8700000047683716\n","...Batch #26300 : Training Loss=0.298721596673131, Training Accuracy=0.8849999904632568\n","...Batch #26400 : Training Loss=0.29653495479375125, Training Accuracy=0.8793749809265137\n","...Batch #26500 : Training Loss=0.31473468720912934, Training Accuracy=0.87562495470047\n","...Batch #26600 : Training Loss=0.32366998098790645, Training Accuracy=0.8675000071525574\n","...Batch #26700 : Training Loss=0.286351355239749, Training Accuracy=0.8843749761581421\n","...Batch #26800 : Training Loss=0.2882081962749362, Training Accuracy=0.8843749761581421\n","...Batch #26900 : Training Loss=0.30087502658367155, Training Accuracy=0.8824999928474426\n","...Batch #27000 : Training Loss=0.29089020103216173, Training Accuracy=0.8812499642372131\n","...Batch #27100 : Training Loss=0.30967815697193146, Training Accuracy=0.8743749856948853\n","...Batch #27200 : Training Loss=0.301246608607471, Training Accuracy=0.8862499594688416\n","...Batch #27300 : Training Loss=0.30774571783840654, Training Accuracy=0.8774999976158142\n","...Batch #27400 : Training Loss=0.322731152176857, Training Accuracy=0.87562495470047\n","...Batch #27500 : Training Loss=0.32557534482330086, Training Accuracy=0.8743749856948853\n","...Batch #27600 : Training Loss=0.34032916981726885, Training Accuracy=0.8587499856948853\n","...Batch #27700 : Training Loss=0.35019557233899834, Training Accuracy=0.8612499833106995\n","...Batch #27800 : Training Loss=0.3259360730648041, Training Accuracy=0.859375\n","...Batch #27900 : Training Loss=0.3147056472115219, Training Accuracy=0.8837499618530273\n","...Batch #28000 : Training Loss=0.32820339538156984, Training Accuracy=0.8668749928474426\n","...Batch #28100 : Training Loss=0.298495192527771, Training Accuracy=0.87562495470047\n","...Batch #28200 : Training Loss=0.3001615585014224, Training Accuracy=0.8843749761581421\n","...Batch #28300 : Training Loss=0.31338124852627514, Training Accuracy=0.8700000047683716\n","...Batch #28400 : Training Loss=0.36143656898289916, Training Accuracy=0.8518750071525574\n","...Batch #28500 : Training Loss=0.3205415417999029, Training Accuracy=0.8737499713897705\n","...Batch #28600 : Training Loss=0.2914715380221605, Training Accuracy=0.8787499666213989\n","...Batch #28700 : Training Loss=0.34808001086115836, Training Accuracy=0.840624988079071\n","...Batch #28800 : Training Loss=0.32993107840418817, Training Accuracy=0.8681249618530273\n","...Batch #28900 : Training Loss=0.2971353879570961, Training Accuracy=0.8787499666213989\n","...Batch #29000 : Training Loss=0.30355213277041915, Training Accuracy=0.8743749856948853\n","...Batch #29100 : Training Loss=0.3124390304833651, Training Accuracy=0.8762499690055847\n","...Batch #29200 : Training Loss=0.3152416031807661, Training Accuracy=0.8725000023841858\n","...Batch #29300 : Training Loss=0.30336272697895766, Training Accuracy=0.8737499713897705\n","...Batch #29400 : Training Loss=0.31610703077167274, Training Accuracy=0.8768749833106995\n","...Batch #29500 : Training Loss=0.30611942019313576, Training Accuracy=0.8681249618530273\n","...Batch #29600 : Training Loss=0.33233741868287325, Training Accuracy=0.8668749928474426\n","...Batch #29700 : Training Loss=0.30841288454830645, Training Accuracy=0.8737499713897705\n","...Batch #29800 : Training Loss=0.3226622473821044, Training Accuracy=0.8643749952316284\n","...Batch #29900 : Training Loss=0.3333239987492561, Training Accuracy=0.8737499713897705\n","...Batch #30000 : Training Loss=0.33130909081548454, Training Accuracy=0.875\n","...Batch #30100 : Training Loss=0.30540803357958796, Training Accuracy=0.871874988079071\n","...Batch #30200 : Training Loss=0.3124575696885586, Training Accuracy=0.8768749833106995\n","...Batch #30300 : Training Loss=0.27091598480939866, Training Accuracy=0.8943749666213989\n","...Batch #30400 : Training Loss=0.30031966127455234, Training Accuracy=0.8774999976158142\n","...Batch #30500 : Training Loss=0.33063674755394457, Training Accuracy=0.8687499761581421\n","...Batch #30600 : Training Loss=0.31770999502390623, Training Accuracy=0.8725000023841858\n","...Batch #30700 : Training Loss=0.3214664073288441, Training Accuracy=0.8831250071525574\n","...Batch #30800 : Training Loss=0.3087686659768224, Training Accuracy=0.8706249594688416\n","...Batch #30900 : Training Loss=0.3191872866824269, Training Accuracy=0.8712499737739563\n","...Batch #31000 : Training Loss=0.2854533941298723, Training Accuracy=0.8700000047683716\n","...Batch #31100 : Training Loss=0.3238617863878608, Training Accuracy=0.8706249594688416\n","...Batch #31200 : Training Loss=0.3080626879259944, Training Accuracy=0.8643749952316284\n","...Batch #31300 : Training Loss=0.3125870453566313, Training Accuracy=0.8737499713897705\n","...Batch #31400 : Training Loss=0.3112332085520029, Training Accuracy=0.8681249618530273\n","...Batch #31500 : Training Loss=0.31599659953266385, Training Accuracy=0.8768749833106995\n","...Batch #31600 : Training Loss=0.32038219515234234, Training Accuracy=0.8737499713897705\n","...Batch #31700 : Training Loss=0.29498018085956573, Training Accuracy=0.8768749833106995\n","...Batch #31800 : Training Loss=0.3125087619572878, Training Accuracy=0.8687499761581421\n","...Batch #31900 : Training Loss=0.3125657778978348, Training Accuracy=0.8768749833106995\n","...Batch #32000 : Training Loss=0.30898519564419985, Training Accuracy=0.8774999976158142\n","...Batch #32100 : Training Loss=0.3127851201966405, Training Accuracy=0.8731249570846558\n","...Batch #32200 : Training Loss=0.3183385771140456, Training Accuracy=0.8768749833106995\n","...Batch #32300 : Training Loss=0.3177135641872883, Training Accuracy=0.8668749928474426\n","...Batch #32400 : Training Loss=0.33546708185225727, Training Accuracy=0.8668749928474426\n","...Batch #32500 : Training Loss=0.2878068505972624, Training Accuracy=0.8812499642372131\n","...Batch #32600 : Training Loss=0.3164766353368759, Training Accuracy=0.8806250095367432\n","...Batch #32700 : Training Loss=0.30757428918033836, Training Accuracy=0.8731249570846558\n","...Batch #32800 : Training Loss=0.30837499640882016, Training Accuracy=0.8806250095367432\n","...Batch #32900 : Training Loss=0.3074962364509702, Training Accuracy=0.85999995470047\n","...Batch #33000 : Training Loss=0.3111105156317353, Training Accuracy=0.871874988079071\n","...Batch #33100 : Training Loss=0.3430998238176107, Training Accuracy=0.8624999523162842\n","...Batch #33200 : Training Loss=0.30298937123268843, Training Accuracy=0.8725000023841858\n","...Batch #33300 : Training Loss=0.31374526146799325, Training Accuracy=0.8806250095367432\n","...Batch #33400 : Training Loss=0.321964663900435, Training Accuracy=0.8687499761581421\n","...Batch #33500 : Training Loss=0.3094302852451801, Training Accuracy=0.8774999976158142\n","...Batch #33600 : Training Loss=0.30784613694995644, Training Accuracy=0.8856250047683716\n","...Batch #33700 : Training Loss=0.3018864019587636, Training Accuracy=0.8787499666213989\n","...Batch #33800 : Training Loss=0.2917408121004701, Training Accuracy=0.8862499594688416\n","...Batch #33900 : Training Loss=0.3148417691141367, Training Accuracy=0.8706249594688416\n","...Batch #34000 : Training Loss=0.2943579537421465, Training Accuracy=0.8862499594688416\n","...Batch #34100 : Training Loss=0.3362336619757116, Training Accuracy=0.8712499737739563\n","...Batch #34200 : Training Loss=0.2783275680243969, Training Accuracy=0.8893749713897705\n","...Batch #34300 : Training Loss=0.3367398908361793, Training Accuracy=0.8693749904632568\n","...Batch #34400 : Training Loss=0.30932690620422365, Training Accuracy=0.8650000095367432\n","...Batch #34500 : Training Loss=0.30945327170193193, Training Accuracy=0.8793749809265137\n","...Batch #34600 : Training Loss=0.3292115972936153, Training Accuracy=0.8637499809265137\n","...Batch #34700 : Training Loss=0.2898410693183541, Training Accuracy=0.8781249523162842\n","...Batch #34800 : Training Loss=0.3058563850075007, Training Accuracy=0.8700000047683716\n","...Batch #34900 : Training Loss=0.3151643535494804, Training Accuracy=0.8693749904632568\n","...Batch #35000 : Training Loss=0.32462185591459275, Training Accuracy=0.8656249642372131\n","...Batch #35100 : Training Loss=0.3279927708953619, Training Accuracy=0.8650000095367432\n","...Batch #35200 : Training Loss=0.326171957552433, Training Accuracy=0.8731249570846558\n","...Batch #35300 : Training Loss=0.31110531467944386, Training Accuracy=0.8812499642372131\n","...Batch #35400 : Training Loss=0.29359226636588576, Training Accuracy=0.8737499713897705\n","...Batch #35500 : Training Loss=0.31168680142611266, Training Accuracy=0.8656249642372131\n","...Batch #35600 : Training Loss=0.31662394538521765, Training Accuracy=0.8650000095367432\n","...Batch #35700 : Training Loss=0.30489466242492197, Training Accuracy=0.871874988079071\n","...Batch #35800 : Training Loss=0.3126097571849823, Training Accuracy=0.8781249523162842\n","...Batch #35900 : Training Loss=0.31880668237805365, Training Accuracy=0.8587499856948853\n","...Batch #36000 : Training Loss=0.2801067543402314, Training Accuracy=0.890625\n","...Batch #36100 : Training Loss=0.3204701214283705, Training Accuracy=0.8700000047683716\n","...Batch #36200 : Training Loss=0.3034994063526392, Training Accuracy=0.8737499713897705\n","...Batch #36300 : Training Loss=0.3108804735541344, Training Accuracy=0.8706249594688416\n","Train: loss 0.3177072441429966, accuracy 0.8702841205726376\n","Valid: loss 0.3500972622139389, accuracy 0.8644853709508882, f1_ass 0.8046323321721118, f1_sweet 0.8962655601659751\n","####################################################################################################\n","--- 36747.37788963318 seconds ---\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"P9vvpKJ5NWML","executionInfo":{"status":"ok","timestamp":1628405399651,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"1d765c15-6a26-407e-aef0-0b9c99ae1c40"},"source":["plt.plot(train_losses, label=\"Train\")\n","plt.plot(valid_losses, label=\"Valid\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.grid()\n","plt.legend()"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f2f30aef450>"]},"metadata":{"tags":[]},"execution_count":35},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"99qR93ctNafL","executionInfo":{"status":"ok","timestamp":1628405401381,"user_tz":-60,"elapsed":1737,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"40653fcb-e31b-41d4-cd78-0ec60e09abf5"},"source":["plt.plot(train_accs)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Training Accuracy\")\n","plt.grid()"],"execution_count":36,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"lf9S1kcZNdrj","executionInfo":{"status":"ok","timestamp":1628405401382,"user_tz":-60,"elapsed":17,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"f401b6d8-99c2-4bb0-b9c2-9fed7b7bf5db"},"source":["plt.plot(valid_accs)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Validation Accuracy\")\n","plt.grid()"],"execution_count":37,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"MPsO1BE2Nfdp","executionInfo":{"status":"ok","timestamp":1628405401383,"user_tz":-60,"elapsed":17,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"adfbd0a3-75b1-4d3c-f2a8-9678dfe35f70"},"source":["plt.plot(valid_f1s_ass)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"F1 w.r.t. Assholes\")\n","plt.grid()"],"execution_count":38,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"K6dtNKt8NhXC","executionInfo":{"status":"ok","timestamp":1628405401384,"user_tz":-60,"elapsed":17,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"a7aa5847-6761-497e-c6bb-8d9dea071718"},"source":["plt.plot(valid_f1s_sweet)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"F1 w.r.t. Sweethearts\")\n","plt.grid()"],"execution_count":39,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"b4Mf6CL3NkX6","executionInfo":{"status":"ok","timestamp":1628405401384,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"543499b3-d55b-43d2-9667-9822a8923310"},"source":["plt.plot(valid_reb_accs)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Rebalanced Validation Accuracy\")\n","plt.grid()"],"execution_count":40,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"cbSRKrLmASS_"},"source":["Report performance on test set"]},{"cell_type":"code","metadata":{"id":"rXnvoZdzitIs","executionInfo":{"status":"ok","timestamp":1628405401385,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def test_model(model, dataloader, device, include_weights=True, print_conf_matr_on=True):\n","    '''\n","    Function that performs evaluation of the given model for one full pass of the samples in the give dataloader\n","    '''\n","\n","    model = model.eval()\n","\n","    losses = []\n","    # Instantiate the total confusion metric for the whole validation epoch\n","    total_conf_matr = np.array([[0, 0], [0, 0]])\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            y = batch['targets'].float().to(device)\n","            if include_weights:\n","                w = batch['weights'].to(device)\n","\n","            # Send the current batch through the model to get output probabilities\n","            y_out = model(input_ids=input_ids, attention_mask=attention_mask)\n","            del input_ids, attention_mask\n","\n","            # Turn the probabilities into binary predictions\n","            y_preds = y_out.detach()\n","            y_preds = torch.where(y_preds > 0.5, 1, 0)\n","\n","            # Get the confusion matrix for the current batch and add it to the total confusion matrix\n","            small_conf_matr = confusion_matrix(y.cpu(), y_preds.cpu())\n","            total_conf_matr += small_conf_matr\n","\n","            # Get the mean loss for the batch\n","            if include_weights:\n","                loss_fn = nn.BCELoss(weight=w, reduction=\"mean\").to(device)\n","            else:\n","                loss_fn = nn.BCELoss(reduction=\"mean\").to(device)\n","            loss = loss_fn(y_out, y.float())\n","            losses.append(loss.item())\n","\n","            torch.cuda.empty_cache()\n","            del y\n","\n","    # Get various metrics from the total confusion matrix\n","    asshole_recall, sweetheart_recall, asshole_precision, sweetheart_precision, accuracy, f1_ass, f1_sweet = get_metrics_from_conf_matrix(total_conf_matr, print_on=print_conf_matr_on)\n","\n","    # Get the average loss for the current evaluation epoch by averaging all batch-averaged losses.\n","    epoch_loss = np.mean(losses)\n","\n","    # Return the appropriate evaluation metrics\n","    return epoch_loss, accuracy, f1_ass, f1_sweet, asshole_precision, sweetheart_precision, asshole_recall, sweetheart_recall"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sf9nMHQniqVP","executionInfo":{"status":"ok","timestamp":1628405445465,"user_tz":-60,"elapsed":44095,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"22e6e2f9-a37f-437c-c25e-9cf10fc6bc56"},"source":["print(\"Performance on test set:\")\n","test_loss, test_accuracy, test_f1_ass, test_f1_sweet, test_precision_ass, test_precision_sweet, test_recall_ass, test_recall_sweet = test_model(model, valid_loader, device, include_weights=False, print_conf_matr_on=True)\n","print()"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Performance on test set:\n","Here is the confusion matrix:\n","[[8964 1211]\n"," [ 864 4273]]\n","\n","Here are the metrics derived from the confusion matrix:\n","recall wrt assholes = 0.8318084485108039\n","recall wrt sweethearts = 0.880982800982801\n","precision wrt assholes = 0.7791757840991976\n","precision wrt sweethearts) = 0.9120879120879121\n","\n","accuracy = 0.8644853709508882\n","f1 wrt assholes = 0.8046323321721118\n","f1 wrt sweethearts = 0.8962655601659751\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U-q4ZvtBpE9k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628405448612,"user_tz":-60,"elapsed":3152,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"c6e6c213-9bda-4978-81f7-01e347132134"},"source":["batch = next(iter(train_loader))\n","\n","input_ids = batch['input_ids'].to(device)\n","attention_mask = batch['attention_mask'].to(device)\n","\n","X = (input_ids, attention_mask)\n","with torch.no_grad():\n","    torch.jit.save(torch.jit.trace(model, (X), check_trace=False), '/content/drive/MyDrive/MastersProject/SavedModels/BertPooledPlusFFNNFinalToClassifyComments.pth')"],"execution_count":43,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:2155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"v5_Spvpmk4QJ","executionInfo":{"status":"ok","timestamp":1628405448613,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def create_texts_predictions_df_for_grasp(model, dataloader, device):\n","    texts = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            y = batch['targets'].float().to(device)\n","            textss = batch['sample_text']\n","\n","            # Send the current batch through the model to get output probabilities\n","            y_out = model(input_ids=input_ids, attention_mask=attention_mask)\n","            del input_ids, attention_mask\n","\n","            # Turn the probabilities into binary predictions\n","            y_preds = y_out.detach()\n","            y_preds = torch.where(y_preds > 0.5, 1, 0)\n","\n","            labels.extend([label.item() for label in y_preds])\n","            texts.extend(textss)\n","    \n","    data_dict = {\"text\": texts, \"predicted_label\": labels}\n","    df = pd.DataFrame(data_dict)\n","    \n","    return df"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDJxuR40IN_2","executionInfo":{"status":"ok","timestamp":1628405492097,"user_tz":-60,"elapsed":43489,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["df = create_texts_predictions_df_for_grasp(model, valid_loader, device)\n","df.to_csv('drive/MyDrive/MastersProject/SavedPredictedLabels-Texts/Comments-PredictedLabels.csv', index=False)"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbPRJzpLIVwD"},"source":[""],"execution_count":null,"outputs":[]}]}