{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PassUnfragmentedThroughBERT.ipynb","provenance":[{"file_id":"18U0Sv4niAisrtN1g4_LfrrRPsrQhpnJh","timestamp":1623080623395}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOH5rZmCjKlVIVYSKOjdUua"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ajOx2WcZ_uLO"},"source":["# Procedural"]},{"cell_type":"markdown","metadata":{"id":"onNkqotk_zFo"},"source":["Mount the my drive and create a folder for the data if it doesn't already exist"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PalDdmCx_fAe","executionInfo":{"status":"ok","timestamp":1623251106490,"user_tz":-60,"elapsed":491,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"bc1305d8-097b-4ca9-e9cf-62f8b50ec9db"},"source":["# Mount my drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Create a folder for the data if it does not already exist\n","import os\n","if not os.path.exists('/content/drive/MyDrive/MastersProject/data/'):\n","    os.makedirs('/content/drive/MyDrive/MastersProject/data/')\n","    print(\"Created the folder!\")\n","else:\n","    print(\"Folder already existed!\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Folder already existed!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHXD3NBE_3Rh","executionInfo":{"status":"ok","timestamp":1623251109538,"user_tz":-60,"elapsed":2447,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"1603d536-a2ff-4f84-91dc-e5c74d3f48e6"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z_nZLxKc_9Hq","executionInfo":{"status":"ok","timestamp":1623251110532,"user_tz":-60,"elapsed":999,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["import transformers\n","import pandas as pd\n","import torch\n","import numpy as np\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"4h2T_rfc_-p4","executionInfo":{"status":"ok","timestamp":1623251110534,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["RANDOM_SEED = 42"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fMoE3JDpAhTN"},"source":["# Prepare the data for BERT"]},{"cell_type":"markdown","metadata":{"id":"wzRHz8D3Ax-e"},"source":["Set the hyperparameters needed for data preparation"]},{"cell_type":"code","metadata":{"id":"gd78Vo-EAkKp","executionInfo":{"status":"ok","timestamp":1623251110535,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["MODEL_NAME = \"bert-base-cased\"\n","MAX_LEN = 20\n","VALID_TEST_PROPORTION = 0.2\n","BATCH_SIZE = 16\n","TEXT_CHOSEN = \"title\"     # in {body, title, text}"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UOoMBnWaBFuK"},"source":["Read the dataset as a pandas dataframe"]},{"cell_type":"code","metadata":{"id":"grZGRxenBFN_","executionInfo":{"status":"ok","timestamp":1623251112156,"user_tz":-60,"elapsed":1625,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["df = pd.read_csv('drive/MyDrive/MastersProject/data/aita_clean.csv')\n","df['text'] = df[\"title\"] + \" \" + df[\"body\"].fillna(\"\")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UjOi1pn1BNPm"},"source":["Specify the BERT dataset class"]},{"cell_type":"code","metadata":{"id":"hcP8QgoqBJER","executionInfo":{"status":"ok","timestamp":1623251112157,"user_tz":-60,"elapsed":13,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["class AITADataset(Dataset):\n","    # Upon onject instance creation, you feed the text samples, their targets, the tokeniser and the max length.\n","    def __init__(self, texts, targets, tokeniser, max_len):\n","        self.texts = texts\n","        self.targets = targets\n","        self.tokeniser = tokeniser\n","        self.max_len = max_len\n","        \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    # This method is called when a batch is created. \"item\" is the index of each sample to be in batch.\n","    def __getitem__(self, item):\n","        # Normally it is already a string\n","        text = str(self.texts[item])\n","\n","        # Create a dictionary constituting the encoding of the current item (i.e. current text)\n","        encoding = tokeniser(\n","            text,\n","            truncation=True,\n","            max_length=self.max_len,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_token_type_ids=False,\n","            return_tensors='pt')\n","        \n","        # These are unnecessary I think\n","        encoding['input_ids'] = encoding['input_ids'].flatten()\n","        encoding['attention_mask'] = encoding['attention_mask'].flatten()\n","        \n","        # In the encoding dictionary for the current text, add the target corresponding to it and the actual test\n","        dic_out = {'input_ids': encoding['input_ids'],\n","                   'attention_mask': encoding['attention_mask'],\n","                   'targets': torch.tensor(self.targets[item], dtype=torch.long),\n","                   'sample_text': text}\n","        \n","        return dic_out"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yQFyFTkVBV3q"},"source":["Make a function that creates a dataloader for BERT"]},{"cell_type":"code","metadata":{"id":"ybmH7DKyBVT4","executionInfo":{"status":"ok","timestamp":1623251112158,"user_tz":-60,"elapsed":12,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def create_data_loader(df, tokeniser, max_len, batch_size, text_chosen):\n","    '''\n","    Creates a dataset from the given dataframe and a dataloader spitting batches of the dataset\n","    '''\n","    if text_chosen == \"title\":\n","        texts = df.title.to_numpy()\n","    elif text_chosen == \"body\":\n","        texts = df.body.to_numpy()\n","    elif text_chosen == \"text\":\n","        texts = df.text.to_numpy()\n","    else:\n","        raise ValueError(\"Invalid TEXT_CHOSEN!\")\n","\n","    ds = AITADataset(\n","        texts=texts,\n","        targets=df.is_asshole.to_numpy(),\n","        tokeniser=tokeniser,\n","        max_len=max_len)\n","    \n","    dataloader = DataLoader(ds, batch_size=batch_size, num_workers=2)\n","    \n","    return dataloader"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"orS5SeVQBeVQ"},"source":["Split the dataframes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PC9b5wQHBcZv","executionInfo":{"status":"ok","timestamp":1623251112158,"user_tz":-60,"elapsed":12,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"23194379-2634-4bd1-8a0f-eef6a9a7f16d"},"source":["df_train, df_test_valid = train_test_split(df, test_size=VALID_TEST_PROPORTION, random_state=RANDOM_SEED)\n","df_valid, df_test = train_test_split(df_test_valid, test_size=0.5, random_state=RANDOM_SEED)\n","print(\"Train dataset:\", df_train.shape)\n","print(\"Valid dataset:\", df_valid.shape)\n","print(\"Test dataset:\", df_test.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Train dataset: (78102, 10)\n","Valid dataset: (9763, 10)\n","Test dataset: (9763, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Qw7HOAc_AUZ"},"source":["Save the targets and weights of the dataset split as above"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjA3_1PafWU-","executionInfo":{"status":"ok","timestamp":1623251112159,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"e173e022-4f05-4c34-f173-c46df59cb7ba"},"source":["y_train = torch.tensor(df_train['is_asshole'].values)\n","print(y_train[:15])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"spg1Cdrq--xC","executionInfo":{"status":"ok","timestamp":1623251112160,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["y_train = torch.tensor(df_train['is_asshole'].values)\n","torch.save(y_train, '/content/drive/MyDrive/MastersProject/BERT_outputs/y_train.pt')\n","del y_train\n","\n","y_valid = torch.tensor(df_valid['is_asshole'].values)\n","torch.save(y_valid, '/content/drive/MyDrive/MastersProject/BERT_outputs/y_valid.pt')\n","del y_valid\n","\n","y_test = torch.tensor(df_test['is_asshole'].values)\n","torch.save(y_test, '/content/drive/MyDrive/MastersProject/BERT_outputs/y_test.pt')\n","del y_test"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yhEYAEqyCkNu"},"source":["Initialise the BERT tokeniser based on the chosen model name"]},{"cell_type":"code","metadata":{"id":"6lpBIPbCCSmV","executionInfo":{"status":"ok","timestamp":1623251116203,"user_tz":-60,"elapsed":4051,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["tokeniser = transformers.BertTokenizer.from_pretrained(MODEL_NAME)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ISNXmhyHCqcC"},"source":["Create a dataloader from the small dataframe to be overfit"]},{"cell_type":"code","metadata":{"id":"oCVNJMz2CrPz","executionInfo":{"status":"ok","timestamp":1623251116204,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["train_loader = create_data_loader(df_train, tokeniser, MAX_LEN, BATCH_SIZE, TEXT_CHOSEN)\n","valid_loader = create_data_loader(df_valid, tokeniser, MAX_LEN, BATCH_SIZE, TEXT_CHOSEN)\n","test_loader = create_data_loader(df_test, tokeniser, MAX_LEN, BATCH_SIZE, TEXT_CHOSEN)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4dcOU87DUVO"},"source":["Inspect a batch from the dataloader"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8KU9IHW-Cskm","executionInfo":{"status":"ok","timestamp":1623251116204,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"ad48a45b-3916-4e37-d182-0631cc9e253e"},"source":["num_tokens_to_print_per_text = 100\n","train_data_batch = next(iter(train_loader))\n","loader_keys = train_data_batch.keys()\n","print(\"Each dataloader batch is like a dictionary with keys:\", loader_keys)\n","print(100*\"-\")\n","print(\"Shapes:\")\n","print()\n","print(\"input_ids:         \", train_data_batch['input_ids'].shape)\n","print(\"attention_mask:    \", train_data_batch['attention_mask'].shape)\n","print(\"targets:           \", train_data_batch['targets'].shape)\n","print(\"sample_text:       \", train_data_batch['targets'].shape)\n","print(100*\"-\")\n","print(\"Here are the first {} tokens of the first 5 tokenised texts in the batch:\".format(num_tokens_to_print_per_text))\n","print()\n","for i in range(0, 5):\n","  current_sample_token_ids = train_data_batch['input_ids'][i,0:num_tokens_to_print_per_text]\n","  current_sample_tokens = tokeniser.convert_ids_to_tokens(current_sample_token_ids)\n","  print(current_sample_tokens)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Each dataloader batch is like a dictionary with keys: dict_keys(['input_ids', 'attention_mask', 'targets', 'sample_text'])\n","----------------------------------------------------------------------------------------------------\n","Shapes:\n","\n","input_ids:          torch.Size([16, 20])\n","attention_mask:     torch.Size([16, 20])\n","targets:            torch.Size([16])\n","sample_text:        torch.Size([16])\n","----------------------------------------------------------------------------------------------------\n","Here are the first 100 tokens of the first 5 tokenised texts in the batch:\n","\n","['[CLS]', 'AI', '##TA', 'for', 'declining', 'to', 'attend', 'and', 'handle', 'my', 'half', 'sisters', 'funeral', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","['[CLS]', 'AI', '##TA', 'Food', 'Safety', 'in', 'Austin', ',', 'Texas', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","['[CLS]', 'AI', '##TA', 'for', 'wanting', 'to', 'have', 'our', 'wedding', '2', 'months', 'before', 'my', 'future', 'S', '##IL', \"'\", 's', 'who', '[SEP]']\n","['[CLS]', 'AI', '##TA', 'for', 'threatening', 'to', 'kick', 'my', 'wife', '’', 's', 'grand', '##ma', 'out', 'of', 'our', 'wedding', '?', '[SEP]', '[PAD]']\n","['[CLS]', 'AI', '##TA', 'for', 'breaking', 'off', 'a', 'toxic', 'friendship', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7MbLr0DWEZEt"},"source":["# Create BERT and send the primary data through it"]},{"cell_type":"code","metadata":{"id":"66sAnR7xO6aA","executionInfo":{"status":"ok","timestamp":1623251116205,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["BERT_OUT_CHOSEN = \"full\"      # in {pooled, full}"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YGauZtSlEjMg"},"source":["Use GPU if available"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9suKi99jDfgB","executionInfo":{"status":"ok","timestamp":1623251116206,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"d3720e10-0bbf-4944-ca8c-72dd39b3c030"},"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","torch.cuda.empty_cache()\n","print(device)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kYnHMJY0EnpH"},"source":["Instantiate BERT using a custom configuration and freeze it"]},{"cell_type":"code","metadata":{"id":"Gt3szaceElEa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623251122204,"user_tz":-60,"elapsed":6003,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"b6466eeb-9bbf-4a06-aa51-08e0c0bcc21f"},"source":["bert_config = transformers.BertConfig(vocab_size=28996,\n","                                      hidden_size=768,\n","                                      num_hidden_layers=12,\n","                                      num_attention_heads=12,\n","                                      max_position_embeddings=512)\n","\n","bert_model = transformers.BertModel.from_pretrained(MODEL_NAME, config=bert_config)\n","# Freeze BERT so that its weights are not further fine-tuned from their pretrained values and when samples are passed into it, grads are not stored in the RAM\n","for param in bert_model.parameters():\n","    param.requires_grad = False\n","bert_model = bert_model.to(device)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"yRYe5VcerP4f"},"source":["Get BERT's pooled output for the training/validation/test data"]},{"cell_type":"code","metadata":{"id":"2dNRt4VStQXd","executionInfo":{"status":"ok","timestamp":1623251122205,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["data_to_get = \"test\"    # In {train, test, valid}"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35RCPRqFFqbp","executionInfo":{"status":"ok","timestamp":1623251138481,"user_tz":-60,"elapsed":16279,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"244c0e19-f552-4bbd-a67a-3fd9c47c2a53"},"source":["# Choose whether to feed the batches of the train, valid or test loader to BERT\n","if data_to_get == \"test\":\n","  loader = test_loader\n","elif data_to_get == \"train\":\n","  loader = train_loader\n","elif data_to_get == \"valid\":\n","  loader = valid_loader\n","else:\n","  raise ValueError (\"Invalid data_to_get selected\")\n","\n","del test_loader\n","del train_loader\n","del valid_loader\n","\n","for i, data_batch in enumerate(loader):\n","    bert_output = bert_model(data_batch['input_ids'].to(device), data_batch['attention_mask'].to(device))\n","\n","    if i == 0:\n","        if BERT_OUT_CHOSEN == \"pooled\":\n","            X = bert_output['pooler_output']\n","        elif BERT_OUT_CHOSEN == \"full\":\n","            X = bert_output['last_hidden_state']\n","        else:\n","            raise ValueError(\"Invalid BERT_OUT_CHOSEN\")\n","    else:\n","        if BERT_OUT_CHOSEN == \"pooled\":\n","            X = torch.cat((X, bert_output['pooler_output']), 0)\n","        elif BERT_OUT_CHOSEN == \"full\":\n","            X = torch.cat((X, bert_output['last_hidden_state']), 0)\n","        else:\n","            raise ValueError(\"Invalid BERT_OUT_CHOSEN\")\n","\n","    del bert_output\n","    torch.cuda.empty_cache()\n","\n","    if (i + 1) % 10 == 0:\n","        print(\"Batch #{} through!\".format(i + 1))\n","\n","print(X.shape)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Batch #10 through!\n","Batch #20 through!\n","Batch #30 through!\n","Batch #40 through!\n","Batch #50 through!\n","Batch #60 through!\n","Batch #70 through!\n","Batch #80 through!\n","Batch #90 through!\n","Batch #100 through!\n","Batch #110 through!\n","Batch #120 through!\n","Batch #130 through!\n","Batch #140 through!\n","Batch #150 through!\n","Batch #160 through!\n","Batch #170 through!\n","Batch #180 through!\n","Batch #190 through!\n","Batch #200 through!\n","Batch #210 through!\n","Batch #220 through!\n","Batch #230 through!\n","Batch #240 through!\n","Batch #250 through!\n","Batch #260 through!\n","Batch #270 through!\n","Batch #280 through!\n","Batch #290 through!\n","Batch #300 through!\n","Batch #310 through!\n","Batch #320 through!\n","Batch #330 through!\n","Batch #340 through!\n","Batch #350 through!\n","Batch #360 through!\n","Batch #370 through!\n","Batch #380 through!\n","Batch #390 through!\n","Batch #400 through!\n","Batch #410 through!\n","Batch #420 through!\n","Batch #430 through!\n","Batch #440 through!\n","Batch #450 through!\n","Batch #460 through!\n","Batch #470 through!\n","Batch #480 through!\n","Batch #490 through!\n","Batch #500 through!\n","Batch #510 through!\n","Batch #520 through!\n","Batch #530 through!\n","Batch #540 through!\n","Batch #550 through!\n","Batch #560 through!\n","Batch #570 through!\n","Batch #580 through!\n","Batch #590 through!\n","Batch #600 through!\n","Batch #610 through!\n","torch.Size([9763, 20, 768])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WN9eyHR5i76S","executionInfo":{"status":"ok","timestamp":1623251140864,"user_tz":-60,"elapsed":2392,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["torch.save(X, '/content/drive/MyDrive/MastersProject/BERT_outputs/X_{}_{}_{}.pt'.format(data_to_get, TEXT_CHOSEN, BERT_OUT_CHOSEN))\n","# to load: X_train_pooled = torch.load('/content/drive/MyDrive/MastersProject/BERT_outputs/X_train_pooled.pt', map_location=torch.device('cpu'))"],"execution_count":20,"outputs":[]}]}