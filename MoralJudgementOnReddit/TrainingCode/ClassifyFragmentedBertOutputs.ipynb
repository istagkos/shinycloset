{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ClassifyFragmentedBertOutputs.ipynb","provenance":[{"file_id":"1BRRSA2hfJIm0C1mlgEWxsN3jyeogsI7U","timestamp":1622024792583}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyO4ETEWkZogovH1rDO8fhSE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_FFIPstv8aAA"},"source":["# Procedural"]},{"cell_type":"markdown","metadata":{"id":"a6UV6GWL8Zxw"},"source":["Mount my drive and create a folder for the data if it doesn't already exist"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YFsyv-gd8LM1","executionInfo":{"status":"ok","timestamp":1622145550783,"user_tz":-60,"elapsed":623,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"81076124-0976-498f-86a0-eff3c23ada8c"},"source":["3# Mount my drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Create a folder for the data if it does not already exist\n","import os\n","if not os.path.exists('/content/drive/MyDrive/MastersProject/data/'):\n","    os.makedirs('/content/drive/MyDrive/MastersProject/data/')\n","    print(\"Created the folder!\")\n","else:\n","    print(\"Folder already existed!\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Folder already existed!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHxuAcBW9HAG","executionInfo":{"status":"ok","timestamp":1622145553643,"user_tz":-60,"elapsed":2458,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"2e601c87-c02c-4755-aae6-9fd0465a9cf9"},"source":["!pip install psutil"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNI8PlTkR4Nr","executionInfo":{"status":"ok","timestamp":1622145556175,"user_tz":-60,"elapsed":2535,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"1551e332-2fe0-452c-a8ac-b0698365892a"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YEWdYHje8sTq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622145557187,"user_tz":-60,"elapsed":1015,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"ae304b6d-2d73-4202-f7f7-9ac8c00f4352"},"source":["import time\n","import pandas as pd\n","import torch\n","import numpy as np\n","from torch import nn, optim, utils\n","import psutil\n","from sklearn.metrics import confusion_matrix\n","from imblearn.over_sampling import SMOTE\n","import matplotlib.pyplot as plt\n","import random\n","import transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OpzjJo088ugg","executionInfo":{"status":"ok","timestamp":1622145557187,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["RANDOM_SEED = 42"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSs9QU6HqPRX"},"source":["# Create the classifer"]},{"cell_type":"markdown","metadata":{"id":"K4FhGUU5qSo3"},"source":["Set the hyperparameters needed for creating the classifier"]},{"cell_type":"code","metadata":{"id":"wzfIqcYSl0Xd","executionInfo":{"status":"ok","timestamp":1622145557187,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["NUMBER_NEURONS_LAYER_1 = 0\n","NUMBER_NEURONS_LAYER_2 = 0\n","NUMBER_NEURONS_LAYER_3 = 0\n","NUMBER_NEURONS_LAYER_4 = 0\n","DROPOUT_PROPORTION = 0.8\n","POOLED_BERT_OUTPUT = False"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-_RiMupsV8X"},"source":["Use GPU if available"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFeDUQTtsTFO","executionInfo":{"status":"ok","timestamp":1622145557188,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"9a64e4ee-2f0a-45f3-83a4-f1707e5a41c7"},"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","torch.cuda.empty_cache()\n","print(device)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p7539q8Cqyy5"},"source":["Specify the architecture of the classifier"]},{"cell_type":"code","metadata":{"id":"EO02BhgkqyRT","executionInfo":{"status":"ok","timestamp":1622145557188,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["class SentimentClassifier(nn.Module):\n","    '''\n","    The sentiment classifier class that handles BERT's output\n","    '''\n","    def __init__(self, n_neurons_1, n_neurons_2, n_neurons_3, n_neurons_4, dropout_proportion, pooled_bert_output):\n","        super(SentimentClassifier, self).__init__()\n","\n","        # Instantiate the straight forward attributes\n","        self.n_neurons_1 = n_neurons_1\n","        self.n_neurons_2 = n_neurons_2\n","        self.n_neurons_3 = n_neurons_3\n","        self.n_neurons_4 = n_neurons_4\n","        self.dropout_proportion = dropout_proportion\n","        self.pooled_bert_output = pooled_bert_output\n","\n","        # Determine the features of each sample input depending on whether it is the pooled output or full last hidden state of BERT.\n","        if self.pooled_bert_output:\n","          self.feats_in = 768\n","        else:\n","          self.feats_in = 768 * 512\n","\n","        # Determine the number of layers based on the given neurons for each layer prior to the last which by default has 1 neuron.\n","        if self.n_neurons_1 == 0:\n","          self.num_layers = 1\n","        elif self.n_neurons_2 == 0:\n","          self.num_layers = 2\n","        elif self.n_neurons_3 == 0:\n","          self.num_layers = 3\n","        elif self.n_neurons_4 == 0:\n","          self.num_layers = 4\n","        else:\n","          self.num_layers = 5\n","\n","        # Structure the architecture of the network depending on the number of layers and their number of neurons\n","        if self.num_layers == 1:\n","\n","          self.classifier = nn.Sequential(\n","              nn.Linear(self.feats_in, 1),\n","              nn.Sigmoid())\n","\n","        elif self.num_layers == 2:\n","\n","          self.classifier = nn.Sequential(\n","              nn.Linear(self.feats_in, self.n_neurons_1),\n","              nn.ReLU(),\n","              nn.BatchNorm1d(self.n_neurons_1),\n","              nn.Dropout(p=self.dropout_proportion),\n","\n","              nn.Linear(self.n_neurons_1, 1),\n","              nn.Sigmoid())\n","          \n","        elif self.num_layers == 3:\n","\n","          self.classifier = nn.Sequential(\n","              nn.Linear(self.feats_in, self.n_neurons_1),\n","              nn.ReLU(),\n","              # nn.BatchNorm1d(self.n_neurons_1),\n","              # nn.Dropout(p=0.9),\n","\n","              nn.Linear(self.n_neurons_1, self.n_neurons_2),\n","              nn.ReLU(),\n","              # nn.BatchNorm1d(self.n_neurons_2),\n","              # nn.Dropout(p=self.dropout_proportion),\n","              \n","              nn.Linear(self.n_neurons_2, 1),\n","              nn.Sigmoid())\n","          \n","        elif self.num_layers == 4:\n","\n","          self.classifier = nn.Sequential(\n","              nn.Linear(self.feats_in, self.n_neurons_1),\n","              nn.ReLU(),\n","              # nn.BatchNorm1d(self.n_neurons_1),\n","              # nn.Dropout(p=0.9),\n","\n","              nn.Linear(self.n_neurons_1, self.n_neurons_2),\n","              nn.ReLU(),\n","              # nn.BatchNorm1d(self.n_neurons_2),\n","              # nn.Dropout(p=self.dropout_proportion),\n","\n","              nn.Linear(self.n_neurons_2, self.n_neurons_3),\n","              nn.ReLU(),\n","              # nn.BatchNorm1d(self.n_neurons_3),\n","              # nn.Dropout(p=self.dropout_proportion),\n","              \n","              nn.Linear(self.n_neurons_3, 1),\n","              nn.Sigmoid())\n","          \n","        elif self.num_layers == 5:\n","\n","          self.classifier = nn.Sequential(\n","              nn.Linear(self.feats_in, self.n_neurons_1),\n","              nn.ReLU(),\n","              nn.BatchNorm1d(self.n_neurons_1),\n","              nn.Dropout(p=0.9),\n","\n","              nn.Linear(self.n_neurons_1, self.n_neurons_2),\n","              nn.ReLU(),\n","              nn.BatchNorm1d(self.n_neurons_2),\n","              nn.Dropout(p=self.dropout_proportion),\n","\n","              nn.Linear(self.n_neurons_2, self.n_neurons_3),\n","              nn.ReLU(),\n","              nn.BatchNorm1d(self.n_neurons_3),\n","              nn.Dropout(p=self.dropout_proportion),\n","\n","              nn.Linear(self.n_neurons_3, self.n_neurons_4),\n","              nn.ReLU(),\n","              nn.BatchNorm1d(self.n_neurons_4),\n","              nn.Dropout(p=self.dropout_proportion),\n","              \n","              nn.Linear(self.n_neurons_4, 1),\n","              nn.Sigmoid())\n","            \n","    def forward(self, X):\n","        if not self.pooled_bert_output:\n","            batch_size = X.shape[0]\n","            X = X.view(batch_size, -1)\n","        \n","        out = self.classifier(X)\n","        out = out.view(-1)\n","\n","        return out"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQ9WRWTcsDwV"},"source":["Instantiate the classifier"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4kwhU_ZsFcZ","executionInfo":{"status":"ok","timestamp":1622145560062,"user_tz":-60,"elapsed":2877,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"cd9d8db0-e765-4920-c74b-fb5d260ff335"},"source":["model = SentimentClassifier(NUMBER_NEURONS_LAYER_1, NUMBER_NEURONS_LAYER_2, NUMBER_NEURONS_LAYER_3, NUMBER_NEURONS_LAYER_4,\n","                            DROPOUT_PROPORTION, POOLED_BERT_OUTPUT)\n","model = model.to(device)\n","\n","# Print info on the model\n","print(model)\n","print()\n","model_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"The total number of trainable parameters in the classifier is: {}\".format(model_trainable_params))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["SentimentClassifier(\n","  (classifier): Sequential(\n","    (0): Linear(in_features=393216, out_features=1, bias=True)\n","    (1): Sigmoid()\n","  )\n",")\n","\n","The total number of trainable parameters in the classifier is: 393217\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p03_34CosfXi"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"1VMEjqt0shvz"},"source":["Set the hyperparameters needed for training"]},{"cell_type":"code","metadata":{"id":"JwEgcpbpJPCD","executionInfo":{"status":"ok","timestamp":1622145560063,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["LR = 0.000005\n","EPOCHS = 2\n","OPTIMISER = \"AdamW\"   # in {SGD, AdamW}\n","BATCH_SIZE = 32"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-KPaQXeTLOAY"},"source":["Create a function that yields evaluation metrics from a give confusion matrix"]},{"cell_type":"code","metadata":{"id":"opbQ2KTjLM_J","executionInfo":{"status":"ok","timestamp":1622145560063,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def get_metrics_from_conf_matrix(confusion_matrix: np.ndarray, print_on=False):\n","    '''\n","    Takes in a confusion matrix and deduces evaluation metrics based on it.\n","\n","    params: confusion_matrix: numpy array giving the confusion matrix of binary classification\n","            print_on: Boolean determining whether or not to print input confusion matrix and deduced matrices\n","    return: Various self-explanatory evaluation metrics\n","    '''\n","\n","    # Get TP, FP, FN, TN (with different names).\n","    true_asshole = confusion_matrix[1][1]\n","    false_asshole = confusion_matrix[0][1]\n","    true_sweetheart = confusion_matrix[0][0]\n","    false_sweetheart = confusion_matrix[1][0]\n","\n","    # Calculate\n","    accuracy = (true_asshole + true_sweetheart) / (true_sweetheart + true_asshole + false_asshole + false_sweetheart)\n","    asshole_detection_precision = true_asshole / (true_asshole + false_asshole)\n","    sweetheart_detection_precision = true_sweetheart / (true_sweetheart + false_sweetheart)\n","    asshole_detection_accuracy = true_asshole / (true_asshole + false_sweetheart)\n","    sweetheart_detection_accuracy = true_sweetheart / (true_sweetheart + false_asshole)\n","    f1_ass = 2 * ((asshole_detection_precision * asshole_detection_accuracy) / (asshole_detection_precision + asshole_detection_accuracy))\n","    f1_sweet = 2 * ((sweetheart_detection_precision * sweetheart_detection_accuracy) / (sweetheart_detection_precision + sweetheart_detection_accuracy))\n","\n","    # If print is on, print the input confusion matrix and its evaluation metrics\n","    if print_on:\n","        print(\"Here is the confusion matrix:\")\n","        print(confusion_matrix)\n","        print()\n","        print(\"Here are the metrics derived from the confusion matrix:\")\n","        print(\"recall wrt assholes =\", asshole_detection_accuracy)\n","        print(\"recall wrt sweethearts =\", sweetheart_detection_accuracy)\n","        print(\"precision wrt assholes =\", asshole_detection_precision)\n","        print(\"precision wrt sweethearts) =\", sweetheart_detection_precision)\n","        print()\n","        print(\"accuracy =\", accuracy)\n","        print(\"f1 wrt assholes =\", f1_ass)\n","        print(\"f1 wrt sweethearts =\", f1_sweet)\n","\n","    # Return various self explanatory evaluation metrics\n","    return(asshole_detection_accuracy, sweetheart_detection_accuracy,\n","           asshole_detection_precision, sweetheart_detection_precision,\n","           accuracy, f1_ass, f1_sweet)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qy16fC_9Jl5G"},"source":["Create a function that performs one epoch of training"]},{"cell_type":"code","metadata":{"id":"hzvNf-npsgj_","executionInfo":{"status":"ok","timestamp":1622145560064,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def train_epoch(model, dataloader, optimiser, device, scheduler=None, print_on=False):\n","    '''\n","    Function that performs one epoch (one pass through each sample in the given loader) of training of the given model.\n","    '''\n","    model = model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    many_batches_losses = []\n","    many_batches_accs = []\n","    for i, batch in enumerate(dataloader):\n","  \n","      X = batch[0].float().to(device)\n","      y = batch[1].float().to(device)\n","      w = batch[2].float().to(device)\n","\n","      # Pass the batch through the classifier (output layers)\n","      y_out = model(X)\n","\n","      # Binarise output probs to predictions in {0, 1}\n","      y_preds = y_out.detach()\n","      y_preds = torch.where(y_preds > 0.5, 1, 0)\n","      correct_predictions += int(torch.sum(y_preds == y))\n","      total_predictions += len(y_preds)\n","      many_batches_accs.append((torch.sum(y_preds == y) / len(y_preds)).cpu())\n","\n","      # Get the mean loss for the batch\n","      loss_fn = nn.BCELoss(weight=w, reduction=\"mean\").to(device)\n","      loss = loss_fn(y_out, y)\n","      loss.backward()\n","      losses.append(loss.item())\n","      many_batches_losses.append(loss.item())\n","      \n","      # Not sure why I am clipping the grad here. Apparently it helps prevent exploding gradients.\n","      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","      # Take an optimisation step\n","      optimiser.step()\n","      if scheduler:\n","        scheduler.step()\n","\n","      if print_on:\n","      # Every so often print the current training accuracy\n","            if (i + 1) % 10 == 0:\n","                print(\"...Batch #{} : Training Loss={}, Training Accuracy={}\".format(i + 1, sum(many_batches_losses) / len(many_batches_losses), sum(many_batches_accs) / len(many_batches_accs)))\n","                many_batches_losses = []\n","                many_batches_accs = []\n","\n","    # Return the training accuracy and the mean training loss for the given epoch\n","    return correct_predictions / total_predictions, np.mean(losses)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ryz2wJ4_LVuX"},"source":["Create a function that performs one epoch of validation"]},{"cell_type":"code","metadata":{"id":"xTfCEBcMI_2I","executionInfo":{"status":"ok","timestamp":1622145560064,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["def eval_model(model, dataloader, device, print_conf_matr_on=True):\n","  '''\n","  Function that performs evaluation of the given model for one full pass of the samples in the give dataloader\n","  '''\n","\n","  model = model.eval()\n","\n","  '''\n","  losses = []\n","  '''\n","  total_conf_matr = np.array([[0, 0], [0, 0]])\n","\n","  with torch.no_grad():\n","        for batch in dataloader:\n","\n","            X = batch[0].float().to(device)\n","            y = batch[1].float().to(device)\n","\n","            # Send the current batch through the model to get output probabilities\n","            y_out = model(X)\n","\n","            '''\n","            # Get the mean loss for the batch\n","            loss_fn = nn.BCELoss(reduction=\"mean\").to(device)\n","            loss = loss_fn(outputs, targets)\n","            losses.append(loss.item())\n","            '''\n","\n","            # Turn the probabilities into binary predictions\n","            y_preds = y_out.detach()\n","            y_preds = torch.where(y_preds > 0.5, 1, 0)\n","\n","            # Get the confusion matrix for the current batch and add it to the total confusion matrix\n","            small_conf_matr = confusion_matrix(y.cpu(), y_preds.cpu())\n","            total_conf_matr += small_conf_matr\n","\n","  # Get various metrics from the total confusion matrix\n","  asshole_recall, sweetheart_recall, asshole_precision, sweetheart_precision, accuracy, f1_ass, f1_sweet = get_metrics_from_conf_matrix(total_conf_matr, print_on=print_conf_matr_on)\n","\n","  # Return the appropriate evaluation metrics\n","  return accuracy, f1_ass, f1_sweet"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5rABChiBNuF1"},"source":["Perform many epochs of validation and training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"zfbe9jOMPhM7","executionInfo":{"status":"error","timestamp":1622145834950,"user_tz":-60,"elapsed":274892,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}},"outputId":"63265fb0-87f9-47a3-8941-69db0343374c"},"source":["start_time = time.time()\n","\n","# Creating the optimiser with its initial learning rate\n","if OPTIMISER == \"AdamW\":\n","    optimiser = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.08)\n","elif OPTIMISER == \"SGD\":\n","    optimiser = optim.SGD(model.parameters(), lr=LR)\n","\n","train_losses = []\n","train_accs = []\n","valid_accs = []\n","valid_reb_accs = []\n","valid_f1s_ass = []\n","valid_f1s_sweet = []\n","\n","for epoch in range(EPOCHS):\n","    print(\"Epoch {} / {}\".format(epoch+1, EPOCHS))\n","\n","    for fragment_idx in range(1, 2):\n","\n","        # Load the data (outputs from BERT, labels and weights) for the particular fragment onto the cpu RAM\n","        X_train = torch.load('/content/drive/MyDrive/MastersProject/BERT_outputs/split_in_10/X_train_{}_unpooled.pt'.format(fragment_idx), map_location=torch.device('cpu'))\n","        y_train = torch.load('/content/drive/MyDrive/MastersProject/BERT_outputs/split_in_10/y_train_{}.pt'.format(fragment_idx), map_location=torch.device('cpu'))\n","        w_train = torch.load('/content/drive/MyDrive/MastersProject/BERT_outputs/split_in_10/w_train_{}.pt'.format(fragment_idx), map_location=torch.device('cpu'))\n","        train_dataset = utils.data.TensorDataset(X_train, y_train, w_train)\n","        train_loader = utils.data.DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2)\n","\n","        # Use the loaded data for training. The fragment data will be split in batches and sent to the GPU RAM one batch at a time for training.\n","        train_acc, train_loss = train_epoch(model, train_loader, optimiser, device, print_on=False)\n","        train_losses.append(train_loss)\n","        train_accs.append(train_acc)\n","        print(\"...Fragment {} / 10: Train: loss {}, accuracy {}\".format(fragment_idx, train_loss, train_acc))\n","\n","    # When all fragment have gone through, i.e. one epoch of training has been completed, delete all data from the CPU RAM\n","    del X_train, y_train, w_train, train_dataset, train_loader\n","    # Every epoch also clear the cache of the GPU memory\n","    torch.cuda.empty_cache()\n","\n","    # Load the validation data onto the CPU RAM\n","    X_valid = torch.load('/content/drive/MyDrive/MastersProject/BERT_outputs/split_in_10/X_valid_unpooled.pt', map_location=torch.device('cpu'))\n","    y_valid = torch.load('/content/drive/MyDrive/MastersProject/BERT_outputs/split_in_10/y_valid.pt', map_location=torch.device('cpu'))\n","    valid_dataset = utils.data.TensorDataset(X_valid, y_valid)\n","    valid_loader = utils.data.DataLoader(valid_dataset, BATCH_SIZE, shuffle=True, num_workers=2)\n","\n","    # Send the validation data for validation. One batch at a time, X_valid will be being sent to the GPU RAM to go through the model.\n","    val_acc, val_f1_ass, val_f1_sweet = eval_model(model, valid_loader, device, print_conf_matr_on=False)\n","\n","    valid_accs.append(val_acc)\n","    valid_f1s_ass.append(val_f1_ass)\n","    valid_f1s_sweet.append(val_f1_sweet)\n","    print(\"Valid: accuracy {}, f1_ass {}, f1_sweet {}\".format(val_acc, val_f1_ass, val_f1_sweet))\n","\n","    # Delete validation data from the CPU RAM\n","    del X_valid, y_valid, valid_dataset, valid_loader\n","    torch.cuda.empty_cache()\n","\n","    # Load the rebalanced validation data onto the CPU RAM\n","    X_valid_reb = torch.load('/content/drive/MyDrive/MastersProject/BERT_outputs/split_in_10/X_valid_reb_unpooled.pt', map_location=torch.device('cpu'))\n","    y_valid_reb = torch.load('/content/drive/MyDrive/MastersProject/BERT_outputs/split_in_10/y_valid_reb.pt', map_location=torch.device('cpu'))\n","    valid_reb_dataset = utils.data.TensorDataset(X_valid_reb, y_valid_reb)\n","    valid_reb_loader = utils.data.DataLoader(valid_reb_dataset, BATCH_SIZE, shuffle=True, num_workers=2)\n","    \n","    # Send the rebalanced validation data for validation. One batch at a time, X_valid_reb will be being sent to the GPU RAM to go through the model.\n","    val_reb_acc, val_reb_f1_ass, val_reb_f1_sweet = eval_model(model, valid_reb_loader, device, print_conf_matr_on=False)\n","    valid_reb_accs.append(val_reb_acc)\n","\n","    # Delete validation data from the CPU RAM\n","    del X_valid_reb, y_valid_reb, valid_reb_dataset, valid_reb_loader\n","    torch.cuda.empty_cache()\n","\n","    print(100*\"#\")\n","\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch 1 / 2\n","...Fragment 1 / 10: Train: loss 0.6917475497236057, accuracy 0.5413572343149808\n","Valid: accuracy 0.7155587421899007, f1_ass 0.12314493211240922, f1_sweet 0.8302463475762578\n"],"name":"stdout"},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-43790c915325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mX_valid_reb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/MastersProject/BERT_outputs/split_in_10/X_valid_reb_unpooled.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0my_valid_reb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/MastersProject/BERT_outputs/split_in_10/y_valid_reb.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mvalid_reb_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_reb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_reb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mvalid_reb_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_reb_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size mismatch between tensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"]}]},{"cell_type":"code","metadata":{"id":"C5OoJ-ahcmXY","executionInfo":{"status":"aborted","timestamp":1622145834947,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["torch.cuda.empty_cache()\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"haISCNopOTQk","executionInfo":{"status":"aborted","timestamp":1622145834947,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["plt.plot(train_losses)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Training Loss\")\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5Zb45WF9c6v","executionInfo":{"status":"aborted","timestamp":1622145834948,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["plt.plot(train_accs)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Training Accuracy\")\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6OEWxhSlf4jF","executionInfo":{"status":"aborted","timestamp":1622145834948,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["plt.plot(valid_accs)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Validation Accuracy\")\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S8Pu1leTB-do","executionInfo":{"status":"aborted","timestamp":1622145834949,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["plt.plot(valid_f1s_ass)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"F1 w.r.t. Assholes\")\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"--YLZ5bcCFa0","executionInfo":{"status":"aborted","timestamp":1622145834949,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["plt.plot(valid_f1s_sweet)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"F1 w.r.t. Sweethearts\")\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnTeAb5p8PB4","executionInfo":{"status":"aborted","timestamp":1622145834950,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ion Stagkos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgj4rUVcMhEW9tY5Cj_SZo8Aj2dbXQta6pzdGcg=s64","userId":"00279017017823147531"}}},"source":["plt.plot(valid_reb_accs)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Rebalanced Validation Accuracy\")\n","plt.grid()"],"execution_count":null,"outputs":[]}]}